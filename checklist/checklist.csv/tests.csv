ID,Topic,Title,Requirement,Explanation,References
1.1,General,Write Descriptive Test Names,"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.","If out tests are narrow and sufficiently descriptive, the test name itself may give us enough information to start debugging. This also helps us to identify what is being tested inside the function.","trenk2014, winters2024"
1.2,General,Keep Tests Focused,"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.","If we test multiple scenarios in a single test, it is hard to idenitfy exactly what went wrong. Keeping one scenario in a single test helps us to isolate problematic scenarios.",yu2018
1.3,General,Prefer Narrow Assertions in Unit Tests,Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.,"If we have overly wide assertions (such as depending on every field of a complex output proto), the test may fail for many unimportant reasons. False positives are the opoosite of actionable.",kent2024
1.4,General,Keep Cause and Effect Clear,Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.,Refrain from using large global test data structures shared across multiple unit tests. This will allow for clear identification of each test's setup and the cause and effect.,yu2017
2.1,Data Presence,Ensure Data File Loads as Expected,"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.","Reading data is a common scenario encountered in ML projects.  This item ensures that the data exists and can be loaded with expected format, and gracefully exit when unable to load the data.",msise2023
2.2,Data Presence,Ensure Saving Data/Figures Function Works as Expected,"Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.",Writing operations create artifacts at different stages of the analysis. Making sure the artifacts are created as expected ensures that the artifacts we obtained at the end of the analysis would be consistent and reproducible.,msise2023
3.1,Data Quality,Files Contain Data,Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.,This checklist item is crucial as it confirms the presence of usable data within the files. It prevents errors in later stages of the project by ensuring data is available from the start.,msise2023
3.2,Data Quality,Data in the Expected Format,Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.,"Ensuring that data and images are in the correct format is essential for compatibility with processing tools and algorithms, which may not handle unexpected formats gracefully.",msise2023
3.3,Data Quality,Data Does Not Contain Null Values or Outliers,Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.,"Null values can lead to errors or inaccurate computations in many data processing applications, while outliers can distort statistical analyses and models. As such, these values should be checked when before the data is being ingested.",msise2023
4.1,Data Ingestion,Cleaning and Transformation Functions Work as Expected,"Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.",Fixed input and output during the data cleaning and transformation routines should be tested so that no unexpected transformation is introduced during these steps.,msise2023
5.1,Model Fitting,Validate Model Input and Output Compatibility,Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.,Ensuring that inputs and outputs conform to expected specifications is critical for the correct functioning of the model in a production environment.,msise2023
5.2,Model Fitting,Check Model is Learning During Fit,"For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.",Making sure the training process is indeed training the model is crucial as model without training is not fitted to any data and the performance would suffer.,msise2023
5.3,Model Fitting,Ensure Model Output Shape Aligns with Expectation,"Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.",Correct output alignment confirms that the model is accurately interpreting the input data and making predictions that are sensible given the context.,jordan2020
5.4,Model Fitting,Ensure Model Output Aligns with Task Trained,"Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.",This ensures that the model's output is interpretable and relevant to the task it was trained for.,jordan2020
5.5,Model Fitting,Validate Loss Reduction on Gradient Update,"If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.",A decrease in training loss after a gradient update demonstrates that the data is adequate to be fitted into the model.,jordan2020
5.6,Model Fitting,Check for Data Leakage,"Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.","Data leakage can compromise the model's ability to generalize to unseen data, making it crucial to ensure datasets are properly segregated.",jordan2020
6.1,Model Evaluation,Dummy Title,This is a dummy item and there is nothing needed to act on.,This is a dummy item to show how the checklist items would be stored inside this YAML file. It serves no other purpose.,"http://128.0.0.1, UBC-MDS"
7.1,Artifact Testing,Invariance Tests,"There are tests to ensure making a set of preturbations to the input that is expected to cause no effect would not change the model's output. For example, in sentiment analysis tasks, changing the name of the subject or the name of a location should not affect the sentence's sentiment.",Models should be making predictions in a robust and consistent manner such that changes to the input which are known or expected to have no effect on the model's task should not affect the model's prediction.,"jordan2020, ribeiro2020accuracy"
7.2,Artifact Testing,Directional Expectation Tests,"There are tests to ensure making a set of preturbations to the input that is expected to affect the predications in a particular direction would change the model's output in a same direction. For example, in a regression model, increasing the number of bathrooms (holding all other features constnat) should not cause a drop in price.",Models should be making predictions in a robust and consistent manner such that changes to the input which are known or expected to have an directional effect should be reflected in the model's predictions.,"jordan2020, ribeiro2020accuracy"
7.3,Artifact Testing,Minimum Functionality Tests,"If there are critical scenarios where prediction errors lead to high consequences, tests should be written for ensuring the model's behavior in the scenario is expected. For example, in text classification, a test can check the model's output for sentences less than 5 words would perform as expected if the performance on short pieces of text is critical.","In real world, some model outcomes are often more important than others. Traditional Machine Learning systems optimize for overall quality which might lead to over-optimistic and/or non-representable performance metrics in cases where the importance of certain outcomes far outweigh the rest. For example, falsely identifying valid emails as spam has a more serious consequence than identifying a spam as a valid email.","jordan2020, ribeiro2020accuracy"
