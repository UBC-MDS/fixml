<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>proposal</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./proposal.html">Capstone Proposal</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./final_report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Final Report</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proposal.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Capstone Proposal</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement">Problem Statement</a></li>
  <li><a href="#our-objectives" id="toc-our-objectives" class="nav-link" data-scroll-target="#our-objectives">Our Objectives</a></li>
  </ul></li>
  <li><a href="#our-product" id="toc-our-product" class="nav-link" data-scroll-target="#our-product">Our Product</a>
  <ul class="collapse">
  <li><a href="#evaluation-artifacts" id="toc-evaluation-artifacts" class="nav-link" data-scroll-target="#evaluation-artifacts">Evaluation Artifacts</a></li>
  <li><a href="#success-metrics" id="toc-success-metrics" class="nav-link" data-scroll-target="#success-metrics">Success Metrics</a></li>
  <li><a href="#data-science-approach" id="toc-data-science-approach" class="nav-link" data-scroll-target="#data-science-approach">Data Science Approach</a></li>
  </ul></li>
  <li><a href="#delivery-timeline" id="toc-delivery-timeline" class="nav-link" data-scroll-target="#delivery-timeline">Delivery Timeline</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin</p>
<section id="executive-summary" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p>The rapid growth of global artificial intelligence (AI) markets presents opportunities and challenges. While AI systems have the potential to impact various aspects of human life, ensuring their software quality remains a significant concern. Current testing strategies for machine learning (ML) systems lack standardization and comprehensiveness, which poses risks to stakeholders, such as financial losses and safety hazards.</p>
<p>Our proposal addresses this challenge by developing a manually curated checklist which contains best practices and recommendations in testing ML systems. Additionally, an end-to-end application incorporating the checklist and Large Language Model (LLM) will be developed to analyze given ML system source codes and provide test completeness evaluation, missing test recommendations, and test function specification generation. Our proposed solution will enable users to systematically assess, improve, and include tests tailored to their ML systems through a combination of human expertise codified within the checklist and parametric memory from LLMs.</p>
<p>In the following weeks, we will develop and refine our product through a swift and efficient iterative development approach, with the aim to deliver a rigorously tested and fully-documented system to our partners by the end of the project.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<section id="problem-statement" class="level3">
<h3 class="anchored" data-anchor-id="problem-statement">Problem Statement</h3>
<p>The global artificial intelligence (AI) market is growing exponentially <span class="citation" data-cites="grand2021artificial">(<a href="#ref-grand2021artificial" role="doc-biblioref">Grand-View-Research 2021</a>)</span>, driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.</p>
<p>However, ensuring the software quality of these systems remains a significant challenge <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span>. Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as substantial financial losses <span class="citation" data-cites="Asheeta2019">(<a href="#ref-Asheeta2019" role="doc-biblioref">Regidi 2019</a>)</span> and safety hazards.</p>
<p>Therefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?</p>
</section>
<section id="our-objectives" class="level3">
<h3 class="anchored" data-anchor-id="our-objectives">Our Objectives</h3>
<p>We propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate a checklist to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software’s trustworthiness, quality, and reproducibility across both the industry and academia <span class="citation" data-cites="kapoor2022leakage">(<a href="#ref-kapoor2022leakage" role="doc-biblioref">Kapoor and Narayanan 2022</a>)</span>.</p>
</section>
</section>
<section id="our-product" class="level2">
<h2 class="anchored" data-anchor-id="our-product">Our Product</h2>
<p>Our solution offers an end-to-end application for evaluating and enhancing the robustness of users’ ML systems.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/proposed_system_overview.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Main components and workflow of the proposed system. The checklist would be written in <a href="https://yaml.org/">YAML</a> to maximize readability for both humans and machines. We hope this will encourage researchers/users to read, understand and modify the checklist items, while keeping the checklist closely integrated with other components in our system.</figcaption>
</figure>
</div>
<p>One big challenge in utilizing LLMs to reliably and consistently evaluate ML systems is their tendency to generate illogical and/or factually wrong information known as hallucination <span class="citation" data-cites="zhang2023sirens">(<a href="#ref-zhang2023sirens" role="doc-biblioref">Zhang et al. 2023</a>)</span>.</p>
<p>To combat this, the proposed system will incorporate a checklist (<a href="overview-diagram">Fig. 1</a>) which would be curated manually and incorporate best practices in software testing and identified areas to be tested inside ML pipeline from human experts and past research.</p>
<p>This checklist will be our basis in evaluating the effectiveness and completeness of existing tests in a given codebase. Relevant information will be injected into a prompt template, which the LLMs would then be prompted to follow the checklist <strong>exactly</strong> during the evaluation.</p>
<p>Here is an example of how the proposed checklist would be structured:</p>
<pre class="{yaml}"><code>%YAML 1.2
---
Title: Checklist for Tests in Machine Learning Projects
Description: &gt;
  This is a comprehensive checklist for evaluating the data and ML pipeline
  based on identified testing strategies from experts in the field.
Test Areas:
  - Topic: General
    Description: &gt;
      The following items describe best practices for all tests to be
      written.
    Tests:
      - Title: Write Descriptive Test Names
        Requirement: &gt;
          Every test function should have a clear, descriptive name
        Explanation: &gt;
          If out tests are narrow and sufficiently descriptive, the test
          name itself may give us enough information to start debugging.
          This also helps us to identify what is being tested inside the
          function.
        References:
          - https://testing.googleblog.com/2014/10/testing-on-toilet-writing-descriptive.html
          - https://testing.googleblog.com/2024/05/test-failures-should-be-actionable.html

      - Title: Keep Tests Focused
        Requirement: &gt;
          Each test should only test one scenario, meaning that in each
          test we should only use one set of mock data.
        Explanation: &gt;
          If we test multiple scenarios in a single test, it is hard to
          idenitfy exactly what went wrong. Keeping one scenario in a
          single test helps us to isolate problematic scenarios.
        References:
          - https://testing.googleblog.com/2018/06/testing-on-toilet-keep-tests-focused.html

      - Title: Prefer Narrow Assertions in Unit Tests
        Requirement: &gt;
          The assertions inside the tests should be narrow, meaning that
          when checking a complex object, any unrelated behavior should
          not be tested - Assert on only relevant behaviors.
        Explanation: &gt;
          If we have overly wide assertions (such as depending on every
          field of a complex output proto), the test may fail for many
          unimportant reasons. False positives are the opposite of
          actionable.
        References:
          - https://testing.googleblog.com/2024/04/prefer-narrow-assertions-in-unit-tests.html

      - Title: Keep Cause and Effect Clear
        Requirement: &gt;
          The modifications and the assertions of an object's behavior
          in a single test should not be far away from each other.
        Explanation: &gt;
          Refrain from using large global test data structures shared
          across multiple unit tests. This will allow for clear
          identification of each test's setup and the cause and effect.
        References:
          - https://testing.googleblog.com/2017/01/testing-on-toilet-keep-cause-and-effect.html

  - Topic: Data Presence
    Description: &gt;
      The following items describe tests that need to be done for testing
      the presence of data.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...

  - Topic: Data Quality
    Description: &gt;
      The following items describe tests that need to be done for testing
      the quality of data.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...

  - Topic: Data Ingestion
    Description: &gt;
      The following items describe tests that need to be done for testing
      if the data is ingestion properly.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...

  - Topic: Model Fitting
    Description: &gt;
      The following items describe tests that need to be done for testing
      the model fitting process.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...

  - Topic: Model Evaluation
    Description: &gt;
      The following items describe tests that need to be done for testing
      the model evaluation process.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...

  - Topic: Artifact Testing
    Description: &gt;
      The following items describe tests that need to be done for testing
      any artifacts that are created from the project.
    Tests:
      - Title: ...
        Requirement: ...
        Explanation: ...
        References:
          - ...</code></pre>
<section id="evaluation-artifacts" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-artifacts">Evaluation Artifacts</h3>
<p>The end goal of our product is to generate the following three artifacts in relation to the evaluation of a given ML system codebase:</p>
<ol type="1">
<li><p><strong>ML Test Completeness Score</strong>: The application utilizes LLMs and our curated checklist to analyze users’ ML system source code and returns a comprehensive score of the system’s test quality.</p></li>
<li><p><strong>Missing Test Recommendations</strong>: The application evaluates the adequacy of existing tests for users’ ML code and offers recommendations for additional, system-specific tests to enhance testing effectiveness.</p></li>
<li><p><strong>Test Function Specification Generation</strong>: Users select desired test recommendations and prompt the application to generate test function specifications and references. These are reliable starting points for users to enrich the ML system test suites.</p></li>
</ol>
</section>
<section id="success-metrics" class="level3">
<h3 class="anchored" data-anchor-id="success-metrics">Success Metrics</h3>
<p>Our product’s success will depend on mutation testing of the test functions developed based on our application-generated specifications. The evaluation metric is the success rate of detecting the perturbations introduced to the ML project code.</p>
<p>Our partners and stakeholders expect a significant improvement in the testing suites of their ML systems post-application usage. As a result, the testing suites will demonstrate high accuracy in detecting faults, ensuring consistency and high quality of ML projects during updates.</p>
</section>
<section id="data-science-approach" class="level3">
<h3 class="anchored" data-anchor-id="data-science-approach">Data Science Approach</h3>
<section id="data-github-repositories" class="level4">
<h4 class="anchored" data-anchor-id="data-github-repositories">Data: GitHub Repositories</h4>
<p>In this project, GitHub repositories are our data.</p>
<p>To develop our testing checklist, we will collect 11 repositories studied in <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span>. Additionally, we will collect 377 repositories identified in the study by <span class="citation" data-cites="wattanakriengkrai2022github">(<a href="#ref-wattanakriengkrai2022github" role="doc-biblioref">Wattanakriengkrai et al. 2022</a>)</span> for our product development.</p>
<p>For each repository, we are interested in the metadata and the ML modeling- and test-related source code. The metadata will be retrieved using the GitHub API, while the source code will be downloaded and filtered using our custom scripts. To ensure the relevance of the repositories to our study, we will apply the following criteria for filtering: 1. Repositories that are related to ML systems. 2. Repositories that include test cases. 3. Repositories whose development is written in the Python programming language.</p>
</section>
<section id="methodologies" class="level4">
<h4 class="anchored" data-anchor-id="methodologies">Methodologies</h4>
<p>Our data science methodology incorporates human expert evaluation and prompt engineering to assess and enhance the test quality of ML systems.</p>
<ul>
<li><p>Human Expert Evaluation</p>
<p>We will begin by formulating a comprehensive checklist for evaluating the data and ML pipeline based on the established testing strategies outlined in <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span> as the foundational framework. Based on the formulated checklist, our team will manually assess the test quality within each repository data. We will refine the checklist to ensure applicability and robustness when testing general ML systems.</p></li>
<li><p>Prompt Engineering</p>
<p>We will engineer the prompts for LLM to incorporate with the ML system code and the curated checklist and to serve various purposes across the three-stage process:</p>
<ol type="1">
<li>Prompts to examine test cases within the ML system source codes and deliver test completeness scores.</li>
<li>Prompts to compare and contrast the existing tests and the checklist and deliver recommendations.</li>
<li>Prompts to generate system-specific test specifications based on user-selected testing recommendations <span class="citation" data-cites="schafer2023empirical">(<a href="#ref-schafer2023empirical" role="doc-biblioref">Schäfer et al. 2023</a>)</span></li>
</ol></li>
</ul>
</section>
<section id="iterative-development-approach" class="level4">
<h4 class="anchored" data-anchor-id="iterative-development-approach">Iterative Development Approach</h4>
<p>We begin by setting up a foundational framework based on the selected GitHub repositories and research on ML testing. The framework might not cover all ML systems or testing practices. Therefore, we adopt an iterative development approach by establishing an open and scalable framework to address these considerations. The application will be continuously refined based on contributors’ insights.</p>
<p>Users are encouraged to interpret the generated artifacts with a grain of salt and recognize the evolving nature of ML system testing practices.</p>
</section>
</section>
</section>
<section id="delivery-timeline" class="level2">
<h2 class="anchored" data-anchor-id="delivery-timeline">Delivery Timeline</h2>
<p>Our team follows the timeline below for our product delivery and prioritizes close communication with our partners to ensure that our developments align closely with their expectations.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Timeline</th>
<th>Milestones</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Week 1 (Apr 29 - May 3)</td>
<td>Prepare and Present Initial Proposal. Scrape repository data.</td>
</tr>
<tr class="even">
<td>Week 2 - 3 (May 6 - 17)</td>
<td>Deliver Proposal. Deliver Draft of ML Pipeline Test Checklist. Develop Minimum Viable Product (Test Completeness Score, Missing Test Recommendation)</td>
</tr>
<tr class="odd">
<td>Week 4 - 5 (May 20 - May 31)</td>
<td>Update Test Checklist. Develop Test Function Specification Generator.</td>
</tr>
<tr class="even">
<td>Week 6 (Jun 3 - Jun 7)</td>
<td>Update Test Checklist. Wrap Up Product.</td>
</tr>
<tr class="odd">
<td>Week 7 (Jun 10 - Jun 14)</td>
<td>Finalize Test Checklist. Perform Product System Test. Present Final Product. Prepare Final Product Report.</td>
</tr>
<tr class="even">
<td>Week 8 (Jun 17 - Jun 21)</td>
<td>Deliver Final Product. Deliver Final Product Report.</td>
</tr>
</tbody>
</table>
</section>
<section id="references" class="level2">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-grand2021artificial" class="csl-entry" role="listitem">
Grand-View-Research. 2021. <span>“Artificial Intelligence Market Size, Share &amp; Trends Analysis Report by Solution, by Technology (Deep Learning, Machine Learning), by End-Use, by Region, and Segment Forecasts, 2023 2030.”</span> Grand View Research San Francisco.
</div>
<div id="ref-kapoor2022leakage" class="csl-entry" role="listitem">
Kapoor, Sayash, and Arvind Narayanan. 2022. <span>“Leakage and the Reproducibility Crisis in ML-Based Science.”</span> <em>arXiv Preprint arXiv:2207.07048</em>.
</div>
<div id="ref-openja2023studying" class="csl-entry" role="listitem">
Openja, Moses, Foutse Khomh, Armstrong Foundjem, Zhen Ming, Mouna Abidi, Ahmed E Hassan, et al. 2023. <span>“Studying the Practices of Testing Machine Learning Software in the Wild.”</span> <em>arXiv Preprint arXiv:2312.12604</em>.
</div>
<div id="ref-Asheeta2019" class="csl-entry" role="listitem">
Regidi, Asheeta. 2019. <span>“SEBI’s Circular: The Black Box Conundrum and Misrepresentation in AI-Based Mutual Funds.”</span> Firstpost. <a href="https://www.firstpost.com/business/sebis-circular-the-black-box-conundrum-and-misrepresentation-in-ai-based-mutual-funds-6625161.html">https://www.firstpost.com/business/sebis-circular-the-black-box-conundrum-and-misrepresentation-in-ai-based-mutual-funds-6625161.html</a>.
</div>
<div id="ref-schafer2023empirical" class="csl-entry" role="listitem">
Schäfer, Max, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. <span>“An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation.”</span> <em>IEEE Transactions on Software Engineering</em>.
</div>
<div id="ref-wattanakriengkrai2022github" class="csl-entry" role="listitem">
Wattanakriengkrai, Supatsara, Bodin Chinthanet, Hideaki Hata, Raula Gaikovina Kula, Christoph Treude, Jin Guo, and Kenichi Matsumoto. 2022. <span>“GitHub Repositories with Links to Academic Papers: Public Access, Traceability, and Evolution.”</span> <em>Journal of Systems and Software</em> 183: 111117.
</div>
<div id="ref-zhang2023sirens" class="csl-entry" role="listitem">
Zhang, Yue, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, et al. 2023. <span>“Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.”</span> <a href="https://arxiv.org/abs/2309.01219">https://arxiv.org/abs/2309.01219</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>