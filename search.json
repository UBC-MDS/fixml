[
  {
    "objectID": "report/proposal.html",
    "href": "report/proposal.html",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "",
    "text": "by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin"
  },
  {
    "objectID": "report/proposal.html#executive-summary",
    "href": "report/proposal.html#executive-summary",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe rapid growth of global artificial intelligence (AI) markets presents opportunities and challenges. While AI systems have the potential to impact various aspects of human life, ensuring their software quality remains a significant concern. Current testing strategies for machine learning (ML) systems lack standardization and comprehensiveness, which poses risks to stakeholders, such as financial losses and safety hazards.\nOur proposal addresses this challenge by developing a manually curated checklist which contains best practices and recommendations in testing ML systems. Additionally, an end-to-end application incorporating the checklist and Large Language Model (LLM) will be developed to analyze given ML system source codes and provide test completeness evaluation, missing test recommendations, and test function specification generation. Our proposed solution will enable users to systematically assess, improve, and include tests tailored to their ML systems through a combination of human expertise codified within the checklist and parametric memory from LLMs.\nIn the following weeks, we will develop and refine our product through a swift and efficient iterative development approach, with the aim to deliver a rigorously tested and fully-documented system to our partners by the end of the project."
  },
  {
    "objectID": "report/proposal.html#introduction",
    "href": "report/proposal.html#introduction",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Introduction",
    "text": "Introduction\n\nProblem Statement\nThe global artificial intelligence (AI) market is growing exponentially (Grand-View-Research 2021), driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.\nHowever, ensuring the software quality of these systems remains a significant challenge (Openja et al. 2023). Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as substantial financial losses (Regidi 2019) and safety hazards.\nTherefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?\n\n\nOur Objectives\nWe propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate a checklist to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software’s trustworthiness, quality, and reproducibility across both the industry and academia (Kapoor and Narayanan 2022)."
  },
  {
    "objectID": "report/proposal.html#our-product",
    "href": "report/proposal.html#our-product",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Our Product",
    "text": "Our Product\nOur solution offers an end-to-end application for evaluating and enhancing the robustness of users’ ML systems.\n\n\n\nMain components and workflow of the proposed system. The checklist would be written in YAML to maximize readability for both humans and machines. We hope this will encourage researchers/users to read, understand and modify the checklist items, while keeping the checklist closely integrated with other components in our system.\n\n\nOne big challenge in utilizing LLMs to reliably and consistently evaluate ML systems is their tendency to generate illogical and/or factually wrong information known as hallucination (Zhang et al. 2023).\nTo combat this, the proposed system will incorporate a checklist (Fig. 1) which would be curated manually and incorporate best practices in software testing and identified areas to be tested inside ML pipeline from human experts and past research.\nThis checklist will be our basis in evaluating the effectiveness and completeness of existing tests in a given codebase. Relevant information will be injected into a prompt template, which the LLMs would then be prompted to follow the checklist exactly during the evaluation.\nHere is an example of how the proposed checklist would be structured:\n%YAML 1.2\n---\nTitle: Checklist for Tests in Machine Learning Projects\nDescription: &gt;\n  This is a comprehensive checklist for evaluating the data and ML pipeline\n  based on identified testing strategies from experts in the field.\nTest Areas:\n  - Topic: General\n    Description: &gt;\n      The following items describe best practices for all tests to be\n      written.\n    Tests:\n      - Title: Write Descriptive Test Names\n        Requirement: &gt;\n          Every test function should have a clear, descriptive name\n        Explanation: &gt;\n          If out tests are narrow and sufficiently descriptive, the test\n          name itself may give us enough information to start debugging.\n          This also helps us to identify what is being tested inside the\n          function.\n        References:\n          - https://testing.googleblog.com/2014/10/testing-on-toilet-writing-descriptive.html\n          - https://testing.googleblog.com/2024/05/test-failures-should-be-actionable.html\n\n      - Title: Keep Tests Focused\n        Requirement: &gt;\n          Each test should only test one scenario, meaning that in each\n          test we should only use one set of mock data.\n        Explanation: &gt;\n          If we test multiple scenarios in a single test, it is hard to\n          idenitfy exactly what went wrong. Keeping one scenario in a\n          single test helps us to isolate problematic scenarios.\n        References:\n          - https://testing.googleblog.com/2018/06/testing-on-toilet-keep-tests-focused.html\n\n      - Title: Prefer Narrow Assertions in Unit Tests\n        Requirement: &gt;\n          The assertions inside the tests should be narrow, meaning that\n          when checking a complex object, any unrelated behavior should\n          not be tested - Assert on only relevant behaviors.\n        Explanation: &gt;\n          If we have overly wide assertions (such as depending on every\n          field of a complex output proto), the test may fail for many\n          unimportant reasons. False positives are the opposite of\n          actionable.\n        References:\n          - https://testing.googleblog.com/2024/04/prefer-narrow-assertions-in-unit-tests.html\n\n      - Title: Keep Cause and Effect Clear\n        Requirement: &gt;\n          The modifications and the assertions of an object's behavior\n          in a single test should not be far away from each other.\n        Explanation: &gt;\n          Refrain from using large global test data structures shared\n          across multiple unit tests. This will allow for clear\n          identification of each test's setup and the cause and effect.\n        References:\n          - https://testing.googleblog.com/2017/01/testing-on-toilet-keep-cause-and-effect.html\n\n  - Topic: Data Presence\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the presence of data.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Data Quality\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the quality of data.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Data Ingestion\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      if the data is ingestion properly.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Model Fitting\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the model fitting process.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Model Evaluation\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the model evaluation process.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Artifact Testing\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      any artifacts that are created from the project.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\nEvaluation Artifacts\nThe end goal of our product is to generate the following three artifacts in relation to the evaluation of a given ML system codebase:\n\nML Test Completeness Score: The application utilizes LLMs and our curated checklist to analyze users’ ML system source code and returns a comprehensive score of the system’s test quality.\nMissing Test Recommendations: The application evaluates the adequacy of existing tests for users’ ML code and offers recommendations for additional, system-specific tests to enhance testing effectiveness.\nTest Function Specification Generation: Users select desired test recommendations and prompt the application to generate test function specifications and references. These are reliable starting points for users to enrich the ML system test suites.\n\n\n\nSuccess Metrics\nOur product’s success will depend on mutation testing of the test functions developed based on our application-generated specifications. The evaluation metric is the success rate of detecting the perturbations introduced to the ML project code.\nOur partners and stakeholders expect a significant improvement in the testing suites of their ML systems post-application usage. As a result, the testing suites will demonstrate high accuracy in detecting faults, ensuring consistency and high quality of ML projects during updates.\n\n\nData Science Approach\n\nData: GitHub Repositories\nIn this project, GitHub repositories are our data.\nTo develop our testing checklist, we will collect 11 repositories studied in (Openja et al. 2023). Additionally, we will collect 377 repositories identified in the study by (Wattanakriengkrai et al. 2022) for our product development.\nFor each repository, we are interested in the metadata and the ML modeling- and test-related source code. The metadata will be retrieved using the GitHub API, while the source code will be downloaded and filtered using our custom scripts. To ensure the relevance of the repositories to our study, we will apply the following criteria for filtering: 1. Repositories that are related to ML systems. 2. Repositories that include test cases. 3. Repositories whose development is written in the Python programming language.\n\n\nMethodologies\nOur data science methodology incorporates human expert evaluation and prompt engineering to assess and enhance the test quality of ML systems.\n\nHuman Expert Evaluation\nWe will begin by formulating a comprehensive checklist for evaluating the data and ML pipeline based on the established testing strategies outlined in (Openja et al. 2023) as the foundational framework. Based on the formulated checklist, our team will manually assess the test quality within each repository data. We will refine the checklist to ensure applicability and robustness when testing general ML systems.\nPrompt Engineering\nWe will engineer the prompts for LLM to incorporate with the ML system code and the curated checklist and to serve various purposes across the three-stage process:\n\nPrompts to examine test cases within the ML system source codes and deliver test completeness scores.\nPrompts to compare and contrast the existing tests and the checklist and deliver recommendations.\nPrompts to generate system-specific test specifications based on user-selected testing recommendations (Schäfer et al. 2023)\n\n\n\n\nIterative Development Approach\nWe begin by setting up a foundational framework based on the selected GitHub repositories and research on ML testing. The framework might not cover all ML systems or testing practices. Therefore, we adopt an iterative development approach by establishing an open and scalable framework to address these considerations. The application will be continuously refined based on contributors’ insights.\nUsers are encouraged to interpret the generated artifacts with a grain of salt and recognize the evolving nature of ML system testing practices."
  },
  {
    "objectID": "report/proposal.html#delivery-timeline",
    "href": "report/proposal.html#delivery-timeline",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Delivery Timeline",
    "text": "Delivery Timeline\nOur team follows the timeline below for our product delivery and prioritizes close communication with our partners to ensure that our developments align closely with their expectations.\n\n\n\n\n\n\n\nTimeline\nMilestones\n\n\n\n\nWeek 1 (Apr 29 - May 3)\nPrepare and Present Initial Proposal. Scrape repository data.\n\n\nWeek 2 - 3 (May 6 - 17)\nDeliver Proposal. Deliver Draft of ML Pipeline Test Checklist. Develop Minimum Viable Product (Test Completeness Score, Missing Test Recommendation)\n\n\nWeek 4 - 5 (May 20 - May 31)\nUpdate Test Checklist. Develop Test Function Specification Generator.\n\n\nWeek 6 (Jun 3 - Jun 7)\nUpdate Test Checklist. Wrap Up Product.\n\n\nWeek 7 (Jun 10 - Jun 14)\nFinalize Test Checklist. Perform Product System Test. Present Final Product. Prepare Final Product Report.\n\n\nWeek 8 (Jun 17 - Jun 21)\nDeliver Final Product. Deliver Final Product Report."
  },
  {
    "objectID": "report/final_report.html",
    "href": "report/final_report.html",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "",
    "text": "by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin"
  },
  {
    "objectID": "report/final_report.html#executive-summary",
    "href": "report/final_report.html#executive-summary",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe global AI market is expanding rapidly and demanding for robust quality assurance for ML systems to prevent risks such as misinformation, social bias, financial losses, and safety hazards. FixML addresses these challenges by offering an automated code review tool embedded with best practices for ML test suites, curated from ML research and industry standards.\nOur approach includes developing the tool in Python package based on Large Language Models (LLMs) and creating comprehensive checklists to enhance ML software’s trustworthiness, quality, and reproducibility. The tool analyzes ML projects, compares test suites against best practices, and delivers evaluations, which can significantly reduce the time and effort required for manual assessments.\nTo ensure reliability, we defined two success metrics: accuracy (comparison with human expert judgments) and consistency (standard deviation across multiple runs). Our findings indicated that while our tool is effective, there is room to improce in both metrics, which requires further prompt engineering and refinement for enhanced performance.\nThe FixML package is available on PyPI and can be used as a CLI tool and a high-level API, which makes it user-friendly and versatile. Future improvements will focus on specialized checklists, enhanced evaluators, customized test specifications, workflow and performance optimization to further improve ML system quality and user experience."
  },
  {
    "objectID": "report/final_report.html#introduction",
    "href": "report/final_report.html#introduction",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Introduction",
    "text": "Introduction\n\nProblem Statement\nThe global artificial intelligence (AI) market is growing exponentially (Grand-View-Research 2021), driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.\nHowever, ensuring the software quality of these systems remains a significant challenge (Openja et al. 2023). Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as misinformation (Belanger 2024), social bias (Nunwick 2023), substantial financial losses (Regidi 2019) and safety hazards (Shepardson 2023)\nTherefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?\n\n\nOur Objectives\nWe propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate checklists based on ML research papers and best practices to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software’s trustworthiness, quality, and reproducibility across both the industry and academia (Kapoor and Narayanan 2022)."
  },
  {
    "objectID": "report/final_report.html#data-science-methods",
    "href": "report/final_report.html#data-science-methods",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Data Science Methods",
    "text": "Data Science Methods\n\nCurrent Approaches\nTo ensure the reproducibility, trustworthiness, and lack of bias in ML systems, comprehensive testing is essential. We outlined some traditional approaches for assessing the completeness of ML system tests with their advantages and drawbacks as follows.\n\nCode Coverage\n\nCode coverage measures the proportion of source code of a program executed when a particular test suite is run. Widely used in software development, it quantifies test quality and is scalable due to its short processing time. However, it cannot indicate the reasons or specific ML areas where the test suites fall short under the context of ML system development.\n\nManual Evaluation\n\nManual evaluation involves human experts reviewing the source code, whom can take the business logic into considerations and identify vulnerabilites. It often provides context-specific improvement suggestions and remains one of the most reliable practices (Openja et al. 2023), (Alexander et al. 2023). However, it is time-consuming and not scalable due to the scarcity of human experts. Moreover, different experts might put emphasis on different ML test areas and lack a comprehensive and holistic review of the ML system test suites.\n\n\nOur Approach\nOur approach is to deliver an automated code review tool with the best practices of ML test suites embedded. This tool aims to educate ML users on best practices while providing comprehensive evaluations of their ML system codes.\nTo establish these best practices, we utilized data from ML research papers and recognized online resources. In collaboration with our partner, we researched industrial best practices (Team 2023), (Jordan 2020) and academic literature (Openja et al. 2023), and consolidated testing strategies into a human-readable and machine-friendly checklist that can be embedded into the automated tool.\nFor development, we collected 11 GitHub repositories of ML projects as studied in (Openja et al. 2023). These Python-based projects include comprehensive test suites. Our tool should be able to analyze these test suites, compare them with embedded best practices, and deliver evaluations.\nWe expect that our approach will provide scalable and reliable test suite evaluations for multiple ML projects. However, we recognize that our current best practices only focus on a few high-priority test areas due to time constraints. We plan to expand this scope in the future. While our tool’s evaluations are not yet as reliable as human evaluations, we will quantify its performance.\n\n\nSuccess Metrics\nTo properly assess the performance of our tool which leverages LLMs capability, we have taken reference of the methods in (Alexander et al. 2023) and defined two success metrics: accuracy and consistency. These metrics will help users (researchers, ML engineers, etc.) gauge the trustworthiness of our tool’s evaluation results.\n\nAccuracy vs Human Expert Judgement\n\nWe run our tool on ML projects from (Openja et al. 2023) to obtain evaluation results for each ML checklist item. These results are then compared with our manually assessed ground truth data based on the same criteria. Accuracy is calculated as the proportion of matching results to the total number of results.\n\nConsistency\n\nWe perform multiple runs on each ML project to obtain evaluation results for each checklist item. Consistency is measured by calculating the standard deviation of these results across multiple runs for each project."
  },
  {
    "objectID": "report/final_report.html#data-product-results",
    "href": "report/final_report.html#data-product-results",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Data Product & Results",
    "text": "Data Product & Results\n\nData Products\nOur solution includes a curated checklist for robust ML testing and a Python package for checklist-based evaluation of ML project testing robustness using LLMs. The package is publicly available on the Python Packaging Index (PyPI).\nJustifications for these products are:\n\nChecklists have been shown to reduce errors in software systems and promote code submissions (Gawande 2010), (Pineau et al. 2021).\nPython is widely used in ML, compatible with various OSes, and integrates well with LLMs. These ensure the ease of use and development.\n\n\nHow to use the product\nThere are two ways to make use of this package:\n\nAs a CLI tool. A runnable command fixml is provided by the package. Once installed, users can perform codebase evaluations, generate test function specifications, and more by running subcommands under fixml in the terminal.\nAs a high-level API. Users can import necessary components from the package into their own systems. Documentation is available through docstrings.\n\nBy offering it as both CLI tool and API, our product is user-friendly to interact with, and versatile to support various use cases such as web application development and scientific research.\n\n\nSystem Design\n(FIXME To be revised)\n\n\n\n\n\n\n\nFigure 1: Diagram of FixML system design\n\n\nThe design of our package follows object-oriented and SOLID principles, which is fully modularity. Users can easily switch between different prompts, models, and checklists, which facilitates code reusability and collaboration to extend its functionality.\nThere are five components in the system of our package:\n\nCode Analyzer\n\nIt extracts test suites from the input codebase, to ensure only the most relevants details are provided to LLMs given token limits.\n\nPrompt Templates\n\nIt stores prompt templates for instructing LLMs to generate responses in the expected format.\n\nChecklist\n\nIt reads the curated checklist from a CSV file into a dictionary with a fixed schema for LLM injection. The package includes a default checklist for distribution.\n\nRunners\n\nIt includes the Evaluator module, which assesses each test suite file using LLMs and outputs evaluation results, and the Generator module, which creates test specifications. Both modules feature validation, retry logic, and record response and relevant information.\n\nParsers\n\nIt reads the report templates and converts Evaluator’s responses into evaluation reports in various formats (QMD, HTML, PDF) using the Jinja template engine, which enables customizable report structures.\n\n\nChecklist Design\nThe embedded checklist contains best practices for testing ML pipelines, and is curated from ML research and recognized online resources. Prompt engineering is applied to further improve the LLM performance. This helps mitigate LLM hallucinations (Zhang et al. 2023) by ensuring strict adherence to the checklist.\n\n\nTable 1: Structure of the checklist in CSV format. Users can easily modify and expand the checklist by adding new rows to the CSV file.\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nID\nUnique Identifier of the checklist item\n\n\nTopic\nTest Area of the checklist item\n\n\nTitle\nTitle of the checklist item\n\n\nRequirement\nPrompt for the checklist item to be injected into LLMs for evaluation\n\n\nExplanations\nDetailed explanations for human understanding\n\n\nReference\nReferences for the checklist item, e.g., academic papers\n\n\nIs Evaluator Applicable\nIndicates if the checklist item is used during evaluation (0 = No, 1 = Yes)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: An example of the checklist exported in PDF format. Users can easily read and distribute the checklist.\n\n\n\n\nArtifacts\nUsing our package results in three artifacts:\n\nEvaluation Responses\n\nThese responses include both LLM evaluation results and process metadata stored in JSON format. This supports various downsteam tasks, such as report rendering and scientific research, by selectively extracting information.\n\n\n\n\n\n\n\nFigure 3: An example of the evaluation responses. It includes call_results for evaluation outcomes and details about the model, repository, checklist, and the run.\n\n\n\nEvaluation Report\n\nThis report provides a well-structured presentation of evaluation results for ML projects. It includes a summary of the completeness score and a detailed breakdown with explanations for each checklist item score.\n\n\n\n\n\n\n\nFigure 4: An example of the evaluation report exported in PDF format using our default template. Users can customize their reports by creating their own templates.\n\n\n\nTest Specification Script\n\nThese are generated test specifications stored as Python scripts.\n\n\n\n\n\n\n\nFigure 5: An example of the generated test specifications\n\n\n\n\n\nEvaluation Results\nAs described in Success Metrics, we conducted 30 iterations on each repository from (Openja et al. 2023) and examined the breakdown of the completeness score to assess our tool’s evaluation quality.\n\nAccuracy\n\nAccuracy is our primary consideration since our tool’s value depends on its ability to align with expert judgment. We targeted 3 of the repositories for human evaluation: lightfm, qlib, DeepSpeech. We compared and plotted the graph to illustrate how well our tool’s outputs align with the ground truth.\n\n\nCode\nimport pandas as pd\ngt = pd.read_csv('../data/processed/ground_truth.csv')\ngt\n\n\n\n\n\n\n\nTable 2: Ground truth scores for 3 repositories per checklist item based on human evaluation. (1 = fully satisfied, 0.5 = partially satisfied, 0 = not satisfied)\n\n\n\nid\ntitle\nDeepSpeech\nlightfm\nqlib\n\n\n\n\n0\n2.1\nEnsure Data File Loads as Expected\n0.0\n1.0\n0.5\n\n\n1\n3.2\nData in the Expected Format\n0.0\n1.0\n1.0\n\n\n2\n3.5\nCheck for Duplicate Records in Data\n0.0\n0.0\n0.0\n\n\n3\n4.2\nVerify Data Split Proportion\n0.0\n1.0\n0.5\n\n\n4\n5.3\nEnsure Model Output Shape Aligns with Expectation\n0.0\n0.5\n1.0\n\n\n5\n6.1\nVerify Evaluation Metrics Implementation\n0.0\n1.0\n1.0\n\n\n6\n6.2\nEvaluate Model's Performance Against Thresholds\n0.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\nimport pandas as pd\n\ndf_repo__stat = pd.read_csv('../data/processed/score_stat_by_repo_3.5-turbo.csv')\ngt = pd.read_csv('../data/processed/ground_truth.csv')\ngt = gt.melt(id_vars=['id', 'title'], var_name='repo', value_name='ground_truth')\n\ndf_repo__stat_with_gt = df_repo__stat.merge(gt, on=['id', 'title', 'repo'])\n\nbase = alt.Chart(\n    df_repo__stat_with_gt.query('repo in [\"lightfm\", \"qlib\", \"DeepSpeech\"]')\n).transform_calculate(\n    min=\"max(0, datum.mean-datum.std)\",\n    max=\"min(1, datum.mean+datum.std)\"\n)\n    \n# generate the points\npoints = base.mark_point(\n    filled=True,\n    size=50,\n    color='black'\n).encode(\n    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title(\"Score\").axis(\n        labelExpr=\"datum.value % 0.5 ? null : datum.label\"\n    ),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))#.scale(domainMin=0, domainMax=1).title('Score'),\n)\n\n# generate the points for ground truth\ngt_points = base.mark_point(\n    filled=True,\n    size=200,\n    color='green',\n    shape=\"diamond\"\n).encode(\n    x=alt.X('ground_truth:Q'),\n    y=alt.Y('id_title:N')\n)\n\n# generate the error bars\nerrorbars = base.mark_errorbar().encode(\n    x=alt.X(\"min:Q\").title('1 SD'), #\"id:N\",\n    x2=\"max:Q\",\n    y=\"id_title:N\"\n)\n\n(gt_points + points + errorbars).facet(\n    column=alt.Column('repo:N').title(None)\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n)\n\n\n\n\n\n\n\nFigure 6: Analysis of accuracy of the scores per checklist item. The black dot and line represent the mean and standard deviation of scores from the tool, while the green diamond represents the ground truth score for a single repository. It shows our tool tends to underrate satisfactory cases.\n\n\n\nWhen examining accuracy, we observed that our tool effectively identifies non-satisfying cases. However, it often classifies fully satisfied items as partially satisfied and partially satisfied items as not satisfied.\nThis indicates that our tool achieves a certain degree of accuracy. The next questions we consider are:\n\nAre there other factors that impact the performance of our tool?\nIn what direction can we improve our tool?\n\n\nConsistency\n\nConsistency is another consideration because it directly impacts the reliability of the evaluation results from user perspective. Given that LLM-generated completeness scores contain some randomness, we plotted the uncertainty of these scores across checklist items and repositories to show how consistent these results were.\n\n\nCode\nstds = df_repo__stat[['repo', 'std', 'id_title']].pivot(index='repo', columns='id_title').copy()\nstds.columns = [col[1] for col in stds.columns]\nstds = stds.reset_index()\nstds = stds.melt(id_vars='repo', var_name='id_title')\n\nbase = alt.Chart(stds)\n\nbox = base.mark_boxplot(\n    color='grey',\n    opacity=0.5,\n    size=20,\n).encode(\n    x=alt.X('value:Q').title('Standard Deviation of Scores'),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))\n)\n\nstripplot = base.mark_circle(size=100).encode(\n    y=alt.Y( \n        'id_title:N',\n        axis=alt.Axis(ticks=False, grid=True, labels=True), \n        scale=alt.Scale(), \n    ), \n    x='value:Q',\n    yOffset=\"jitter:Q\",\n    color=alt.Color('id_title:N', legend=None),\n    tooltip='repo'\n).transform_calculate(\n    # Generate Gaussian jitter with a Box-Muller transform\n    jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\"\n)\n\n(\n    box + stripplot\n).configure_view( \n    stroke=None\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n).properties(\n    height=300, \n    width=600,\n) \n\n\n\n\n\n\n\nFigure 7: Analysis of the uncertainty of scores (measured in standard deviation on a scale of 0 to 1) per checklist item. Each dot represents the uncertainty of scores from 30 runs of a single repository. It shows different patterns across checklist items.\n\n\n\nWhen we examined the consistency, we observed various patterns and sought to identify potential causes, which could provide ideas for improvements. We identified two categories with diverging patterns:\n\nHigh Uncertainty\n\nItems like 6.1 Verify Evaluation Metrics Implementation showed high standard deviations across repositories (median = 0.12). This might suggest potential issues with prompt quality for the LLM to produce consistent results, which could be mitigated through improved prompt engineering.\n\nOutliers with High Uncertainty\n\nItems like 2.1 Ensure Data File Loads as Expected had outliers with exceptionally high standard deviations, which is possibly due to unorthodox repositories. A careful manual examination is required for a more definitive conclusion.\n\nComparison among LLMs\nTo evaluate if newer LLMs improve performance, we obtained outputs from gpt-4o and gpt-4-turbo on the lightfm repository and plotted the graph to compare how our tool performs in terms of accuracy and consistency when switched the LLMs.\nBy comparing with the results from gpt-3.5-turbo (shown in Figure 6), we observed an increase in consistency using newer LLMs given the smaller standard deviations. However, we found that gpt-4o returned “Satisfied” for all items, while gpt-4-turbo deviated more from the ground truth for item 3.5 Check for Duplicate Records in Data, 5.3 Ensure Model Output Shape Aligns with Expectation compared to gpt-3.5-turbo.\n\n\nCode\ndf_repo_4o__stat = pd.read_csv('../data/processed/score_stat_by_repo_4o.csv')\ndf_repo_4o__stat_with_gt = df_repo_4o__stat.merge(gt, on=['id', 'title', 'repo'])\ndf_repo_4o__stat_with_gt['model'] = 'gpt-4o'\n\ndf_repo_4turbo__stat = pd.read_csv('../data/processed/score_stat_by_repo_4-turbo.csv')\ndf_repo_4turbo__stat_with_gt = df_repo_4turbo__stat.merge(gt, on=['id', 'title', 'repo'])\ndf_repo_4turbo__stat_with_gt['model'] = 'gpt-4-turbo'\n\ndf_model_comp = pd.concat(\n    (df_repo_4turbo__stat_with_gt, df_repo_4o__stat_with_gt), \n    axis=0\n)\n\nbase = alt.Chart(\n    df_model_comp\n).transform_calculate(\n    min=\"max(0, datum.mean-datum.std)\",\n    max=\"min(1, datum.mean+datum.std)\"\n)\n    \n# generate the points\npoints = base.mark_point(\n    filled=True,\n    size=50,\n    color='black'\n).encode(\n    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title(\"Score\").axis(\n        labelExpr=\"datum.value % 0.5 ? null : datum.label\"\n    ),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False)),\n)\n\n# generate the points for ground truth\ngt_points = base.mark_point(\n    filled=True,\n    size=200,\n    color='green',\n    shape=\"diamond\"\n).encode(\n    x=alt.X('ground_truth:Q'),\n    y=alt.Y('id_title:N')\n)\n\n# generate the error bars\nerrorbars = base.mark_errorbar().encode(\n    x=alt.X(\"min:Q\").title('1 SD'),\n    x2=\"max:Q\",\n    y=\"id_title:N\"\n)\n\n(\n    gt_points + points + errorbars\n).facet(\n    column=alt.Column('model:N').title(None)\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n)\n\n\n\n\n\n\n\nFigure 8: Analysis of the scores per checklist item on repository lightfm using different GPT versions. The black dot and line represent the mean and standard deviation of scores from the tool, while the green diamond represents the ground truth score.\n\n\n\nThe graph suggests a potential improvement in consistency and/or accuracy when switching to newer LLMs. However, it also indicates that what works well with the current LLM may not necessarily perform well with newer models. This implies the need for exploration in different structures, such as prompt engineering for gpt-4-turbo."
  },
  {
    "objectID": "report/final_report.html#conclusion",
    "href": "report/final_report.html#conclusion",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\nWrap Up\nThe development of FixML has been driven by the need of better quality assurance in ML systems and the current limitations of traditional testing methods on ML projects. FixML provides curated checklists and automated tools that enhance the evaluation and creation of test suites for ML projects. This in return, significantly reduces the time and effort required to assess the completeness of ML test suites, and thus promotes thorough and efficient assessment on ML projects.\n\n\nLimitation & Future Improvement\nWhile FixML provides substantial benefits, there are limitations and areas to be addressed in future development:\n\nSpecialized Checklist\n\nAlthough the default checklist is general and may not cover all requirements for different ML projects, the current checklist structure allows users to edit checklist easily (shown in Table 1). Future development will focus on creating specialized checklists for tailored evaluations across various domains and project types. Collaboration with ML researchers is welcomed for creating specialized checklists based on specific use cases.\n\nEnhanced Test Evaluator\n\nAs shown in session Evaluation Results, there are potential accuracy and consistency issues on the evaluation results using OpenAI gpt-3.5-turbo model (shown in Figure 6, Figure 7). Future improvements involves better prompt engineering techniques and support for multiple LLMs for enhanced performance and flexibility. User guidelines in prompt creation will be provided to facilitate collaboration with ML developers.\n\nCustomized Test Specification\n\nAs shown in Figure 5, the current generator produces general test function skeletons without project-specific details. Future developments will integrate project-specific information to produce customized test function skeletons. This may further encourage users to create comprehensive tests.\n\nFurther Optimization\n\nThe cost associated with LLM usage is an important consideration for users of our tool. Future improvements will include sharing our cost data and providing calculations for estimated costs (e.g. cost per line of code). This will help users estimate their expenses and conduct a cost-benefit analysis to make informed decisions when using our tool.\nBy addressing these limitations and implementing future improvements, we aim for FixML to achieve better performance and contribute to the development of better ML systems, and ultimately enhance human life."
  }
]