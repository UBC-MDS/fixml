<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>FixML - Checklists and LLM prompts for efficient and effective test creation in data analysis - Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../report/final_report.html">Capstone Final Report</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../img/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../report/final_report.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Capstone Final Report</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../report/proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Proposal</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement">Problem Statement</a></li>
  <li><a href="#our-objectives" id="toc-our-objectives" class="nav-link" data-scroll-target="#our-objectives">Our Objectives</a></li>
  </ul></li>
  <li><a href="#data-science-methods" id="toc-data-science-methods" class="nav-link" data-scroll-target="#data-science-methods">Data Science Methods</a>
  <ul class="collapse">
  <li><a href="#current-approaches" id="toc-current-approaches" class="nav-link" data-scroll-target="#current-approaches">Current Approaches</a></li>
  <li><a href="#our-approach" id="toc-our-approach" class="nav-link" data-scroll-target="#our-approach">Our Approach</a></li>
  <li><a href="#success-metrics" id="toc-success-metrics" class="nav-link" data-scroll-target="#success-metrics">Success Metrics</a></li>
  </ul></li>
  <li><a href="#data-product-results" id="toc-data-product-results" class="nav-link" data-scroll-target="#data-product-results">Data Product &amp; Results</a>
  <ul class="collapse">
  <li><a href="#data-products" id="toc-data-products" class="nav-link" data-scroll-target="#data-products">Data Products</a></li>
  <li><a href="#evaluation-results" id="toc-evaluation-results" class="nav-link" data-scroll-target="#evaluation-results">Evaluation Results</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#limitation-future-improvement" id="toc-limitation-future-improvement" class="nav-link" data-scroll-target="#limitation-future-improvement">Limitation &amp; Future Improvement</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin</p>
<section id="executive-summary" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p>The global artificial intelligence (AI) market is expanding rapidly, with demand for robust quality assurance for Machine Learning (ML) systems, to prevent risks such as misinformation, social bias, financial losses, and safety hazards. FixML addresses these challenges by offering an automated code review tool embedded with best practices for ML test suites, curated from ML research and industry standards.</p>
<p>Our approach includes developing the tool called FixML, a Python package based on large language models (LLMs), and creating comprehensive checklists to enhance ML software’s trustworthiness, quality, and reproducibility. The tool analyzes ML projects, compares test suites against best practices, and delivers evaluations and test specifications. This functionality can significantly reduce the time and effort required for manual assessments.</p>
<p>We defined two success metrics to ensure reliability: accuracy (comparison with human expert judgments) and consistency (standard deviation across multiple runs). Our findings indicated that while our tool is effective, there is room to improve in both metrics, by further prompt engineering and refinement for enhanced performance.</p>
<p>The FixML package is available on PyPI and can be used as a user-friendly and versatile CLI tool and a high-level API. Future improvements will focus on specialized checklists, enhanced evaluators, customized test specifications, and other optimizations to improve ML system quality and user experience.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<section id="problem-statement" class="level3">
<h3 class="anchored" data-anchor-id="problem-statement">Problem Statement</h3>
<p>The global AI market is growing exponentially <span class="citation" data-cites="grand2021artificial">(<a href="#ref-grand2021artificial" role="doc-biblioref">Grand-View-Research 2021</a>)</span>, driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis. However, ensuring the software quality of these systems remains a significant challenge <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span>. Specifically, the need for a standardized and comprehensive approach to testing ML systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as misinformation <span class="citation" data-cites="Ashley2024">(<a href="#ref-Ashley2024" role="doc-biblioref">Belanger 2024</a>)</span>, social bias <span class="citation" data-cites="Alice2023">(<a href="#ref-Alice2023" role="doc-biblioref">Nunwick 2023</a>)</span>, substantial financial losses <span class="citation" data-cites="Asheeta2019">(<a href="#ref-Asheeta2019" role="doc-biblioref">Regidi 2019</a>)</span>, and safety hazards <span class="citation" data-cites="David2023">(<a href="#ref-David2023" role="doc-biblioref">Shepardson 2023</a>)</span>. Therefore, it is crucial to define and promote an industry standard and establish robust testing methodologies for these systems. But how?</p>
</section>
<section id="our-objectives" class="level3">
<h3 class="anchored" data-anchor-id="our-objectives">Our Objectives</h3>
<p>We propose to develop testing suite diagnostic tools based on LLMs and curate checklists based on ML research papers and best practices to facilitate comprehensive testing of ML systems with flexibility. We aim to enhance applied ML software’s trustworthiness, quality, and reproducibility across the industry and academia <span class="citation" data-cites="kapoor2022leakage">(<a href="#ref-kapoor2022leakage" role="doc-biblioref">Kapoor and Narayanan 2022</a>)</span>.</p>
</section>
</section>
<section id="data-science-methods" class="level2">
<h2 class="anchored" data-anchor-id="data-science-methods">Data Science Methods</h2>
<section id="current-approaches" class="level3">
<h3 class="anchored" data-anchor-id="current-approaches">Current Approaches</h3>
<p>Comprehensive testing is essential to ensure the reproducibility, trustworthiness, and lack of bias in ML systems. We outlined some traditional approaches for assessing the completeness of ML system tests and their advantages and drawbacks.</p>
<ol type="1">
<li><strong>Code Coverage</strong></li>
</ol>
<p>Code coverage measures the proportion of program source code executed when running a test suite. It is widely used in software development to quantify test quality and is scalable due to its short processing time. However, code coverage cannot indicate the reasons or specific ML areas where the test suites fall short in the context of ML system development.</p>
<ol start="2" type="1">
<li><strong>Manual Evaluation</strong></li>
</ol>
<p>Manual evaluation involves human experts reviewing the source code, who can consider the business logic and identify vulnerabilities. It often provides context-specific improvement suggestions and remains one of the most reliable practices <span class="citation" data-cites="openja2023studying alexander2023evaluating">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>; <a href="#ref-alexander2023evaluating" role="doc-biblioref">Alexander et al. 2023</a>)</span>. However, manual evaluation is time-consuming and not scalable due to the scarcity of human experts. Moreover, experts may emphasize different ML test areas, and may not have a comprehensive and holistic review of the ML system test suites.</p>
</section>
<section id="our-approach" class="level3">
<h3 class="anchored" data-anchor-id="our-approach">Our Approach</h3>
<p>Our approach is to deliver an automated code review tool embedded with the best practices of ML test suites. This tool educates ML users on best practices while comprehensively evaluating their ML system codes.</p>
<p>We established these best practices using data from ML research papers and well-recognized online resources. In collaboration with our partner, we researched industrial best practices <span class="citation" data-cites="msise2023 jordan2020">(<a href="#ref-msise2023" role="doc-biblioref">Team 2023</a>; <a href="#ref-jordan2020" role="doc-biblioref">Jordan 2020</a>)</span> and academic literature <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span>, and consolidated testing strategies into a human-readable and machine-friendly checklist that can be embedded into the automated tool.</p>
<p>We collected 11 GitHub repositories of ML projects for development as studied in <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span>. These Python-based projects include comprehensive test suites. Our tool should be able to analyze these test suites, compare them with embedded best practices, and deliver evaluations.</p>
<p>Our approach will provide scalable and reliable test suite evaluations for multiple ML projects. However, due to time constraints, we recognize that our current best practices only focus on a few high-priority test areas. We plan to expand this scope in the future. While our tool’s evaluations are less reliable than human evaluations, we will quantify its performance.</p>
</section>
<section id="success-metrics" class="level3">
<h3 class="anchored" data-anchor-id="success-metrics">Success Metrics</h3>
<p>To assess the performance of our tool, which leverages LLMs capability, we have referred to the methods in <span class="citation" data-cites="alexander2023evaluating">(<a href="#ref-alexander2023evaluating" role="doc-biblioref">Alexander et al. 2023</a>)</span> and defined two success metrics: accuracy and consistency. These metrics will help users (researchers, ML engineers, etc.) gauge the trustworthiness of our tool’s evaluation results.</p>
<ol type="1">
<li><strong>Accuracy vs Human Expert Judgement</strong></li>
</ol>
<p>We run our tool on ML projects from <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span> to obtain evaluation results for each ML checklist item. These results are then compared with our manually assessed ground truth data based on the same criteria. Accuracy is the proportion of matching results to the total number of results.</p>
<ol start="2" type="1">
<li><strong>Consistency</strong></li>
</ol>
<p>We perform multiple runs on each ML project to obtain evaluation results for each checklist item. Consistency is measured by calculating the standard deviation of these results across multiple runs for each project.</p>
</section>
</section>
<section id="data-product-results" class="level2">
<h2 class="anchored" data-anchor-id="data-product-results">Data Product &amp; Results</h2>
<section id="data-products" class="level3">
<h3 class="anchored" data-anchor-id="data-products">Data Products</h3>
<p>Our solution includes a curated checklist for robust ML testing and FixML, a Python package for checklist-based evaluation of ML projects’ testing robustness using LLMs. The package is publicly available on the Python Packaging Index (PyPI).</p>
<p>Justifications for these products are:</p>
<ul>
<li>Checklists have been shown to reduce errors in software systems and promote code submissions <span class="citation" data-cites="Atul2010 pineau2021improving">(<a href="#ref-Atul2010" role="doc-biblioref">Gawande 2010</a>; <a href="#ref-pineau2021improving" role="doc-biblioref">Pineau et al. 2021</a>)</span>.</li>
<li>Python is widely used in ML, compatible with various OSes, and integrates well with LLMs. These ensure the ease of use and development.</li>
</ul>
<section id="how-to-use-fixml" class="level4">
<h4 class="anchored" data-anchor-id="how-to-use-fixml">How to use FixML</h4>
<p>There are two ways to make use of FixML:</p>
<ol type="1">
<li><p><strong>As a CLI tool.</strong> The FixML package provides a runnable command <code>fixml</code>. Once installed, users can perform codebase evaluations, generate test function specifications, and more by running subcommands under <code>fixml</code> in the terminal.</p></li>
<li><p><strong>As a high-level API.</strong> Users can import necessary components from the FixML package into their systems. Documentation is available through docstrings.</p></li>
</ol>
<p>By offering FixML as a CLI tool and API, our product is user-friendly and versatile enough to support various use cases, such as web application development and scientific research.</p>
</section>
<section id="system-design" class="level4">
<h4 class="anchored" data-anchor-id="system-design">System Design</h4>
<div id="fig-system" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../img/proposed_system_overview.png" class="lightbox img-fluid figure-img" width="600"></p>
</figure>
</div>
<figcaption class="figure-caption">Figure&nbsp;1: A diagram showing the high-level overview of FixML system design. The oval-shaped items are the various components, each with a different responsibility in the pipeline. While the square-shaped items denote the data and artifacts ingested, processed, and produced by the system at different parts of the pipeline. The arrows show the flow of data from outside the system, i.e., the codebase, then processed into code snippets and attached to the constructed prompt with checklist and prompt templates, and finally transformed into run results.</figcaption>
</figure>
</div>
<p>Our package’s design follows object-oriented and SOLID principles and is fully modular. Users can easily switch between different prompts, models, and checklists. This design facilitates code reusability and collaboration to extend its functionality.</p>
<p>There are five components in the system of our package:</p>
<ol type="1">
<li><strong>Code Analyzer</strong></li>
</ol>
<p>FixML extracts test suites from the input codebase to ensure only the most relevant details are provided to LLMs given token limits.</p>
<ol start="2" type="1">
<li><strong>Prompt Templates</strong></li>
</ol>
<p>FixML stores prompt templates for instructing LLMs to generate responses in the expected format.</p>
<ol start="3" type="1">
<li><strong>Checklist</strong></li>
</ol>
<p>FixML reads the curated checklist from a CSV file into a dictionary with a fixed schema for LLM injection. The package includes a default checklist for distribution.</p>
<ol start="4" type="1">
<li><strong>Runners</strong></li>
</ol>
<p>FixML includes the Evaluator module, which assesses each test suite file using LLMs and outputs evaluation results, and the Generator module, which creates test specifications. Both modules feature validation, retry logic, and record response and relevant information.</p>
<ol start="5" type="1">
<li><strong>Parsers</strong></li>
</ol>
<p>FixML reads the report templates and converts the Evaluator’s responses into evaluation reports in various formats (QMD, HTML, PDF) using the Jinja template engine, which enables customizable report structures.</p>
</section>
<section id="checklist-design" class="level4">
<h4 class="anchored" data-anchor-id="checklist-design">Checklist Design</h4>
<p>The embedded checklist contains best practices for testing ML pipelines and is curated from ML research and recognized online resources. Prompt engineering is applied to further improve LLM performance and mitigate LLM hallucinations <span class="citation" data-cites="zhang2023sirens">(<a href="#ref-zhang2023sirens" role="doc-biblioref">Zhang et al. 2023</a>)</span> by ensuring strict adherence to the checklist.</p>
<div id="tbl-checklist-schema" class="anchored">
<table class="table">
<caption>Table&nbsp;1: The checklist’s structure is in CSV format. Users can easily modify and expand the checklist by adding new rows to the CSV file.</caption>
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">Column</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">ID</td>
<td style="text-align: left;">Unique Identifier of the checklist item</td>
</tr>
<tr class="even">
<td style="text-align: right;">Topic</td>
<td style="text-align: left;">Test Area of the checklist item</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Title</td>
<td style="text-align: left;">Title of the checklist item</td>
</tr>
<tr class="even">
<td style="text-align: right;">Requirement</td>
<td style="text-align: left;">Prompt for the checklist item to be injected into LLMs for evaluation</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Explanations</td>
<td style="text-align: left;">Detailed explanations for human understanding</td>
</tr>
<tr class="even">
<td style="text-align: right;">Reference</td>
<td style="text-align: left;">References for the checklist item, e.g.&nbsp;academic papers</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Is Evaluator Applicable</td>
<td style="text-align: left;">Indicates if the checklist item is used during evaluation (0 = No, 1 = Yes)</td>
</tr>
</tbody>
</table>
</div>
<div id="fig-checklist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../img/checklist_sample.png" class="lightbox img-fluid figure-img" width="600"></p>
</figure>
</div>
<figcaption class="figure-caption">Figure&nbsp;2: An example of the checklist exported in PDF format. Users can easily read and distribute the checklist.</figcaption>
</figure>
</div>
</section>
<section id="artifacts" class="level4">
<h4 class="anchored" data-anchor-id="artifacts">Artifacts</h4>
<p>Using our package results in three artifacts:</p>
<ol type="1">
<li><strong>Evaluation Responses</strong></li>
</ol>
<p>The evaluation responses include LLM evaluation results and process metadata stored in JSON format. These responses support various downstream tasks, such as report rendering and scientific research, by selectively extracting information.</p>
<div id="fig-responses" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../img/test_evaluation_responses_sample.png" class="lightbox img-fluid figure-img" width="600"></p>
</figure>
</div>
<figcaption class="figure-caption">Figure&nbsp;3: An example of the evaluation responses, which includes <code>call_results</code> for evaluation outcomes and details about the <code>model</code>, <code>repository</code>, <code>checklist</code>, and the run.</figcaption>
</figure>
</div>
<ol start="2" type="1">
<li><strong>Evaluation Report</strong></li>
</ol>
<p>The evaluation report provides a well-structured presentation of evaluation results for ML projects. It includes a summary of the completeness score and a detailed breakdown explaining each checklist item score.</p>
<div id="fig-report" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../img/test_evaluation_report_sample.png" class="lightbox img-fluid figure-img" width="600"></p>
</figure>
</div>
<figcaption class="figure-caption">Figure&nbsp;4: An example of the evaluation report exported in PDF format using our default template. Users can customize their reports by creating their own templates.</figcaption>
</figure>
</div>
<ol start="3" type="1">
<li><strong>Test Specification Script</strong></li>
</ol>
<p>The test specification script is a generated Python script, storing the specification of the test function corresponding to each checklist item.</p>
<div id="fig-testspec" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="../img/test_spec_sample.png" class="lightbox img-fluid figure-img" width="600"></p>
</figure>
</div>
<figcaption class="figure-caption">Figure&nbsp;5: An example of the generated test specifications.</figcaption>
</figure>
</div>
</section>
</section>
<section id="evaluation-results" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-results">Evaluation Results</h3>
<p>As described in <code>Success Metrics</code>, we conducted 30 iterations on each repository from <span class="citation" data-cites="openja2023studying">(<a href="#ref-openja2023studying" role="doc-biblioref">Openja et al. 2023</a>)</span> and examined the breakdown of the completeness score to assess our tool’s evaluation quality.</p>
<ol type="1">
<li><strong>Accuracy</strong></li>
</ol>
<p>Accuracy is our primary consideration since our tool’s value depends on its ability to align with expert judgment. We targeted 3 of the repositories for human evaluation: <a href="https://github.com/lyst/lightfm"><code>lightfm</code></a>, <a href="https://github.com/microsoft/qlib"><code>qlib</code></a>, <a href="https://github.com/mozilla/DeepSpeech"><code>DeepSpeech</code></a>. We compared and plotted the graph to illustrate how well our tool’s outputs align with the ground truth.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>gt <span class="op">=</span> pd.read_csv(<span class="st">'../data/processed/ground_truth.csv'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>gt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<div id="tbl-gt" class="anchored">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table&nbsp;2: Ground truth scores for 3 repositories per checklist item based on human evaluation. (1 = fully satisfied, 0.5 = partially satisfied, 0 = not satisfied)</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">DeepSpeech</th>
<th data-quarto-table-cell-role="th">lightfm</th>
<th data-quarto-table-cell-role="th">qlib</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.1</td>
<td>Ensure Data File Loads as Expected</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3.2</td>
<td>Data in the Expected Format</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3.5</td>
<td>Check for Duplicate Records in Data</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.2</td>
<td>Verify Data Split Proportion</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.3</td>
<td>Ensure Model Output Shape Aligns with Expectation</td>
<td>0.0</td>
<td>0.5</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>6.1</td>
<td>Verify Evaluation Metrics Implementation</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>6.2</td>
<td>Evaluate Model's Performance Against Thresholds</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div>

</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> altair <span class="im">as</span> alt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df_repo__stat <span class="op">=</span> pd.read_csv(<span class="st">'../data/processed/score_stat_by_repo_3.5-turbo.csv'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>gt <span class="op">=</span> pd.read_csv(<span class="st">'../data/processed/ground_truth.csv'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>gt <span class="op">=</span> gt.melt(id_vars<span class="op">=</span>[<span class="st">'id'</span>, <span class="st">'title'</span>], var_name<span class="op">=</span><span class="st">'repo'</span>, value_name<span class="op">=</span><span class="st">'ground_truth'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>df_repo__stat_with_gt <span class="op">=</span> df_repo__stat.merge(gt, on<span class="op">=</span>[<span class="st">'id'</span>, <span class="st">'title'</span>, <span class="st">'repo'</span>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> alt.Chart(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    df_repo__stat_with_gt.query(<span class="st">'repo in ["lightfm", "qlib", "DeepSpeech"]'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>).transform_calculate(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">min</span><span class="op">=</span><span class="st">"max(0, datum.mean-datum.std)"</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">max</span><span class="op">=</span><span class="st">"min(1, datum.mean+datum.std)"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the points</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> base.mark_point(</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'black'</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>).encode(</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'mean:Q'</span>).scale(domainMin<span class="op">=</span><span class="dv">0</span>, domainMax<span class="op">=</span><span class="dv">1</span>).title(<span class="st">"Score"</span>).axis(</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        labelExpr<span class="op">=</span><span class="st">"datum.value % 0.5 ? null : datum.label"</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'id_title:N'</span>, title<span class="op">=</span><span class="va">None</span>, axis<span class="op">=</span>alt.Axis(labelPadding<span class="op">=</span><span class="dv">10</span>, labelLimit<span class="op">=</span><span class="dv">1000</span>, grid<span class="op">=</span><span class="va">False</span>))<span class="co">#.scale(domainMin=0, domainMax=1).title('Score'),</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the points for ground truth</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>gt_points <span class="op">=</span> base.mark_point(</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span><span class="st">"diamond"</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>).encode(</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'ground_truth:Q'</span>),</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'id_title:N'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the error bars</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>errorbars <span class="op">=</span> base.mark_errorbar().encode(</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">"min:Q"</span>).title(<span class="st">'1 SD'</span>), <span class="co">#"id:N",</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">=</span><span class="st">"max:Q"</span>,</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"id_title:N"</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>(gt_points <span class="op">+</span> points <span class="op">+</span> errorbars).facet(</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span>alt.Column(<span class="st">'repo:N'</span>).title(<span class="va">None</span>)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>).configure_axis( </span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    labelFontSize<span class="op">=</span><span class="dv">12</span>, </span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    titleFontSize<span class="op">=</span><span class="dv">12</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-accu-mean-sd-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

<style>
  #altair-viz-45d72c07099c44a6a16f346bed395bf7.vega-embed {
    width: 100%;
    display: flex;
  }

  #altair-viz-45d72c07099c44a6a16f346bed395bf7.vega-embed details,
  #altair-viz-45d72c07099c44a6a16f346bed395bf7.vega-embed details summary {
    position: relative;
  }
</style>
<div id="altair-viz-45d72c07099c44a6a16f346bed395bf7"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-45d72c07099c44a6a16f346bed395bf7") {
      outputDiv = document.getElementById("altair-viz-45d72c07099c44a6a16f346bed395bf7");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm/vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm/vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm/vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "5.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}, "axis": {"labelFontSize": 12, "titleFontSize": 12}}, "data": {"name": "data-a00fed27dd81291d0f791ce5bf77eeda"}, "facet": {"column": {"field": "repo", "title": null, "type": "nominal"}}, "spec": {"layer": [{"mark": {"type": "point", "color": "green", "filled": true, "shape": "diamond", "size": 200}, "encoding": {"x": {"field": "ground_truth", "type": "quantitative"}, "y": {"field": "id_title", "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}, {"mark": {"type": "point", "color": "black", "filled": true, "size": 50}, "encoding": {"x": {"axis": {"labelExpr": "datum.value % 0.5 ? null : datum.label"}, "field": "mean", "scale": {"domainMin": 0, "domainMax": 1}, "title": "Score", "type": "quantitative"}, "y": {"axis": {"grid": false, "labelLimit": 1000, "labelPadding": 10}, "field": "id_title", "title": null, "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}, {"mark": {"type": "errorbar"}, "encoding": {"x": {"field": "min", "title": "1 SD", "type": "quantitative"}, "x2": {"field": "max"}, "y": {"field": "id_title", "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}]}, "$schema": "https://vega.github.io/schema/vega-lite/v5.17.0.json", "datasets": {"data-a00fed27dd81291d0f791ce5bf77eeda": [{"repo": "DeepSpeech", "id": 2.1, "count": 30.0, "mean": 0.9666666666666668, "std": 0.1825741858350553, "title": "Ensure Data File Loads as Expected", "id_title": "2.1. Ensure Data File Loads as Expected", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 3.2, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Data in the Expected Format", "id_title": "3.2. Data in the Expected Format", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 3.5, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Check for Duplicate Records in Data", "id_title": "3.5. Check for Duplicate Records in Data", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 4.2, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Verify Data Split Proportion", "id_title": "4.2. Verify Data Split Proportion", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 5.3, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Ensure Model Output Shape Aligns with Expectation", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 6.1, "count": 30.0, "mean": 0.2, "std": 0.4068381021724861, "title": "Verify Evaluation Metrics Implementation", "id_title": "6.1. Verify Evaluation Metrics Implementation", "ground_truth": 0.0}, {"repo": "DeepSpeech", "id": 6.2, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Evaluate Model's Performance Against Thresholds", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "ground_truth": 0.0}, {"repo": "lightfm", "id": 2.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Ensure Data File Loads as Expected", "id_title": "2.1. Ensure Data File Loads as Expected", "ground_truth": 1.0}, {"repo": "lightfm", "id": 3.2, "count": 30.0, "mean": 0.5, "std": 0.0, "title": "Data in the Expected Format", "id_title": "3.2. Data in the Expected Format", "ground_truth": 1.0}, {"repo": "lightfm", "id": 3.5, "count": 30.0, "mean": 0.0, "std": 0.0, "title": "Check for Duplicate Records in Data", "id_title": "3.5. Check for Duplicate Records in Data", "ground_truth": 0.0}, {"repo": "lightfm", "id": 4.2, "count": 30.0, "mean": 0.8166666666666667, "std": 0.2450662589267805, "title": "Verify Data Split Proportion", "id_title": "4.2. Verify Data Split Proportion", "ground_truth": 1.0}, {"repo": "lightfm", "id": 5.3, "count": 30.0, "mean": 0.4833333333333333, "std": 0.0912870929175276, "title": "Ensure Model Output Shape Aligns with Expectation", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "ground_truth": 0.5}, {"repo": "lightfm", "id": 6.1, "count": 30.0, "mean": 0.9166666666666666, "std": 0.1895245108947258, "title": "Verify Evaluation Metrics Implementation", "id_title": "6.1. Verify Evaluation Metrics Implementation", "ground_truth": 1.0}, {"repo": "lightfm", "id": 6.2, "count": 30.0, "mean": 0.9833333333333332, "std": 0.0912870929175276, "title": "Evaluate Model's Performance Against Thresholds", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "ground_truth": 1.0}, {"repo": "qlib", "id": 2.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Ensure Data File Loads as Expected", "id_title": "2.1. Ensure Data File Loads as Expected", "ground_truth": 0.5}, {"repo": "qlib", "id": 3.2, "count": 30.0, "mean": 0.7666666666666667, "std": 0.2537081317024624, "title": "Data in the Expected Format", "id_title": "3.2. Data in the Expected Format", "ground_truth": 1.0}, {"repo": "qlib", "id": 3.5, "count": 30.0, "mean": 0.1166666666666666, "std": 0.2150915335760381, "title": "Check for Duplicate Records in Data", "id_title": "3.5. Check for Duplicate Records in Data", "ground_truth": 0.0}, {"repo": "qlib", "id": 4.2, "count": 30.0, "mean": 0.4833333333333333, "std": 0.2069204966986668, "title": "Verify Data Split Proportion", "id_title": "4.2. Verify Data Split Proportion", "ground_truth": 0.5}, {"repo": "qlib", "id": 5.3, "count": 30.0, "mean": 0.55, "std": 0.2012889499682243, "title": "Ensure Model Output Shape Aligns with Expectation", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "ground_truth": 1.0}, {"repo": "qlib", "id": 6.1, "count": 30.0, "mean": 0.6333333333333333, "std": 0.2916461404928373, "title": "Verify Evaluation Metrics Implementation", "id_title": "6.1. Verify Evaluation Metrics Implementation", "ground_truth": 1.0}, {"repo": "qlib", "id": 6.2, "count": 30.0, "mean": 0.6, "std": 0.203419051086243, "title": "Evaluate Model's Performance Against Thresholds", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "ground_truth": 1.0}]}}, {"mode": "vega-lite"});
</script>
<figcaption class="figure-caption">Figure&nbsp;6: Analysis of the accuracy of the scores per checklist item. The black dot and line represent the mean and standard deviation of scores from the tool, while the green diamond represents the ground truth score for a single repository. The result shows that our tool tends to underrate satisfactory cases.</figcaption>
</figure>
</div>
</div>
<p>When examining accuracy, we observed that our tool effectively identifies non-satisfying cases. However, our tool often classifies fully satisfied items as partially satisfied and partially satisfied items as not satisfied. This observation indicates that our tool achieves a certain degree of accuracy. The following questions we consider are:</p>
<ul>
<li>Are there other factors that impact the performance of our tool?</li>
<li>In what direction can we improve our tool?</li>
</ul>
<ol start="2" type="1">
<li><strong>Consistency</strong></li>
</ol>
<p>Consistency is another consideration because it directly impacts the reliability of the evaluation results from a user’s perspective. Given that LLM-generated completeness scores contain some randomness, we plotted the uncertainty of these scores across checklist items and repositories to show how consistent these results were.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> df_repo__stat[[<span class="st">'repo'</span>, <span class="st">'std'</span>, <span class="st">'id_title'</span>]].pivot(index<span class="op">=</span><span class="st">'repo'</span>, columns<span class="op">=</span><span class="st">'id_title'</span>).copy()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>stds.columns <span class="op">=</span> [col[<span class="dv">1</span>] <span class="cf">for</span> col <span class="kw">in</span> stds.columns]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> stds.reset_index()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> stds.melt(id_vars<span class="op">=</span><span class="st">'repo'</span>, var_name<span class="op">=</span><span class="st">'id_title'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> alt.Chart(stds)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>box <span class="op">=</span> base.mark_boxplot(</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'grey'</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    opacity<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>).encode(</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'value:Q'</span>).title(<span class="st">'Standard Deviation of Scores'</span>),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'id_title:N'</span>, title<span class="op">=</span><span class="va">None</span>, axis<span class="op">=</span>alt.Axis(labelPadding<span class="op">=</span><span class="dv">10</span>, labelLimit<span class="op">=</span><span class="dv">1000</span>, grid<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>stripplot <span class="op">=</span> base.mark_circle(size<span class="op">=</span><span class="dv">100</span>).encode(</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y( </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'id_title:N'</span>,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span>alt.Axis(ticks<span class="op">=</span><span class="va">False</span>, grid<span class="op">=</span><span class="va">True</span>, labels<span class="op">=</span><span class="va">True</span>), </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        scale<span class="op">=</span>alt.Scale(), </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'value:Q'</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    yOffset<span class="op">=</span><span class="st">"jitter:Q"</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>alt.Color(<span class="st">'id_title:N'</span>, legend<span class="op">=</span><span class="va">None</span>),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span><span class="st">'repo'</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>).transform_calculate(</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate Gaussian jitter with a Box-Muller transform</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    jitter<span class="op">=</span><span class="st">"sqrt(-2*log(random()))*cos(2*PI*random())"</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    box <span class="op">+</span> stripplot</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>).configure_view( </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    stroke<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>).configure_axis( </span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    labelFontSize<span class="op">=</span><span class="dv">12</span>, </span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    titleFontSize<span class="op">=</span><span class="dv">12</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>).properties(</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">600</span>,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-cons-sd-box-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

<style>
  #altair-viz-e1f09a4c4dba447da04913c98b16f98c.vega-embed {
    width: 100%;
    display: flex;
  }

  #altair-viz-e1f09a4c4dba447da04913c98b16f98c.vega-embed details,
  #altair-viz-e1f09a4c4dba447da04913c98b16f98c.vega-embed details summary {
    position: relative;
  }
</style>
<div id="altair-viz-e1f09a4c4dba447da04913c98b16f98c"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-e1f09a4c4dba447da04913c98b16f98c") {
      outputDiv = document.getElementById("altair-viz-e1f09a4c4dba447da04913c98b16f98c");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm/vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm/vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm/vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "5.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 300, "continuousHeight": 300, "stroke": null}, "axis": {"labelFontSize": 12, "titleFontSize": 12}}, "layer": [{"mark": {"type": "boxplot", "color": "grey", "opacity": 0.5, "size": 20}, "encoding": {"x": {"field": "value", "title": "Standard Deviation of Scores", "type": "quantitative"}, "y": {"axis": {"grid": false, "labelLimit": 1000, "labelPadding": 10}, "field": "id_title", "title": null, "type": "nominal"}}}, {"mark": {"type": "circle", "size": 100}, "encoding": {"color": {"field": "id_title", "legend": null, "type": "nominal"}, "tooltip": {"field": "repo", "type": "nominal"}, "x": {"field": "value", "type": "quantitative"}, "y": {"axis": {"grid": true, "labels": true, "ticks": false}, "field": "id_title", "scale": {}, "type": "nominal"}, "yOffset": {"field": "jitter", "type": "quantitative"}}, "transform": [{"calculate": "sqrt(-2*log(random()))*cos(2*PI*random())", "as": "jitter"}]}], "data": {"name": "data-6aab5f6156d183a21fdd1c0623d42004"}, "height": 300, "width": 600, "$schema": "https://vega.github.io/schema/vega-lite/v5.17.0.json", "datasets": {"data-6aab5f6156d183a21fdd1c0623d42004": [{"repo": "DeepSpeech", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.1825741858350553}, {"repo": "apollo", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "deepchem", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "lightfm", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "magenta", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0912870929175276}, {"repo": "mmf", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "mycroft-core", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "nanodet", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "nupic", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.409653624363343}, {"repo": "paperless-ng", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "qlib", "id_title": "2.1. Ensure Data File Loads as Expected", "value": 0.0}, {"repo": "DeepSpeech", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "apollo", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "deepchem", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "lightfm", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "magenta", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "mmf", "id_title": "3.2. Data in the Expected Format", "value": 0.2330457998496995}, {"repo": "mycroft-core", "id_title": "3.2. Data in the Expected Format", "value": 0.0912870929175276}, {"repo": "nanodet", "id_title": "3.2. Data in the Expected Format", "value": 0.2542738138578039}, {"repo": "nupic", "id_title": "3.2. Data in the Expected Format", "value": 0.2248882225544018}, {"repo": "paperless-ng", "id_title": "3.2. Data in the Expected Format", "value": 0.0}, {"repo": "qlib", "id_title": "3.2. Data in the Expected Format", "value": 0.2537081317024624}, {"repo": "DeepSpeech", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "apollo", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "deepchem", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.1268540658512312}, {"repo": "lightfm", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "magenta", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "mmf", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.2537081317024624}, {"repo": "mycroft-core", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "nanodet", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "nupic", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "paperless-ng", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.0}, {"repo": "qlib", "id_title": "3.5. Check for Duplicate Records in Data", "value": 0.2150915335760381}, {"repo": "DeepSpeech", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "apollo", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "deepchem", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "lightfm", "id_title": "4.2. Verify Data Split Proportion", "value": 0.2450662589267805}, {"repo": "magenta", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "mmf", "id_title": "4.2. Verify Data Split Proportion", "value": 0.1728729518208802}, {"repo": "mycroft-core", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "nanodet", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "nupic", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "paperless-ng", "id_title": "4.2. Verify Data Split Proportion", "value": 0.0}, {"repo": "qlib", "id_title": "4.2. Verify Data Split Proportion", "value": 0.2069204966986668}, {"repo": "DeepSpeech", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "apollo", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "deepchem", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "lightfm", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0912870929175276}, {"repo": "magenta", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "mmf", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.2491364395612199}, {"repo": "mycroft-core", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0912870929175276}, {"repo": "nanodet", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "nupic", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.1895245108947258}, {"repo": "paperless-ng", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.0}, {"repo": "qlib", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "value": 0.2012889499682243}, {"repo": "DeepSpeech", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.4068381021724861}, {"repo": "apollo", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.1268540658512312}, {"repo": "deepchem", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.0}, {"repo": "lightfm", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.1895245108947258}, {"repo": "magenta", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.0}, {"repo": "mmf", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.1525642883146823}, {"repo": "mycroft-core", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.0912870929175276}, {"repo": "nanodet", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.2491364395612199}, {"repo": "nupic", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.0}, {"repo": "paperless-ng", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.0}, {"repo": "qlib", "id_title": "6.1. Verify Evaluation Metrics Implementation", "value": 0.2916461404928373}, {"repo": "DeepSpeech", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0}, {"repo": "apollo", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0}, {"repo": "deepchem", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0}, {"repo": "lightfm", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0912870929175276}, {"repo": "magenta", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0}, {"repo": "mmf", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.2248882225544018}, {"repo": "mycroft-core", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.1268540658512312}, {"repo": "nanodet", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.1268540658512312}, {"repo": "nupic", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.3511066249289032}, {"repo": "paperless-ng", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.0}, {"repo": "qlib", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "value": 0.203419051086243}]}}, {"mode": "vega-lite"});
</script>
<figcaption class="figure-caption">Figure&nbsp;7: Analysis of the uncertainty of scores (measured in standard deviation on a scale of 0 to 1) per checklist item. Each dot represents the uncertainty of scores from 30 runs of a single repository. The analysis shows different patterns across checklist items.</figcaption>
</figure>
</div>
</div>
<p>When we examined the consistency, we observed various patterns and sought to identify potential causes, which could provide ideas for improvements. We identified two categories with diverging patterns:</p>
<ol type="i">
<li><strong>High Uncertainty</strong></li>
</ol>
<p>Items like <code>6.1 Verify Evaluation Metrics Implementation</code> showed high standard deviations across repositories (median = 0.12). These high standard deviations might suggest potential issues with prompt quality for the LLM to produce consistent results. The issues could be mitigated through improved prompt engineering.</p>
<ol start="2" type="i">
<li><strong>Outliers with High Uncertainty</strong></li>
</ol>
<p>Items like <code>2.1 Ensure Data File Loads as Expected</code> had outliers with exceptionally high standard deviations, possibly due to unorthodox repositories. A careful manual examination is required for a more definitive conclusion.</p>
<section id="comparison-among-llms" class="level4">
<h4 class="anchored" data-anchor-id="comparison-among-llms">Comparison among LLMs</h4>
<p>To evaluate if newer LLMs improve performance, we obtained outputs from <code>gpt-4o</code> and <code>gpt-4-turbo</code> on the <code>lightfm</code> repository. We plotted the graph to compare how our tool performs in terms of accuracy and consistency when switching the LLMs.</p>
<p>By comparing with the results from <code>gpt-3.5-turbo</code> (shown in <a href="#fig-accu-mean-sd-plot">Figure&nbsp;6</a>), we observed an increase in consistency using newer LLMs given the smaller standard deviations. However, we found that <code>gpt-4o</code> returned “Satisfied” for all items. At the same time, <code>gpt-4-turbo</code> deviated more from the ground truth for items <code>3.5 Check for Duplicate Records in Data</code>, <code>5.3 Ensure Model Output Shape Aligns with Expectation</code> compared to <code>gpt-3.5-turbo</code>.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df_repo_4o__stat <span class="op">=</span> pd.read_csv(<span class="st">'../data/processed/score_stat_by_repo_4o.csv'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>df_repo_4o__stat_with_gt <span class="op">=</span> df_repo_4o__stat.merge(gt, on<span class="op">=</span>[<span class="st">'id'</span>, <span class="st">'title'</span>, <span class="st">'repo'</span>])</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>df_repo_4o__stat_with_gt[<span class="st">'model'</span>] <span class="op">=</span> <span class="st">'gpt-4o'</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>df_repo_4turbo__stat <span class="op">=</span> pd.read_csv(<span class="st">'../data/processed/score_stat_by_repo_4-turbo.csv'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>df_repo_4turbo__stat_with_gt <span class="op">=</span> df_repo_4turbo__stat.merge(gt, on<span class="op">=</span>[<span class="st">'id'</span>, <span class="st">'title'</span>, <span class="st">'repo'</span>])</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>df_repo_4turbo__stat_with_gt[<span class="st">'model'</span>] <span class="op">=</span> <span class="st">'gpt-4-turbo'</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>df_model_comp <span class="op">=</span> pd.concat(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    (df_repo_4turbo__stat_with_gt, df_repo_4o__stat_with_gt), </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    axis<span class="op">=</span><span class="dv">0</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> alt.Chart(</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    df_model_comp</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>).transform_calculate(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">min</span><span class="op">=</span><span class="st">"max(0, datum.mean-datum.std)"</span>,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">max</span><span class="op">=</span><span class="st">"min(1, datum.mean+datum.std)"</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the points</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> base.mark_point(</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'black'</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>).encode(</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'mean:Q'</span>).scale(domainMin<span class="op">=</span><span class="dv">0</span>, domainMax<span class="op">=</span><span class="dv">1</span>).title(<span class="st">"Score"</span>).axis(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        labelExpr<span class="op">=</span><span class="st">"datum.value % 0.5 ? null : datum.label"</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'id_title:N'</span>, title<span class="op">=</span><span class="va">None</span>, axis<span class="op">=</span>alt.Axis(labelPadding<span class="op">=</span><span class="dv">10</span>, labelLimit<span class="op">=</span><span class="dv">1000</span>, grid<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the points for ground truth</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>gt_points <span class="op">=</span> base.mark_point(</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span><span class="st">"diamond"</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>).encode(</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'ground_truth:Q'</span>),</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'id_title:N'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the error bars</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>errorbars <span class="op">=</span> base.mark_errorbar().encode(</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">"min:Q"</span>).title(<span class="st">'1 SD'</span>),</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">=</span><span class="st">"max:Q"</span>,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"id_title:N"</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    gt_points <span class="op">+</span> points <span class="op">+</span> errorbars</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>).facet(</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span>alt.Column(<span class="st">'model:N'</span>).title(<span class="va">None</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>).configure_axis( </span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    labelFontSize<span class="op">=</span><span class="dv">12</span>, </span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    titleFontSize<span class="op">=</span><span class="dv">12</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-llm-mean-sd-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

<style>
  #altair-viz-c789b889bd754d6abb0965cf721e24ae.vega-embed {
    width: 100%;
    display: flex;
  }

  #altair-viz-c789b889bd754d6abb0965cf721e24ae.vega-embed details,
  #altair-viz-c789b889bd754d6abb0965cf721e24ae.vega-embed details summary {
    position: relative;
  }
</style>
<div id="altair-viz-c789b889bd754d6abb0965cf721e24ae"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-c789b889bd754d6abb0965cf721e24ae") {
      outputDiv = document.getElementById("altair-viz-c789b889bd754d6abb0965cf721e24ae");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm/vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm/vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm/vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "5.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}, "axis": {"labelFontSize": 12, "titleFontSize": 12}}, "data": {"name": "data-72007bf239d9a1281a3190e2987a51ce"}, "facet": {"column": {"field": "model", "title": null, "type": "nominal"}}, "spec": {"layer": [{"mark": {"type": "point", "color": "green", "filled": true, "shape": "diamond", "size": 200}, "encoding": {"x": {"field": "ground_truth", "type": "quantitative"}, "y": {"field": "id_title", "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}, {"mark": {"type": "point", "color": "black", "filled": true, "size": 50}, "encoding": {"x": {"axis": {"labelExpr": "datum.value % 0.5 ? null : datum.label"}, "field": "mean", "scale": {"domainMin": 0, "domainMax": 1}, "title": "Score", "type": "quantitative"}, "y": {"axis": {"grid": false, "labelLimit": 1000, "labelPadding": 10}, "field": "id_title", "title": null, "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}, {"mark": {"type": "errorbar"}, "encoding": {"x": {"field": "min", "title": "1 SD", "type": "quantitative"}, "x2": {"field": "max"}, "y": {"field": "id_title", "type": "nominal"}}, "transform": [{"calculate": "max(0, datum.mean-datum.std)", "as": "min"}, {"calculate": "min(1, datum.mean+datum.std)", "as": "max"}]}]}, "$schema": "https://vega.github.io/schema/vega-lite/v5.17.0.json", "datasets": {"data-72007bf239d9a1281a3190e2987a51ce": [{"repo": "lightfm", "id": 2.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Ensure Data File Loads as Expected", "id_title": "2.1. Ensure Data File Loads as Expected", "ground_truth": 1.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 3.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Data in the Expected Format", "id_title": "3.2. Data in the Expected Format", "ground_truth": 1.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 3.5, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Check for Duplicate Records in Data", "id_title": "3.5. Check for Duplicate Records in Data", "ground_truth": 0.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 4.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Verify Data Split Proportion", "id_title": "4.2. Verify Data Split Proportion", "ground_truth": 1.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 5.3, "count": 30.0, "mean": 0.6166666666666667, "std": 0.2150915335760382, "title": "Ensure Model Output Shape Aligns with Expectation", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "ground_truth": 0.5, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 6.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Verify Evaluation Metrics Implementation", "id_title": "6.1. Verify Evaluation Metrics Implementation", "ground_truth": 1.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 6.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Evaluate Model's Performance Against Thresholds", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "ground_truth": 1.0, "model": "gpt-4-turbo"}, {"repo": "lightfm", "id": 2.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Ensure Data File Loads as Expected", "id_title": "2.1. Ensure Data File Loads as Expected", "ground_truth": 1.0, "model": "gpt-4o"}, {"repo": "lightfm", "id": 3.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Data in the Expected Format", "id_title": "3.2. Data in the Expected Format", "ground_truth": 1.0, "model": "gpt-4o"}, {"repo": "lightfm", "id": 3.5, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Check for Duplicate Records in Data", "id_title": "3.5. Check for Duplicate Records in Data", "ground_truth": 0.0, "model": "gpt-4o"}, {"repo": "lightfm", "id": 4.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Verify Data Split Proportion", "id_title": "4.2. Verify Data Split Proportion", "ground_truth": 1.0, "model": "gpt-4o"}, {"repo": "lightfm", "id": 5.3, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Ensure Model Output Shape Aligns with Expectation", "id_title": "5.3. Ensure Model Output Shape Aligns with Expectation", "ground_truth": 0.5, "model": "gpt-4o"}, {"repo": "lightfm", "id": 6.1, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Verify Evaluation Metrics Implementation", "id_title": "6.1. Verify Evaluation Metrics Implementation", "ground_truth": 1.0, "model": "gpt-4o"}, {"repo": "lightfm", "id": 6.2, "count": 30.0, "mean": 1.0, "std": 0.0, "title": "Evaluate Model's Performance Against Thresholds", "id_title": "6.2. Evaluate Model's Performance Against Thresholds", "ground_truth": 1.0, "model": "gpt-4o"}]}}, {"mode": "vega-lite"});
</script>
<figcaption class="figure-caption">Figure&nbsp;8: Analysis of the scores per checklist item on repository <code>lightfm</code> using different GPT versions. The black dot and line represent the mean and standard deviation of scores from the tool, while the green diamond represents the ground truth score.</figcaption>
</figure>
</div>
</div>
<p>The graph suggests a potential consistency and accuracy improvement when switching to newer LLMs. However, the graph also indicates that what works well with the current LLM may not perform well with newer models. The result implies the need to explore different structures, such as prompt engineering for <code>gpt-4-turbo</code>.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The need for better quality assurance in ML systems and the current limitations of traditional testing methods on ML projects has driven the development of FixML. FixML provides curated checklists and automated tools that enhance evaluating and creating test suites for ML projects. FixML significantly reduces the time and effort required to assess the completeness of ML test suites, thus promoting thorough and efficient assessment of ML projects.</p>
<section id="limitation-future-improvement" class="level3">
<h3 class="anchored" data-anchor-id="limitation-future-improvement">Limitation &amp; Future Improvement</h3>
<p>While FixML provides substantial benefits, there are limitations and areas to be addressed in future development:</p>
<ol type="1">
<li><strong>Specialized Checklist</strong></li>
</ol>
<p>Although the default checklist is general and may not cover all requirements for different ML projects, the current checklist structure allows users to edit the checklist easily (shown in <a href="#tbl-checklist-schema">Table&nbsp;1</a>). Future development will focus on creating specialized checklists for tailored evaluations across various domains and project types. Collaboration with ML researchers is welcomed for creating specialized checklists based on specific use cases.</p>
<ol start="2" type="1">
<li><strong>Enhanced Test Evaluator</strong></li>
</ol>
<p>As shown in session <code>Evaluation Results</code>, there are potential accuracy and consistency issues on the evaluation results using the OpenAI <code>gpt-3.5-turbo</code> model (shown in <a href="#fig-accu-mean-sd-plot">Figure&nbsp;6</a>, <a href="#fig-cons-sd-box-plot">Figure&nbsp;7</a>). Future improvements involve better prompt engineering techniques and support for multiple LLMs for enhanced performance and flexibility. User guidelines in prompt creation will be provided to facilitate collaboration with ML developers.</p>
<ol start="3" type="1">
<li><strong>Customized Test Specification</strong></li>
</ol>
<p>As shown in <a href="#fig-testspec">Figure&nbsp;5</a>, the current generator produces general test function skeletons without project-specific details. Future developments will integrate project-specific information to deliver customized test function skeletons, encouraging users to create comprehensive tests.</p>
<ol start="4" type="1">
<li><strong>Further Optimization</strong></li>
</ol>
<p>The cost associated with LLM usage is an essential consideration for users of our tool. Future improvements will include sharing our cost data and calculating estimated costs (e.g., cost per line of code). The cost information will help users assess their expenses and conduct a cost-benefit analysis to make informed decisions using our tool.</p>
<p>By addressing these limitations and implementing future improvements, we aim for FixML to achieve better performance, contribute to developing better ML systems, and ultimately enhance human life.</p>
</section>
</section>
<section id="references" class="level2">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-alexander2023evaluating" class="csl-entry" role="listitem">
Alexander, Rohan, Lindsay Katz, Callandra Moore, and Zane Schwartz. 2023. <span>“Evaluating the Decency and Consistency of Data Validation Tests Generated by LLMs.”</span> <em>arXiv Preprint arXiv:2310.01402</em>.
</div>
<div id="ref-Ashley2024" class="csl-entry" role="listitem">
Belanger, Ashley. 2024. <span>“Air Canada Must Honor Refund Policy Invented by Airline’s Chatbot.”</span> Ars Technica. <a href="https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/">https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/</a>.
</div>
<div id="ref-Atul2010" class="csl-entry" role="listitem">
Gawande, Atul. 2010. <em>Checklist Manifesto, the (HB)</em>. Penguin Books India.
</div>
<div id="ref-grand2021artificial" class="csl-entry" role="listitem">
Grand-View-Research. 2021. <span>“Artificial Intelligence Market Size, Share &amp; Trends Analysis Report by Solution, by Technology (Deep Learning, Machine Learning), by End-Use, by Region, and Segment Forecasts, 2023 2030.”</span> Grand View Research San Francisco.
</div>
<div id="ref-jordan2020" class="csl-entry" role="listitem">
Jordan, Jeremy. 2020. <span>“Effective Testing for Machine Learning Systems.”</span> <a href="https://www.jeremyjordan.me/testing-ml/">https://www.jeremyjordan.me/testing-ml/</a>.
</div>
<div id="ref-kapoor2022leakage" class="csl-entry" role="listitem">
Kapoor, Sayash, and Arvind Narayanan. 2022. <span>“Leakage and the Reproducibility Crisis in ML-Based Science.”</span> <em>arXiv Preprint arXiv:2207.07048</em>.
</div>
<div id="ref-Alice2023" class="csl-entry" role="listitem">
Nunwick, Alice. 2023. <span>“ITutorGroup Settles AI Hiring Lawsuit Alleging Age Discrimination.”</span> Verdict. <a href="https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/">https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/</a>.
</div>
<div id="ref-openja2023studying" class="csl-entry" role="listitem">
Openja, Moses, Foutse Khomh, Armstrong Foundjem, Zhen Ming, Mouna Abidi, Ahmed E Hassan, et al. 2023. <span>“Studying the Practices of Testing Machine Learning Software in the Wild.”</span> <em>arXiv Preprint arXiv:2312.12604</em>.
</div>
<div id="ref-pineau2021improving" class="csl-entry" role="listitem">
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. <span>“Improving Reproducibility in Machine Learning Research (a Report from the Neurips 2019 Reproducibility Program).”</span> <em>Journal of Machine Learning Research</em> 22 (164): 1–20.
</div>
<div id="ref-Asheeta2019" class="csl-entry" role="listitem">
Regidi, Asheeta. 2019. <span>“SEBI’s Circular: The Black Box Conundrum and Misrepresentation in AI-Based Mutual Funds.”</span> Firstpost. <a href="https://www.firstpost.com/business/sebis-circular-the-black-box-conundrum-and-misrepresentation-in-ai-based-mutual-funds-6625161.html">https://www.firstpost.com/business/sebis-circular-the-black-box-conundrum-and-misrepresentation-in-ai-based-mutual-funds-6625161.html</a>.
</div>
<div id="ref-David2023" class="csl-entry" role="listitem">
Shepardson, David. 2023. <span>“GM’s Cruise Recalling 950 Driverless Cars After Pedestrian Dragged in Crash.”</span> Reuters. <a href="https://www.reuters.com/business/autos-transportation/gms-cruise-recall-950-driverless-cars-after-accident-involving-pedestrian-2023-11-08/">https://www.reuters.com/business/autos-transportation/gms-cruise-recall-950-driverless-cars-after-accident-involving-pedestrian-2023-11-08/</a>.
</div>
<div id="ref-msise2023" class="csl-entry" role="listitem">
Team, Microsoft Industry Solutions Engineering. 2023. <span>“Testing Data Science and MLOps Code.”</span> <em>Testing Data Science and MLOps Code - Engineering Fundamentals Playbook</em>. <a href="https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-testing/">https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-testing/</a>.
</div>
<div id="ref-zhang2023sirens" class="csl-entry" role="listitem">
Zhang, Yue, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, et al. 2023. <span>“Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.”</span> <a href="https://arxiv.org/abs/2309.01219">https://arxiv.org/abs/2309.01219</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>