{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb2cc86-28c7-4080-a288-4bbdbb675bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from test_creation.modules.checklist.checklist import Checklist\n",
    "from test_creation.modules.workflow.prompt_format import GenerationPromptFormat\n",
    "from test_creation.modules.workflow.runners.generator import NaiveTestGenerator\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "checklist = Checklist(checklist_path='../checklist/checklist.csv')\n",
    "prompt_format = GenerationPromptFormat()\n",
    "\n",
    "generator = NaiveTestGenerator(llm, prompt_format, checklist=checklist)\n",
    "result = generator.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9571619f-ffe2-45a4-a3fb-fc2d85e254f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_file_loads(data_file: str) -> bool:\n",
      "    \"\"\"Test function to ensure data file loads as expected.\n",
      "\n",
      "    Parameters:\n",
      "    data_file (str): The path to the data file.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if data file loads successfully, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_data_format(data: pd.DataFrame) -> bool:\n",
      "    \"\"\"Test function to verify that the data matches the expected format.\n",
      "\n",
      "    Parameters:\n",
      "    data (pd.DataFrame): The input data to be checked.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if data matches expected format, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_duplicate_records(data: pd.DataFrame) -> bool:\n",
      "    \"\"\"Test function to verify that there are no duplicate records in the loaded data.\n",
      "\n",
      "    Parameters:\n",
      "    data (pd.DataFrame): The input data to check for duplicates.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if no duplicate records found, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_data_split_proportion(train_data: pd.DataFrame, test_data: pd.DataFrame) -> bool:\n",
      "    \"\"\"Test function to check that the data is split into training and testing sets in the expected proportion.\n",
      "\n",
      "    Parameters:\n",
      "    train_data (pd.DataFrame): The training data set.\n",
      "    test_data (pd.DataFrame): The testing data set.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if data split is in expected proportion, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_model_output_shape(model_output: np.ndarray, expected_shape: Tuple[int]) -> bool:\n",
      "    \"\"\"Test function to ensure that the structure of the model's output matches the expected format.\n",
      "\n",
      "    Parameters:\n",
      "    model_output (np.ndarray): The output of the model.\n",
      "    expected_shape (Tuple[int]): The expected shape of the model output.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if model output shape aligns with expectation, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_evaluation_metrics(model_output: np.ndarray, true_labels: np.ndarray) -> bool:\n",
      "    \"\"\"Test function to verify that the evaluation metrics are correctly implemented and appropriate for the model's task.\n",
      "\n",
      "    Parameters:\n",
      "    model_output (np.ndarray): The output of the model.\n",
      "    true_labels (np.ndarray): The true labels for the data.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if evaluation metrics implementation is correct, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "def test_model_performance(train_metrics: Dict[str, float], test_metrics: Dict[str, float], threshold: float) -> bool:\n",
      "    \"\"\"Test function to compute evaluation metrics for both the training and testing datasets and verify performance against thresholds.\n",
      "\n",
      "    Parameters:\n",
      "    train_metrics (Dict[str, float]): Evaluation metrics for the training data.\n",
      "    test_metrics (Dict[str, float]): Evaluation metrics for the testing data.\n",
      "    threshold (float): The threshold value for acceptable model performance.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if model performance meets thresholds, False otherwise.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in result:\n",
    "    print(res['Function'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f42aa-3877-426c-a4b8-d47bef272eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation-111]",
   "language": "python",
   "name": "conda-env-test-creation-111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
