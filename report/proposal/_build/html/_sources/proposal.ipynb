{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis\n",
    "\n",
    "by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The rapid growth of global artificial intelligence (AI) market presents both opportunities and challenges. While AI systems have the potential to impact various aspects of human life, ensuring their software quality remains a significant concern. Current testing strategies for machine learning (ML) systems lack standardization and comprehensiveness, which poses risks to stakeholders such as financial losses and safety hazards.\n",
    "\n",
    "Our proposal addresses this challenge by developing an end-to-end application that provides comprehensive test evaluation, tailored test recommendations, and automated test specifications generation. Through these features, users can systematically assess, improve, and include tests tailored to their ML systems. By leveraging human expertise and prompt engineering, we build our product to deliver actionable insights for improving users' test strategies and overall ML system reliability.\n",
    "\n",
    "With an iterative development approach, we aim to develop our minimum viable product (MVP) in the first three weeks. We will further iterate and refine our product over the next 3 weeks. During the last 2 weeks, we will carry out rigorous system testing, finalize our final product and report, and deliver to our partners. Ultimately, our goal is to mitigate potential negative societal impacts associated with unreliable ML systems and promote trustworthiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Global artificial intelligence (AI) market is growing exponentially {cite}`grand2021artificial`, which is driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis, etc. \n",
    "\n",
    "However, ensuring the software quality of these systems remains a significant challenge {cite}`openja2023studying`. Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as substantial financial losses {cite}`Asheeta2019` and safety hazards. \n",
    "\n",
    "Therefore, it is crucial to define and promote an industry standard and establish robust testing methodologies for these systems. But how?\n",
    "\n",
    "### Our Objectives\n",
    "\n",
    "We propose to develop testing suites diagnostic tool based on Large Language Models (LLMs) and curate checklist, to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance the trustworthiness, quality and reproducibility of applied ML softwares across both the industry and academia {cite}`kapoor2022leakage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Product\n",
    "\n",
    "Our solution offers an end-to-end application for evaluating and enhancing the robustness of users' ML systems.\n",
    "\n",
    "Here is a diagram serving as a high-level overview of our proposed system:\n",
    "\n",
    "![](../../img/proposed_system_overview.png)\n",
    "\n",
    "### Description\n",
    "\n",
    "Our product facilitates a three-stage process:\n",
    "\n",
    "1. **ML Test Completeness Score**: The application utilizes LLMs and our curated checklist to analyze users' ML system source code, and returns a comprehensive score of the system's test quality.\n",
    "  \n",
    "2. **Missing Test Recommendations**: The application evaluates the adequacy of existing tests for users' ML code , and offers recommendations for additional, system-specific tests to enhance testing effectiveness.\n",
    "  \n",
    "3. **Test Function Specification Generation**: Users select desired test recommendations and prompt the application to generate test function specifications and references. These serve as reliable starting points for users to enrich the ML system test suites.\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "The success of our product will be dependent on the mutation testing outcome of the test cases, which are created based on the specifications generated by the application. By introducing perturbations to the ML project code, the success rate of detecting these perturbations will be recorded as the evaluation metric.\n",
    "\n",
    "Our partner and stakeholders would expect a significant improvement in testing suites of their ML systems post-application usage. As a result, the testing suites would demonstrate high accuracy in detecting faults, which ensure consistency and high quality of ML projects during updates.\n",
    "\n",
    "### Data Science Approach\n",
    "\n",
    "#### Data: GitHub Repositories\n",
    "\n",
    "In this project, GitHub repositories are our data. \n",
    "\n",
    "We will collect 11 repositories studied in {cite}`openja2023studying` for the development of our testing checklist. Additionally, we will collect 377 repositories identified in the study by {cite}`wattanakriengkrai2022github` for our product development.\n",
    "\n",
    "For each repository, we are interested in the repository metadata, as well as the ML modeling- and test-related source code. The metadata will be retrieved using the GitHub API, while the source code will be downloaded and filtered using our custom scripts. To ensure the relevance of the repositories to our study, we will apply the following criteria for filtering:\n",
    " 1. Repositories that are related to ML systems.\n",
    " 2. Repositories that include test cases.\n",
    " 3. Repositories whose development are written in the Python programming language.\n",
    "\n",
    "#### Methodologies\n",
    "\n",
    "Our data science methodology incorporates both human expert evaluation and prompt engineering to assess and enhance the test quality of ML systems.\n",
    "\n",
    "- Human Expert Evaluation\n",
    "\n",
    "    We will begin by formulating a comprehensive checklist for evaluating the data and ML pipeline based on the established testing strategies outlined in {cite}`openja2023studying` as the foundational framework. Our team will manually evaluate the test quality within each repository data based on the formulated checklist. The checklist will be refined during the process to ensure its applicability and robustness in testing general ML systems.\n",
    "\n",
    "- Prompt Engineering\n",
    "\n",
    "    We will engineer the prompts for LLM to incorporate with the ML system code and the curated checklist and to serve various purposes across the three-stage process:\n",
    "    1. Prompts to examine test cases within the ML system source codes and deliver comprehensive test scores.\n",
    "    2. Prompts to compare and contrast the ML system source codes and deliver recommendations.\n",
    "    3. Prompts to generate system-specific test specifications based on user selected testing recommendations {cite}`schafer2023empirical`\n",
    "\n",
    "#### Iterative Development Approach\n",
    "\n",
    "We begin by setting up a foundational framework based on the selected GitHub repositories and researches on ML testing. It is important to acknowledge that this might not cover all ML systems or testing practices. To address these considerations, we adopt an iterative development approach by establishing an open and scalable framework. The application could be continuously refined based on contributors' insights.\n",
    "\n",
    "Users are encouraged to interpret the generated artifacts with a grain of salt and recognize the evolving nature of ML system testing practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery Timeline\n",
    "\n",
    "Our team follow the below timeline for our product delivery and prioritize close communication with our partners to ensure that our development align closely with their expectations.\n",
    "\n",
    "| Timeline | Milestones |\n",
    "|---|---|\n",
    "| Week 1 (Apr 29 - May 3) | Prepare and Present Initial Proposal. Scrape repository data. |\n",
    "| Week 2 - 3 (May 6 - 17) | Deliver Proposal. Deliver Draft of ML Pipeline Test Checklist. Develop Minimum Viable Product (Test Completeness Score, Missing Test Recommendation) |\n",
    "| Week 4 - 5 (May 20 - May 31) | Update Test Checklist. Develop Test Function Spec Generator. |\n",
    "| Week 6 (Jun 3 - Jun 7) | Update Test Checklist. Wrap Up Product. |\n",
    "| Week 7 (Jun 10 - Jun 14) | Finalize Test Checklist. Perform Product System Test. Present Final Product. Prepare Final Product Report. |\n",
    "| Week 8 (Jun 17 - Jun 21) | Deliver Final Product. Deliver Final Product Report. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
