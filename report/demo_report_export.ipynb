{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669bb292-2b53-4a28-8d5f-ef6f3687f440",
   "metadata": {},
   "source": [
    "## Evaluation Report Export Function Demo - For Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import *\n",
    "from analyze import TestEvaluator\n",
    "from modules.checklist.checklist import Checklist, ChecklistFormat\n",
    "from modules.code_analyzer.repo import Repository\n",
    "from modules.workflow.files import PythonTestFileExtractor, RepoFileExtractor\n",
    "from modules.workflow.parse import ResponseParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0a59a9-185c-4f17-a0dd-fa2534958ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = '../../../lightfm/'\n",
    "checklist_path = '../../checklist/checklist_demo.csv'\n",
    "report_output_path_html = '../../report/evaluation_report.html'\n",
    "report_output_path_pdf = '../../report/evaluation_report.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d717ba5d-dc9d-477d-a9db-ccb993f48f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "                                                                                         Requirement  \\\n",
      "ID  Title                                                                                              \n",
      "1.1 Write Descriptive Test Names                   Each test function should have a clear, descri...   \n",
      "1.2 Keep Tests Focused                             Each test should focus on a single scenario, u...   \n",
      "2.1 Ensure Data File Loads as Expected             Ensure that data-loading functions correctly l...   \n",
      "5.1 Validate Model Input and Output Compatibility  Confirm that the model accepts inputs of the c...   \n",
      "\n",
      "                                                   is_Satisfied  \\\n",
      "ID  Title                                                         \n",
      "1.1 Write Descriptive Test Names                              1   \n",
      "1.2 Keep Tests Focused                                        1   \n",
      "2.1 Ensure Data File Loads as Expected                        0   \n",
      "5.1 Validate Model Input and Output Compatibility             0   \n",
      "\n",
      "                                                   n_files_tested  \\\n",
      "ID  Title                                                           \n",
      "1.1 Write Descriptive Test Names                                2   \n",
      "1.2 Keep Tests Focused                                          2   \n",
      "2.1 Ensure Data File Loads as Expected                          2   \n",
      "5.1 Validate Model Input and Output Compatibility               2   \n",
      "\n",
      "                                                                                        Observations  \\\n",
      "ID  Title                                                                                              \n",
      "1.1 Write Descriptive Test Names                   [(test_cross_validation.py) The test function ...   \n",
      "1.2 Keep Tests Focused                             [(test_cross_validation.py) The test function ...   \n",
      "2.1 Ensure Data File Loads as Expected             [(test_cross_validation.py) The code does not ...   \n",
      "5.1 Validate Model Input and Output Compatibility  [(test_cross_validation.py) The code does not ...   \n",
      "\n",
      "                                                                                 Function References  \n",
      "ID  Title                                                                                             \n",
      "1.1 Write Descriptive Test Names                   [{'File Path': '../../../lightfm/tests/test_cr...  \n",
      "1.2 Keep Tests Focused                             [{'File Path': '../../../lightfm/tests/test_cr...  \n",
      "2.1 Ensure Data File Loads as Expected             [{'File Path': '../../../lightfm/tests/test_cr...  \n",
      "5.1 Validate Model Input and Output Compatibility  [{'File Path': '../../../lightfm/tests/test_cr...  \n",
      "\n",
      "Score: 2/4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2/4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "checklist = Checklist(checklist_path, checklist_format=ChecklistFormat.CSV)\n",
    "extractor = PythonTestFileExtractor(Repository(repo_path))\n",
    "\n",
    "evaluator = TestEvaluator(llm, extractor, checklist)\n",
    "response = evaluator.evaluate()\n",
    "\n",
    "parser = ResponseParser(response)\n",
    "parser.get_completeness_score(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273db18c-13c4-4c86-a4c8-f42e0b0e37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.export_evaluation_report(report_output_path_html, 'html', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a682a42-8807-48c6-9de4-0558838e3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.export_evaluation_report(report_output_path_pdf, 'pdf', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07875448-9c58-4ec0-94b8-de9be8870011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
