{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5741f4-3cef-4bfd-b66b-d4f4d38f83c7",
   "metadata": {},
   "source": [
    "#### NOTE: the result is based on the code base [abb9a21](https://github.com/UBC-MDS/test-creation/pull/136/commits/abb9a21828cb257ff8f2629261dc1ad64ad7dcb0), which is similar to the commit [69d61a9](https://github.com/UBC-MDS/test-creation/tree/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2) in the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ccf9d-c6b7-4982-8c72-c1d08aba6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ebdff-17ae-47ad-a84d-f3bd3f55a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(response):\n",
    "    report = []\n",
    "    for result in response.call_results:\n",
    "        if result.parsed_response:\n",
    "            resp = result.parsed_response['results']\n",
    "            for item in resp:\n",
    "                item['file'] = result.files_evaluated[0] \n",
    "                item['success'] = result.success\n",
    "                report.append(item)\n",
    "        else:\n",
    "            report.append({\n",
    "                'ID': '2.1', # FIXME\n",
    "                'Title': '',\n",
    "                'Requirement': '',\n",
    "                'Observation': '',\n",
    "                'Functions': [],\n",
    "                'Evaluation': '',\n",
    "                'Score': 0,\n",
    "                'file': result.files_evaluated[0],\n",
    "                'success': result.success\n",
    "            })\n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "def extract_file_and_scores(resp_path):\n",
    "    #print(resp_path)\n",
    "    with open(resp_path, 'rb') as file:\n",
    "        response = pickle.load(file)\n",
    "    report = get_report(response)\n",
    "    df = (\n",
    "        report\n",
    "        .pivot(index='file', columns='ID', values='Score')\n",
    "        .rename_axis(None, axis=1)\n",
    "    )\n",
    "    df['success'] = report.groupby(['file'])['success'].all()\n",
    "    df['response_path'] = resp_path\n",
    "    return df.reset_index()\n",
    "\n",
    "def generate_stat_plot(df_repo__stat, ground_truth=None, facet_col='repo', repo=None, id=None):\n",
    "    \"\"\"\n",
    "    Generate Stat plot across all repo and all checklist item\n",
    "    Optional to incorporate ground truth and select specific repo/checklist item\n",
    "    \"\"\"\n",
    "    if facet_col == 'repo':\n",
    "        x_col = 'id'\n",
    "        x_title = 'Checklist ID'\n",
    "    elif facet_col == 'id':\n",
    "        x_col = 'repo'\n",
    "        x_title = 'Repository'\n",
    "    \n",
    "    # the base chart\n",
    "    if repo:\n",
    "        df_repo__stat = df_repo__stat.query(f'repo == \"{repo}\"')\n",
    "    if id:\n",
    "        df_repo__stat = df_repo__stat.query(f'id == \"{id}\"')\n",
    "    \n",
    "    base = alt.Chart().transform_calculate(\n",
    "        min=\"max(0, datum.mean-datum.std)\",\n",
    "        max=\"min(1, datum.mean+datum.std)\"\n",
    "    )\n",
    "    \n",
    "    # generate the points\n",
    "    points = base.mark_point(\n",
    "        filled=True,\n",
    "        size=50,\n",
    "        color='black'\n",
    "    ).encode(\n",
    "        x=alt.X(f'{x_col}:O').axis(labelAngle=0).title(x_title),\n",
    "        y=alt.Y('mean:Q').scale(domainMin=0, domainMax=1).title('Score'),\n",
    "    )\n",
    "    \n",
    "    # generate the error bars\n",
    "    errorbars = base.mark_errorbar().encode(\n",
    "        x=f\"{x_col}:O\",\n",
    "        y=alt.Y(\"min:Q\").title('1 SD'),\n",
    "        y2=\"max:Q\"\n",
    "    )\n",
    "\n",
    "    plot = points + errorbars\n",
    "    \n",
    "    if ground_truth is not None:\n",
    "        # generate points of ground truth\n",
    "        if repo:\n",
    "            ground_truth = ground_truth.query(f'repo == \"{repo}\"')\n",
    "        if id:\n",
    "            ground_truth = ground_truth.query(f'id == \"{id}\"')\n",
    "        \n",
    "        df_repo__stat = pd.merge(df_repo__stat, ground_truth, how='left', on=['repo', 'id'])\n",
    "        \n",
    "        gt_points = alt.Chart().mark_point(\n",
    "            filled=True,\n",
    "            size=100,\n",
    "            color='green',\n",
    "            shape=\"diamond\"\n",
    "        ).encode(\n",
    "            x=alt.X(f'{x_col}:O'),\n",
    "            y=alt.Y('score:Q')\n",
    "        )\n",
    "\n",
    "        plot += gt_points\n",
    "\n",
    "    plot = alt.layer(\n",
    "                plot,\n",
    "                data=df_repo__stat\n",
    "            ).properties(\n",
    "                width=400,\n",
    "            ).facet(\n",
    "                column=f'{facet_col}',\n",
    "                columns=2\n",
    "            )\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e9615-c929-4fd1-9f6f-771fbd5d43d0",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a6fc2-a143-4117-8432-e32465e3fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_ids = ['2.1', '3.2', '3.5', '4.2', '5.3', '6.1', '6.2']\n",
    "\n",
    "result_path = '../draft/batch_run_results/record_combine.yml'\n",
    "with open(result_path, 'r') as file:\n",
    "    config = pd.DataFrame(yaml.safe_load(file))\n",
    "\n",
    "# prepare score data by repo, run, file\n",
    "tmp = [\n",
    "    extract_file_and_scores(path) for path in config['response_path']\n",
    "]\n",
    "tmp = pd.concat(tmp, axis=0).reset_index(drop=True)\n",
    "\n",
    "df_repo_run_file = config.merge(tmp, on='response_path', how='left')\n",
    "\n",
    "# filter non-test files in qlib\n",
    "df_repo_run_file = df_repo_run_file.query('(repo != \"qlib\") | (file.str.contains(\"../data/raw/openja/qlib/tests/\"))')\n",
    "\n",
    "# prepare score data by repo, run\n",
    "df_repo_run = df_repo_run_file.groupby(['repo', 'run']).agg({\n",
    "    id: ['max'] for id in checklist_ids\n",
    "})\n",
    "df_repo_run.columns = [col[0] for col in df_repo_run.columns]\n",
    "df_repo_run = df_repo_run.reset_index()\n",
    "\n",
    "# prepare statistics of scores by repo\n",
    "df_repo__stat = df_repo_run.groupby(['repo']).agg({\n",
    "    id: ['mean', 'std', 'count'] for id in checklist_ids\n",
    "})\n",
    "df_repo__stat = pd.melt(df_repo__stat.reset_index(), id_vars=[('repo', '')])\n",
    "df_repo__stat.columns = ['repo', 'id', 'stat', 'value']\n",
    "df_repo__stat = (\n",
    "    df_repo__stat.pivot(index=['repo', 'id'], columns='stat', values='value')\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "\n",
    "# prepare counting of scores by repo\n",
    "df_repo__count = df_repo_run.groupby(['repo'])['2.1'].apply(Counter).reset_index()\n",
    "for id in checklist_ids[1:]:\n",
    "    df_repo__count = df_repo__count.merge(\n",
    "        df_repo_run.groupby(['repo'])[id].apply(Counter).reset_index(),\n",
    "        on=['repo', 'level_1'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "df_repo__count = df_repo__count.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108c053-1646-4203-80b2-110d95ce8af7",
   "metadata": {},
   "source": [
    "### Runs Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c784c-8634-4c26-9136-f31a6ea42fbf",
   "metadata": {},
   "source": [
    "#### 1. Some non-test files are included in the evaluation\n",
    "\n",
    "For example, the `./nanodet/nanodet/trainer/task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efb11a-2ad3-43e0-a7b8-8795fc5a1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('repo == \"nanodet\"')['file'].unique()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804c368-7a96-48ae-9f7d-c30e0c2cd1fa",
   "metadata": {},
   "source": [
    "#### 2. Evaluation on the file `magenta/magenta/models/music_vae/data_test.py` is always failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82a59-9272-4025-94c0-6f440e3e963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file[~df_repo_run_file.success]['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28bf5-7ef0-4528-aeba-6b32d4bbf98c",
   "metadata": {},
   "source": [
    "#### 3. (so far) `lightfm` and `magenta` have the least test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e52f61-1fa7-4ebd-aa2d-40e441600ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('run == 1').groupby(['repo'])['file'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215fdc0-663d-4556-af39-b34d3be3c487",
   "metadata": {},
   "source": [
    "#### 4. The test files are not always in a `tests/` folder. Is it be good practice to always do that? Should it be one of the checklist item to ensure all tests placed under `tests/` folder?\n",
    "\n",
    "For example, `magenta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93a571-d215-406b-9fbf-d0528de60b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('repo == \"magenta\"')['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67060f0f-ba59-4ac0-bc84-04853b2f5216",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18118436-6899-465d-bae5-feafaa49a293",
   "metadata": {},
   "source": [
    "#### 1. Overview of accuracy and consistency `lightfm` evaluation\n",
    "\n",
    "Let the ground truth of the `lightfm` is as the [following](https://github.com/UBC-MDS/test-creation/blob/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2/report/repo_human_evaluation/human_evaluation_report-lightfm.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893d9a5-4dba-4ed7-ad93-6e26f004876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "ground_truth = pd.DataFrame([\n",
    "    {'repo': 'lightfm', 'id': '2.1', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '3.2', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '3.5', 'score': 0},\n",
    "    {'repo': 'lightfm', 'id': '4.2', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '5.3', 'score': 0.5},\n",
    "    {'repo': 'lightfm', 'id': '6.1', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '6.2', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '2.1', 'score': 0.5},\n",
    "    {'repo': 'qlib', 'id': '3.2', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '3.5', 'score': 0},\n",
    "    {'repo': 'qlib', 'id': '4.2', 'score': 0.5},\n",
    "    {'repo': 'qlib', 'id': '5.3', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '6.1', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '6.2', 'score': 1},\n",
    "])\n",
    "ground_truth[ground_truth.repo == 'lightfm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f58d6-e16c-4fb3-ba73-1981c375a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"lightfm\", facet_col='repo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553667f-d2aa-4349-8bb8-12807f83871e",
   "metadata": {},
   "source": [
    "The distribution of the scores for each checklist items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fbb35-d05f-4c0c-bb2f-15434446f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"lightfm\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a01ee9-515e-4b30-9acb-0110de9dd16d",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "The system evaluation kind of aligns with our evaluation, that is,\n",
    " - for those items that we believe \"Satisfied\" (Score = 1), the system mostly output 0.5 or 1\n",
    " - for those items that we believe \"Partially Satisfied\" or \"Not Satisfied\", the system mostly output 0.5 or 0\n",
    " - #TBC It shows greater variation for Item 3.5 and Item 5.3 and Item 6.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf7f4-14e8-4fed-bd24-34281772624f",
   "metadata": {},
   "source": [
    "#### 2. Overview of `qlib`\n",
    "Let the ground truth of the `qlib` is as the following (FIXME: to be confirmed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6255ae9-ec5d-4088-aee0-11b05d89f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "ground_truth[ground_truth.repo == 'qlib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e12df-0077-44e1-891d-206f1c5f6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"qlib\", facet_col='repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3f4b-8940-48ba-9b6b-7c8a8856e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"qlib\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558e093-6732-40fa-89eb-c6a426904f06",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "- There are more disagreement between system and manual evaluation, especially for Item 5.3, Item 6.1, Item 6.2. #TBC\n",
    "- #TBC Compared to consistency results for `lightfm`, the variation for Item 3.5 is greatly reduced, but there is greater variation for Item 3.2. Moreover, the variation for Item 5.3 remain big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffd6db-e68b-495a-bc6a-050b593cfcf3",
   "metadata": {},
   "source": [
    "#### 3. The consistency for each checklist items\n",
    " - Why is it important?\n",
    "   If the score of a particular item varies a lot when evaluating a repository, it might mean that its prompt (`Requirement`) is confusing to the LLM, or the checklist item itself is not well defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b154cba-6f2c-4423-bf20-97cce23e1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__stat.pivot(index='id', columns='repo', values='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04455456-9516-44a7-ba7e-8f6c76012a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_repo__stat).mark_boxplot().encode(\n",
    "    x=\"std:Q\",\n",
    "    y='id:N'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e72eb-03a0-4718-85aa-04989c6e44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, facet_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc620a-e7ae-483f-8bb7-e327d49a03d4",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 2.1 `Ensure Data File Loads as Expected` and item 4.2 `Verify Data Split Proportion` are particularly stable. When evaluating a repository, it usually has the lowest variance of scores.\n",
    " - #TBC The standard deviation for Item 3.5 and 5.3 shows great variation, which might imply that test cases in some repo might be confusing to LLM while some are clear.\n",
    " - #TBC The standard deviation for Item 5.3, 6.1, 6.2 are relatively high and consistent, which might imply that there is room for refining the prompt to reduce consistency issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc16e-fc8d-42ec-9680-485dbdcc0d36",
   "metadata": {},
   "source": [
    "#### 4. The consistency for each checklist items, compared to the `lightfm`\n",
    " - Why is it important? We optimized the consistency of our system using `lightfm`. Therefore, we treat this repository as a benchmark. If a particular checklist item has a much worse consistency in other repository, that might mean that the prompt for that item is not generalizable.\n",
    "\n",
    "Below shows the standard deviations in a 30 runs for each checklist item for each repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbe813-87bd-4e0a-a368-40b920f3923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df_repo__stat[['repo', 'std', 'id']].pivot(index='repo', columns='id')\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522f7ca-f472-4ba4-8b02-8aff0794c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "scipy.stats.f.interval(0.95, 29, 29, loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5883b9a-c852-4d2f-82fd-7f5434edcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = stds.iloc[1:] / stds.iloc[0]\n",
    "\n",
    "base = alt.Chart(\n",
    "    F.melt(ignore_index=False).reset_index()[['repo', 'id', 'value']]\n",
    ").transform_calculate(\n",
    "    benchmark=\"1\",\n",
    "    \n",
    ")\n",
    "\n",
    "point = base.mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    ").encode(\n",
    "    x=alt.X('value:Q').title(\"std ratio (c.f. lightfm)\"),\n",
    "    y='id:N',\n",
    "    color='repo'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")\n",
    "\n",
    "gt_lines = alt.Chart().mark_rule(\n",
    "                strokeDash=[3]\n",
    "            ).encode(\n",
    "                x=alt.X('score:Q').stack(False),\n",
    "                color=alt.value('grey'),\n",
    "                size=alt.value(2),\n",
    "            )\n",
    "\n",
    "base.mark_rule(color='black').encode(x=\"benchmark:Q\") + point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef79ff-a4fa-4412-973d-7e4fdfe1be71",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 3.2 `Data in the Expected Format` becomes much more unstable in other repositories.\n",
    " - That of the 2.1 is not stable in the repo `magenta`, but it may be due to the repo itself.\n",
    "\n",
    "TODO: to look into the 3.2's scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860559f-bde8-47c5-ac35-94797beb6054",
   "metadata": {},
   "source": [
    "#### TODO: Given ground truth == 1, distribution of system score?\n",
    "#### TODO: Given ground truth == 0, distribution of system score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1179d8-ef8c-46dd-878b-55e24f848764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_density_plot(df_repo_run_long, df_ground_truth=None, repo=None, id=None):    \n",
    "    # data\n",
    "    repo_data = df_repo_run_long.copy()\n",
    "    if repo:\n",
    "        repo_data = repo_data.query(f'repo == \"{repo}\"')\n",
    "    if id:\n",
    "        repo_data = repo_data.query(f'id == \"{id}\"')\n",
    "\n",
    "    # base density chart\n",
    "    base = alt.Chart().transform_density(\n",
    "                'eval_score', \n",
    "                groupby=['repo', 'id'], \n",
    "                as_=['score', 'density'],\n",
    "                extent=[0, 1],\n",
    "            ).mark_line().encode(\n",
    "                x='score:Q', \n",
    "                y=alt.Y('density:Q').stack(False), \n",
    "                color='id',\n",
    "                opacity=alt.value(0.8),\n",
    "            )\n",
    "    \n",
    "    if df_ground_truth is not None:\n",
    "        # data\n",
    "        gt_data = df_ground_truth.copy()\n",
    "        if repo:\n",
    "            gt_data = gt_data.query(f'repo == \"{repo}\"')\n",
    "        if id:\n",
    "            gt_data = gt_data.query(f'id == \"{id}\"')\n",
    "        \n",
    "        repo_data = pd.merge(repo_data, gt_data, how='left', on=['repo', 'id'])\n",
    "        \n",
    "        # ground true line chart\n",
    "        gt_lines = alt.Chart().mark_rule(\n",
    "                        strokeDash=[3]\n",
    "                    ).encode(\n",
    "                        x=alt.X('score:Q').stack(False),\n",
    "                        color=alt.value('grey'),\n",
    "                        size=alt.value(2),\n",
    "                    )\n",
    "\n",
    "        base += gt_lines\n",
    "\n",
    "    charts = alt.layer(\n",
    "                base,\n",
    "                data=repo_data\n",
    "            ).properties(\n",
    "                width=200,\n",
    "                height=200,\n",
    "            ).facet(\n",
    "                row='repo',\n",
    "                column='id'\n",
    "            )        \n",
    "    \n",
    "    return charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe627c9-6fc0-465e-8ef9-83c5bdfa07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_long = df_repo_run.melt(\n",
    "    id_vars=['repo', 'run'],\n",
    "    var_name='id',\n",
    "    value_name='eval_score',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853ee1c-19c5-4e20-acd8-ac92d6c75c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = lightfm_gt\n",
    "ground_truth['repo'] = 'lightfm'\n",
    "charts = generate_density_plot(df_repo_run_long, df_ground_truth=ground_truth)\n",
    "charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e22d2-f800-4821-bafe-493fb225c476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fa41b-eed6-47ea-bdb2-6d0a424f91a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7f99d-9e65-4eaf-911e-44ca1c0eb77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation2]",
   "language": "python",
   "name": "conda-env-test-creation2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
