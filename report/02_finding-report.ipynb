{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5741f4-3cef-4bfd-b66b-d4f4d38f83c7",
   "metadata": {},
   "source": [
    "#### NOTE: the result is based on the code base [abb9a21](https://github.com/UBC-MDS/test-creation/pull/136/commits/abb9a21828cb257ff8f2629261dc1ad64ad7dcb0), which is similar to the commit [69d61a9](https://github.com/UBC-MDS/test-creation/tree/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2) in the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ccf9d-c6b7-4982-8c72-c1d08aba6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ebdff-17ae-47ad-a84d-f3bd3f55a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(response):\n",
    "    report = []\n",
    "    for result in response.call_results:\n",
    "        if result.parsed_response:\n",
    "            resp = result.parsed_response['results']\n",
    "            for item in resp:\n",
    "                item['file'] = result.files_evaluated[0] \n",
    "                item['success'] = result.success\n",
    "                report.append(item)\n",
    "        else:\n",
    "            report.append({\n",
    "                'ID': '2.1', # FIXME\n",
    "                'Title': '',\n",
    "                'Requirement': '',\n",
    "                'Observation': '',\n",
    "                'Functions': [],\n",
    "                'Evaluation': '',\n",
    "                'Score': 0,\n",
    "                'file': result.files_evaluated[0],\n",
    "                'success': result.success\n",
    "            })\n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "def extract_file_and_scores(resp_path, verbose=False):\n",
    "    if verbose:\n",
    "        print(resp_path)\n",
    "    with open(resp_path, 'rb') as file:\n",
    "        response = pickle.load(file)\n",
    "    report = get_report(response)\n",
    "    df = (\n",
    "        report\n",
    "        .pivot(index='file', columns='ID', values='Score')\n",
    "        .rename_axis(None, axis=1)\n",
    "    )\n",
    "    df['success'] = report.groupby(['file'])['success'].all()\n",
    "    df['response_path'] = resp_path\n",
    "    return df.reset_index()\n",
    "\n",
    "def generate_stat_plot(df_repo__stat, repo, ground_truth=None):\n",
    "    # the base chart\n",
    "    base = alt.Chart(df_repo__stat.query(f'repo == \"{repo}\"')).transform_calculate(\n",
    "        min=\"max(0, datum.mean-datum.std)\",\n",
    "        max=\"min(1, datum.mean+datum.std)\"\n",
    "    )\n",
    "    \n",
    "    # generate the points\n",
    "    points = base.mark_point(\n",
    "        filled=True,\n",
    "        size=50,\n",
    "        color='black'\n",
    "    ).encode(\n",
    "        x=alt.X('id:O').axis(labelAngle=0).title('Checklist Id'),\n",
    "        y=alt.Y('mean:Q').scale(domainMin=0, domainMax=1).title('Score'),\n",
    "    )\n",
    "    \n",
    "    # generate the error bars\n",
    "    errorbars = base.mark_errorbar().encode(\n",
    "        x=\"id:O\",\n",
    "        y=alt.Y(\"min:Q\").title('1 SD'),\n",
    "        y2=\"max:Q\"\n",
    "    )\n",
    "\n",
    "    plot = points + errorbars\n",
    "    \n",
    "    if ground_truth is not None:\n",
    "        # generate points of ground truth\n",
    "        gt_points = alt.Chart(ground_truth).mark_point(\n",
    "            filled=True,\n",
    "            size=100,\n",
    "            color='green',\n",
    "            shape=\"diamond\"\n",
    "        ).encode(\n",
    "            x=alt.X('id:O'),\n",
    "            y=alt.Y('score:Q')\n",
    "        )\n",
    "\n",
    "        plot += gt_points\n",
    "\n",
    "    return plot.properties(width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e9615-c929-4fd1-9f6f-771fbd5d43d0",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55be151-3c8a-44b3-985c-7da87e3566b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checklist_ids = ['2.1', '3.2', '3.5', '4.2', '5.3', '6.1', '6.2']\n",
    "\n",
    "with open('../data/processed/batch_run/record_combine.yml', 'r') as file:\n",
    "    config = pd.DataFrame(yaml.safe_load(file))\n",
    "\n",
    "# prepare score data by repo, run, file\n",
    "tmp = [\n",
    "    extract_file_and_scores(path) for path in config['response_path']\n",
    "]\n",
    "tmp = pd.concat(tmp, axis=0).reset_index(drop=True)\n",
    "\n",
    "raw_df_repo_run_file = config.merge(tmp, on='response_path', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a6fc2-a143-4117-8432-e32465e3fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter non-test files in qlib\n",
    "df_repo_run_file = raw_df_repo_run_file.query('(repo != \"qlib\") | (file.str.contains(\"../data/raw/openja/qlib/tests/\"))')\n",
    "\n",
    "# prepare score data by repo, run\n",
    "df_repo_run = df_repo_run_file.groupby(['repo', 'run']).agg({\n",
    "    id: ['max'] for id in checklist_ids\n",
    "})\n",
    "df_repo_run.columns = [col[0] for col in df_repo_run.columns]\n",
    "df_repo_run = df_repo_run.reset_index()\n",
    "\n",
    "# prepare statistics of scores by repo\n",
    "df_repo__stat = df_repo_run.groupby(['repo']).agg({\n",
    "    id: ['mean', 'std', 'count'] for id in checklist_ids\n",
    "})\n",
    "df_repo__stat = pd.melt(df_repo__stat.reset_index(), id_vars=[('repo', '')])\n",
    "df_repo__stat.columns = ['repo', 'id', 'stat', 'value']\n",
    "df_repo__stat = (\n",
    "    df_repo__stat.pivot(index=['repo', 'id'], columns='stat', values='value')\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "\n",
    "# prepare counting of scores by repo\n",
    "df_repo__count = df_repo_run.groupby(['repo'])['2.1'].apply(Counter).reset_index()\n",
    "for id in checklist_ids[1:]:\n",
    "    df_repo__count = df_repo__count.merge(\n",
    "        df_repo_run.groupby(['repo'])[id].apply(Counter).reset_index(),\n",
    "        on=['repo', 'level_1'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "df_repo__count = df_repo__count.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108c053-1646-4203-80b2-110d95ce8af7",
   "metadata": {},
   "source": [
    "### Runs Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c784c-8634-4c26-9136-f31a6ea42fbf",
   "metadata": {},
   "source": [
    "#### 1. Some non-test files are included in the evaluation\n",
    "\n",
    "For example, the `./nanodet/nanodet/trainer/task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efb11a-2ad3-43e0-a7b8-8795fc5a1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_repo_run_file.query('repo == \"nanodet\"')['file'].unique()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804c368-7a96-48ae-9f7d-c30e0c2cd1fa",
   "metadata": {},
   "source": [
    "#### 2. Evaluation on the file `magenta/magenta/models/music_vae/data_test.py` is always failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82a59-9272-4025-94c0-6f440e3e963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file[~df_repo_run_file.success]['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28bf5-7ef0-4528-aeba-6b32d4bbf98c",
   "metadata": {},
   "source": [
    "#### 3. `DeepSpeech`, `lightfm` and `magenta` have the least (Python) test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e52f61-1fa7-4ebd-aa2d-40e441600ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('run == 1').groupby(['repo'])['file'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215fdc0-663d-4556-af39-b34d3be3c487",
   "metadata": {},
   "source": [
    "#### 4. The test files are not always in a `tests/` folder. Is it be good practice to always do that? Should it be one of the checklist item to ensure all tests placed under `tests/` folder?\n",
    "\n",
    "For example, `magenta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93a571-d215-406b-9fbf-d0528de60b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('repo == \"magenta\"')['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67060f0f-ba59-4ac0-bc84-04853b2f5216",
   "metadata": {},
   "source": [
    "### Findings on 8 repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad96150-ef13-44fb-95fc-d0657b6a866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.repo.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18118436-6899-465d-bae5-feafaa49a293",
   "metadata": {},
   "source": [
    "#### 1. Overview of accuracy and consistency `lightfm` evaluation\n",
    "\n",
    "Let the ground truth of the `lightfm` is as the [following](https://github.com/UBC-MDS/test-creation/blob/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2/report/repo_human_evaluation/human_evaluation_report-lightfm.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893d9a5-4dba-4ed7-ad93-6e26f004876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "lightfm_gt = pd.DataFrame([\n",
    "    {'id': '2.1', 'score': 1},\n",
    "    {'id': '3.2', 'score': 1},\n",
    "    {'id': '3.5', 'score': 0},\n",
    "    {'id': '4.2', 'score': 1},\n",
    "    {'id': '5.3', 'score': 0.5},\n",
    "    {'id': '6.1', 'score': 1},\n",
    "    {'id': '6.2', 'score': 1},\n",
    "])\n",
    "lightfm_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f58d6-e16c-4fb3-ba73-1981c375a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, \"lightfm\", lightfm_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553667f-d2aa-4349-8bb8-12807f83871e",
   "metadata": {},
   "source": [
    "The distribution of the scores for each checklist items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fbb35-d05f-4c0c-bb2f-15434446f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"lightfm\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a01ee9-515e-4b30-9acb-0110de9dd16d",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "The system evaluation kind of aligns with our evaluation, that is,\n",
    " - for those items that we believe \"Satisfied\" (Score = 1), the system mostly output 0.5 or 1\n",
    " - for those items that we believe \"Partially Satisfied\" or \"Not Satisfied\", the system mostly output 0.5 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf7f4-14e8-4fed-bd24-34281772624f",
   "metadata": {},
   "source": [
    "#### 2. Overview of `qlib`\n",
    "Let the ground truth of the `qlib` is as the following (FIXME: to be confirmed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6255ae9-ec5d-4088-aee0-11b05d89f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "qlib_gt = pd.DataFrame([\n",
    "    {'id': '2.1', 'score': 0.5},\n",
    "    {'id': '3.2', 'score': 1},\n",
    "    {'id': '3.5', 'score': 0},\n",
    "    {'id': '4.2', 'score': 0},\n",
    "    {'id': '5.3', 'score': 1},\n",
    "    {'id': '6.1', 'score': 1},\n",
    "    {'id': '6.2', 'score': 1},\n",
    "])\n",
    "qlib_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e12df-0077-44e1-891d-206f1c5f6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, \"qlib\", qlib_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3f4b-8940-48ba-9b6b-7c8a8856e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"qlib\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558e093-6732-40fa-89eb-c6a426904f06",
   "metadata": {},
   "source": [
    "**Observations**: There are more disagreement between system and manual evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffd6db-e68b-495a-bc6a-050b593cfcf3",
   "metadata": {},
   "source": [
    "#### 3. The consistency for each checklist items\n",
    " - Why is it important?\n",
    "   If the score of a particular item varies a lot when evaluating a repository, it might mean that its prompt (`Requirement`) is confusing to the LLM, or the checklist item itself is not well defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04455456-9516-44a7-ba7e-8f6c76012a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_repo__stat).mark_boxplot().encode(\n",
    "    x=\"std:Q\",\n",
    "    y='id:N'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc620a-e7ae-483f-8bb7-e327d49a03d4",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 2.1 `Ensure Data File Loads as Expected` is usually stable. When evaluating a repository, 50% of the time its standard deviation is smaller than 0.05, the smallest among the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc16e-fc8d-42ec-9680-485dbdcc0d36",
   "metadata": {},
   "source": [
    "#### 4. The consistency for each checklist items, compared to the `lightfm`\n",
    " - Why is it important? We optimized the consistency of our system using `lightfm`. Therefore, we treat this repository as a benchmark. If a particular checklist item has a much worse consistency in other repository, that might mean that the prompt for that item is not generalizable.\n",
    "\n",
    "Below shows the standard deviations in a 30 runs for each checklist item for each repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbe813-87bd-4e0a-a368-40b920f3923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df_repo__stat[['repo', 'std', 'id']].pivot(index='repo', columns='id')\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5883b9a-c852-4d2f-82fd-7f5434edcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = stds.drop(index='lightfm') / stds.loc['lightfm']\n",
    "\n",
    "base = alt.Chart(\n",
    "    F.melt(ignore_index=False).reset_index()[['repo', 'id', 'value']]\n",
    ").transform_calculate(\n",
    "    benchmark=\"1\",\n",
    "    threshold=f\"{scipy.stats.f.ppf(0.975, 29, 29)}\"\n",
    ")\n",
    "\n",
    "point = base.mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    ").encode(\n",
    "    x=alt.X('value:Q').title(\"std ratio (c.f. lightfm)\"),\n",
    "    y='id:N',\n",
    "    color='repo',\n",
    "    tooltip='repo'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")\n",
    "\n",
    "point \\\n",
    "+ base.mark_rule(color='black').encode(x=\"benchmark:Q\") \\\n",
    "+ base.mark_rule(color='red').encode(x=\"threshold:Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef79ff-a4fa-4412-973d-7e4fdfe1be71",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 3.2 `Data in the Expected Format` becomes much more unstable in most of other repositories.\n",
    " - That of the 2.1 is significantly unstable in the repo `paperless-ng`, `magenta` and `DeepSpeech`, but it may be due to the repo itself.\n",
    "\n",
    "TODO: to look into the 3.2's scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860559f-bde8-47c5-ac35-94797beb6054",
   "metadata": {},
   "source": [
    "#### TODO: Given ground truth == 1, distribution of system score?\n",
    "#### TODO: Given ground truth == 0, distribution of system score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1179d8-ef8c-46dd-878b-55e24f848764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation-111]",
   "language": "python",
   "name": "conda-env-test-creation-111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
