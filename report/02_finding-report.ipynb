{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5741f4-3cef-4bfd-b66b-d4f4d38f83c7",
   "metadata": {},
   "source": [
    "#### NOTE: the result is based on the code base [abb9a21](https://github.com/UBC-MDS/test-creation/pull/136/commits/abb9a21828cb257ff8f2629261dc1ad64ad7dcb0), which is similar to the commit [69d61a9](https://github.com/UBC-MDS/test-creation/tree/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2) in the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ccf9d-c6b7-4982-8c72-c1d08aba6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ebdff-17ae-47ad-a84d-f3bd3f55a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(response):\n",
    "    report = []\n",
    "    for result in response.call_results:\n",
    "        if result.parsed_response:\n",
    "            resp = result.parsed_response['results']\n",
    "            for item in resp:\n",
    "                item['file'] = result.files_evaluated[0] \n",
    "                item['success'] = result.success\n",
    "                report.append(item)\n",
    "        else:\n",
    "            report.append({\n",
    "                'ID': '2.1', # FIXME\n",
    "                'Title': '',\n",
    "                'Requirement': '',\n",
    "                'Observation': '',\n",
    "                'Functions': [],\n",
    "                'Evaluation': '',\n",
    "                'Score': 0,\n",
    "                'file': result.files_evaluated[0],\n",
    "                'success': result.success\n",
    "            })\n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "def extract_file_and_scores(resp_path, verbose=False):\n",
    "    if verbose:\n",
    "        print(resp_path)\n",
    "    with open(resp_path, 'rb') as file:\n",
    "        response = pickle.load(file)\n",
    "    report = get_report(response)\n",
    "    df = (\n",
    "        report\n",
    "        .pivot(index='file', columns='ID', values='Score')\n",
    "        .rename_axis(None, axis=1)\n",
    "    )\n",
    "    df['success'] = report.groupby(['file'])['success'].all()\n",
    "    df['response_path'] = resp_path\n",
    "    return df.reset_index()\n",
    "\n",
    "def generate_stat_plot(df_repo__stat, ground_truth=None, facet_col='repo', repo=None, id=None):\n",
    "    \"\"\"\n",
    "    Generate Stat plot across all repo and all checklist item\n",
    "    Optional to incorporate ground truth and select specific repo/checklist item\n",
    "    \"\"\"\n",
    "    if facet_col == 'repo':\n",
    "        x_col = 'id'\n",
    "        x_title = 'Checklist ID'\n",
    "    elif facet_col == 'id':\n",
    "        x_col = 'repo'\n",
    "        x_title = 'Repository'\n",
    "    \n",
    "    # the base chart\n",
    "    if repo:\n",
    "        df_repo__stat = df_repo__stat.query(f'repo == \"{repo}\"')\n",
    "    if id:\n",
    "        df_repo__stat = df_repo__stat.query(f'id == \"{id}\"')\n",
    "    \n",
    "    base = alt.Chart().transform_calculate(\n",
    "        min=\"max(0, datum.mean-datum.std)\",\n",
    "        max=\"min(1, datum.mean+datum.std)\"\n",
    "    )\n",
    "    \n",
    "    # generate the points\n",
    "    points = base.mark_point(\n",
    "        filled=True,\n",
    "        size=50,\n",
    "        color='black'\n",
    "    ).encode(\n",
    "        x=alt.X(f'{x_col}:O').axis(labelAngle=0).title(x_title),\n",
    "        y=alt.Y('mean:Q').scale(domainMin=0, domainMax=1).title('Score'),\n",
    "    )\n",
    "    \n",
    "    # generate the error bars\n",
    "    errorbars = base.mark_errorbar().encode(\n",
    "        x=f\"{x_col}:O\",\n",
    "        y=alt.Y(\"min:Q\").title('1 SD'),\n",
    "        y2=\"max:Q\"\n",
    "    )\n",
    "\n",
    "    plot = points + errorbars\n",
    "    \n",
    "    if ground_truth is not None:\n",
    "        # generate points of ground truth\n",
    "        if repo:\n",
    "            ground_truth = ground_truth.query(f'repo == \"{repo}\"')\n",
    "        if id:\n",
    "            ground_truth = ground_truth.query(f'id == \"{id}\"')\n",
    "        \n",
    "        df_repo__stat = pd.merge(df_repo__stat, ground_truth, how='left', on=['repo', 'id'])\n",
    "        \n",
    "        gt_points = alt.Chart().mark_point(\n",
    "            filled=True,\n",
    "            size=100,\n",
    "            color='green',\n",
    "            shape=\"diamond\"\n",
    "        ).encode(\n",
    "            x=alt.X(f'{x_col}:O'),\n",
    "            y=alt.Y('score:Q')\n",
    "        )\n",
    "\n",
    "        plot += gt_points\n",
    "\n",
    "    plot = alt.layer(\n",
    "                plot,\n",
    "                data=df_repo__stat\n",
    "            ).properties(\n",
    "                width=400,\n",
    "            ).facet(\n",
    "                column=f'{facet_col}',\n",
    "                columns=2\n",
    "            )\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e9615-c929-4fd1-9f6f-771fbd5d43d0",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55be151-3c8a-44b3-985c-7da87e3566b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checklist_ids = ['2.1', '3.2', '3.5', '4.2', '5.3', '6.1', '6.2']\n",
    "\n",
    "result_path = '../draft/batch_run_results/record_combine.yml'\n",
    "with open(result_path, 'r') as file:\n",
    "    config = pd.DataFrame(yaml.safe_load(file))\n",
    "\n",
    "# prepare score data by repo, run, file\n",
    "tmp = [\n",
    "    extract_file_and_scores(path) for path in config['response_path'] # FIXME: excluded deepchem\n",
    "]\n",
    "tmp = pd.concat(tmp, axis=0).reset_index(drop=True)\n",
    "\n",
    "raw_df_repo_run_file = config.merge(tmp, on='response_path', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a6fc2-a143-4117-8432-e32465e3fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter non-test files in qlib\n",
    "df_repo_run_file = raw_df_repo_run_file.query('(repo != \"qlib\") | (file.str.contains(\"../data/raw/openja/qlib/tests/\"))')\n",
    "\n",
    "# prepare score data by repo, run\n",
    "df_repo_run = df_repo_run_file.groupby(['repo', 'run']).agg({\n",
    "    id: ['max'] for id in checklist_ids\n",
    "})\n",
    "df_repo_run.columns = [col[0] for col in df_repo_run.columns]\n",
    "df_repo_run = df_repo_run.reset_index()\n",
    "\n",
    "# prepare statistics of scores by repo\n",
    "df_repo__stat = df_repo_run.groupby(['repo']).agg({\n",
    "    id: ['mean', 'std', 'count'] for id in checklist_ids\n",
    "})\n",
    "df_repo__stat = pd.melt(df_repo__stat.reset_index(), id_vars=[('repo', '')])\n",
    "df_repo__stat.columns = ['repo', 'id', 'stat', 'value']\n",
    "df_repo__stat = (\n",
    "    df_repo__stat.pivot(index=['repo', 'id'], columns='stat', values='value')\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "\n",
    "# prepare counting of scores by repo\n",
    "df_repo__count = df_repo_run.groupby(['repo'])['2.1'].apply(Counter).reset_index()\n",
    "for id in checklist_ids[1:]:\n",
    "    df_repo__count = df_repo__count.merge(\n",
    "        df_repo_run.groupby(['repo'])[id].apply(Counter).reset_index(),\n",
    "        on=['repo', 'level_1'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "df_repo__count = df_repo__count.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108c053-1646-4203-80b2-110d95ce8af7",
   "metadata": {},
   "source": [
    "### Runs Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c784c-8634-4c26-9136-f31a6ea42fbf",
   "metadata": {},
   "source": [
    "#### 1. Some non-test files are included in the evaluation\n",
    "\n",
    "For example, the `./nanodet/nanodet/trainer/task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efb11a-2ad3-43e0-a7b8-8795fc5a1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_repo_run_file.query('repo == \"nanodet\"')['file'].unique()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804c368-7a96-48ae-9f7d-c30e0c2cd1fa",
   "metadata": {},
   "source": [
    "#### 2. Evaluation on the file `magenta/magenta/models/music_vae/data_test.py` is always failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82a59-9272-4025-94c0-6f440e3e963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file[~df_repo_run_file.success]['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28bf5-7ef0-4528-aeba-6b32d4bbf98c",
   "metadata": {},
   "source": [
    "#### 3. `DeepSpeech`, `lightfm` and `magenta` have the least (Python) test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e52f61-1fa7-4ebd-aa2d-40e441600ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('run == 1').groupby(['repo'])['file'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215fdc0-663d-4556-af39-b34d3be3c487",
   "metadata": {},
   "source": [
    "#### 4. The test files are not always in a `tests/` folder. Is it be good practice to always do that? Should it be one of the checklist item to ensure all tests placed under `tests/` folder?\n",
    "\n",
    "For example, `magenta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93a571-d215-406b-9fbf-d0528de60b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.query('repo == \"magenta\"')['file'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67060f0f-ba59-4ac0-bc84-04853b2f5216",
   "metadata": {},
   "source": [
    "### Findings on 8 repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad96150-ef13-44fb-95fc-d0657b6a866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_file.repo.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18118436-6899-465d-bae5-feafaa49a293",
   "metadata": {},
   "source": [
    "#### 1. Overview of accuracy and consistency `lightfm` evaluation\n",
    "\n",
    "Let the ground truth of the `lightfm` is as the [following](https://github.com/UBC-MDS/test-creation/blob/69d61a9f5ac62baefca23ee293a6cd09fe41eeb2/report/repo_human_evaluation/human_evaluation_report-lightfm.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893d9a5-4dba-4ed7-ad93-6e26f004876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "ground_truth = pd.DataFrame([\n",
    "    {'repo': 'lightfm', 'id': '2.1', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '3.2', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '3.5', 'score': 0},\n",
    "    {'repo': 'lightfm', 'id': '4.2', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '5.3', 'score': 0.5},\n",
    "    {'repo': 'lightfm', 'id': '6.1', 'score': 1},\n",
    "    {'repo': 'lightfm', 'id': '6.2', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '2.1', 'score': 0.5},\n",
    "    {'repo': 'qlib', 'id': '3.2', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '3.5', 'score': 0},\n",
    "    {'repo': 'qlib', 'id': '4.2', 'score': 0.5},\n",
    "    {'repo': 'qlib', 'id': '5.3', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '6.1', 'score': 1},\n",
    "    {'repo': 'qlib', 'id': '6.2', 'score': 1},\n",
    "])\n",
    "ground_truth[ground_truth.repo == 'lightfm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a30b6-924e-4b92-92bb-ac16896ddad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f58d6-e16c-4fb3-ba73-1981c375a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"lightfm\", facet_col='repo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553667f-d2aa-4349-8bb8-12807f83871e",
   "metadata": {},
   "source": [
    "The distribution of the scores for each checklist items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fbb35-d05f-4c0c-bb2f-15434446f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"lightfm\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a01ee9-515e-4b30-9acb-0110de9dd16d",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "The system evaluation kind of aligns with our evaluation, that is,\n",
    " - for those items that we believe \"Satisfied\" (Score = 1), the system mostly output 0.5 or 1\n",
    " - for those items that we believe \"Partially Satisfied\" or \"Not Satisfied\", the system mostly output 0.5 or 0\n",
    " - some checklist items display high variance, e.g. 3.5, 5.3 and 6.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf7f4-14e8-4fed-bd24-34281772624f",
   "metadata": {},
   "source": [
    "#### 2. Overview of `qlib`\n",
    "Let the ground truth of the `qlib` is as the following (FIXME: to be confirmed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6255ae9-ec5d-4088-aee0-11b05d89f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "ground_truth[ground_truth.repo == 'qlib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e12df-0077-44e1-891d-206f1c5f6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"qlib\", facet_col='repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3f4b-8940-48ba-9b6b-7c8a8856e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__count.query('repo == \"qlib\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558e093-6732-40fa-89eb-c6a426904f06",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "- There are more disagreement between system and manual evaluation\n",
    "  - especially for 5.3, 6.1, 6.2.\n",
    "- The items consistency in this repo are not similar to those in `lightfm`.\n",
    "  - e.g. Variance for 3.5 is greatly reduced. Variance for 3.2 becomes larger.\n",
    "- However, `qlib` is not just a machine learning project, it also contains a software inside.\n",
    "  - e.g. It has a lot of randomly generated data by itself, instead of reading a data to perform analysis, it seems to deviate from the objective of 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffd6db-e68b-495a-bc6a-050b593cfcf3",
   "metadata": {},
   "source": [
    "#### 3. The consistency for each checklist items\n",
    " - Why is it important?\n",
    "   If the score of a particular item varies a lot when evaluating a repository, it might mean that its prompt (`Requirement`) is confusing to the LLM, or the checklist item itself is not well defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b154cba-6f2c-4423-bf20-97cce23e1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo__stat.pivot(index='id', columns='repo', values='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04455456-9516-44a7-ba7e-8f6c76012a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_repo__stat).mark_boxplot().encode(\n",
    "    x=\"std:Q\",\n",
    "    y='id:N'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc7998-7d58-426d-942e-5f4d9c94c6f5",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 2.1 `Ensure Data File Loads as Expected` is usually stable.\n",
    "   - When evaluating a repository, 50% of the time its standard deviation is smaller than 0.05, the smallest among the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5eb994-adf1-47e1-8ad1-49e57e1f3b4b",
   "metadata": {},
   "source": [
    "Below shows the breakdown of item scores for each repository:  \n",
    "(NOTE: only `lightfm` and `qlib` have ground truth, in green diamond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e72eb-03a0-4718-85aa-04989c6e44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stat_plot(df_repo__stat, ground_truth=ground_truth, facet_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc620a-e7ae-483f-8bb7-e327d49a03d4",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - (TBC) The standard deviation for Item 3.5 and 5.3 shows great variation, which might imply that test cases in some repo might be confusing to LLM while some are clear.\n",
    " - (TBC) The standard deviation for Item 5.3, 6.1, 6.2 are relatively high and consistent, which might imply that there is room for refining the prompt to reduce consistency issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc16e-fc8d-42ec-9680-485dbdcc0d36",
   "metadata": {},
   "source": [
    "#### 4. The consistency for each checklist items, compared to the `lightfm`\n",
    " - Why is it important? We optimized the consistency of our system using `lightfm`. Therefore, we treat this repository as a benchmark. If a particular checklist item has a much worse consistency in other repository, that might mean that the prompt for that item is not generalizable.\n",
    "\n",
    "Below shows the standard deviations in a 30 runs for each checklist item for each repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbe813-87bd-4e0a-a368-40b920f3923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df_repo__stat[['repo', 'std', 'id']].pivot(index='repo', columns='id')\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ecf33-efac-407d-baac-0d2eeb61f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds_p = stds.copy()\n",
    "stds_p.columns = [col[1] for col in stds_p.columns]\n",
    "stds_p = stds_p.reset_index()\n",
    "stds_p = stds_p.melt(id_vars='repo', var_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430735d-b0f7-4092-8af6-80d24f43f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6ad28-25d7-4957-b9aa-5facf2c75f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripplot = (\n",
    "#     alt.Chart(stds_p)\n",
    "#     .mark_point(filled=True, size=100)\n",
    "#     .transform_calculate( \n",
    "#         # Generate Gaussian jitter with a Box-Muller transform \n",
    "#         jitter='sqrt(-2*log(random()))*cos(2*PI*random())'\n",
    "#         # jitter='random()'\n",
    "#     ).encode( \n",
    "#         y=alt.Y( \n",
    "#             'jitter:Q', \n",
    "#             title=None, \n",
    "#             axis=alt.Axis(ticks=False, grid=True, labels=False), \n",
    "#             scale=alt.Scale(), \n",
    "#         ), \n",
    "#         x=alt.X('value:Q'), \n",
    "#         color=alt.Color('repo:N'),\n",
    "#         row=alt.Row( \n",
    "#             'id:N',\n",
    "#             header=alt.Header(\n",
    "#                 labelFontSize=16,\n",
    "#                 labelAngle=0\n",
    "#             )\n",
    "#         ),\n",
    "#         tooltip='repo'\n",
    "#     ).configure_facet( \n",
    "#         spacing=0\n",
    "#     ).configure_view( \n",
    "#         stroke=None\n",
    "#     ).configure_axis( \n",
    "#         labelFontSize=16, \n",
    "#         titleFontSize=16\n",
    "#     ).properties(\n",
    "#         height=50, \n",
    "#         width=600\n",
    "#     ) \n",
    "# )\n",
    "    \n",
    "# stripplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2629c-eb21-496a-8166-33d44e1c89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jitterbox_plot(df_stds_p):\n",
    "    \"\"\"\n",
    "    Generate jitterbox plot across all repo and all checklist item\n",
    "    \"\"\"\n",
    "    box = alt.Chart().mark_boxplot(\n",
    "        color='grey',\n",
    "        opacity=0.5,\n",
    "        size=20,\n",
    "    ).encode(\n",
    "        x=alt.X('value:Q').title('SD(Score)'),\n",
    "        y=alt.Y('id:N', title=None, axis=alt.Axis(labelPadding=10, grid=False))\n",
    "    )\n",
    "    \n",
    "    stripplot = alt.Chart().mark_circle(size=100).encode(\n",
    "        y=alt.Y( \n",
    "            'id:N',\n",
    "            axis=alt.Axis(ticks=False, grid=True, labels=True), \n",
    "            scale=alt.Scale(), \n",
    "        ), \n",
    "        x='value:Q',\n",
    "        yOffset=\"jitter:Q\",\n",
    "        color=alt.Color('id:N', legend=None),\n",
    "        tooltip='repo'\n",
    "    ).transform_calculate(\n",
    "        # Generate Gaussian jitter with a Box-Muller transform\n",
    "        jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\"\n",
    "    )\n",
    "    \n",
    "    plot = alt.layer(\n",
    "        box,\n",
    "        stripplot,\n",
    "        data=df_stds_p\n",
    "    ).configure_view( \n",
    "        stroke=None\n",
    "    ).configure_axis( \n",
    "        labelFontSize=16, \n",
    "        titleFontSize=16\n",
    "    ).properties(\n",
    "        height=300, \n",
    "        width=600\n",
    "    ) \n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fcf56-8fc6-4241-8a6b-b86a618ddb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_jitterbox_plot(stds_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db9a51-29da-4913-8c10-1dba36a52a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_repo__stat).mark_boxplot().encode(\n",
    "    x=\"std:Q\",\n",
    "    y='id:N'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cf03c-b48d-400e-ae3d-a4a7f434c8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install altair_catplot\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833bbcd-ba7b-4f01-999b-16e0be3b99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import altair_catplot\n",
    "\n",
    "# altair_catplot.catplot(\n",
    "#     stds_p, \n",
    "#     transform ='jitterbox', \n",
    "#     mark ='point', \n",
    "#     encoding = dict(\n",
    "#         x = alt.X('value:Q'), \n",
    "#         y = alt.Y('id:N'), \n",
    "#         color = alt.Color('repo:N')\n",
    "#     ) \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5883b9a-c852-4d2f-82fd-7f5434edcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = stds.drop(index='lightfm') / stds.loc['lightfm']\n",
    "\n",
    "base = alt.Chart(\n",
    "    F.melt(ignore_index=False).reset_index()[['repo', 'id', 'value']]\n",
    ").transform_calculate(\n",
    "    benchmark=\"1\",\n",
    "    threshold=f\"{scipy.stats.f.ppf(0.975, 29, 29)}\"\n",
    ")\n",
    "\n",
    "point = base.mark_point(\n",
    "    filled=True,\n",
    "    size=100,\n",
    ").encode(\n",
    "    x=alt.X('value:Q').title(\"std ratio (c.f. lightfm)\"),\n",
    "    y='id:N',\n",
    "    color='repo',\n",
    "    tooltip='repo'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=400\n",
    ")\n",
    "\n",
    "point \\\n",
    "+ base.mark_rule(color='black').encode(x=\"benchmark:Q\") \\\n",
    "+ base.mark_rule(color='red').encode(x=\"threshold:Q\")\n",
    "# jitter instead of mark_point <-- prompt vs. repo problem?\n",
    "# prompt: sd of checklist item for all repo is high\n",
    "# repo: most of repo have low sd, the repo we're looking at has outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef79ff-a4fa-4412-973d-7e4fdfe1be71",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    " - The evaluation of the checklist item 3.2 `Data in the Expected Format` becomes much more unstable in most of other repositories.\n",
    " - That of the 2.1 is significantly unstable in the repo `paperless-ng`, `magenta` and `DeepSpeech`, but it may be due to the repo itself.\n",
    "\n",
    "TODO: to look into the 3.2's scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860559f-bde8-47c5-ac35-94797beb6054",
   "metadata": {},
   "source": [
    "#### TODO: Given ground truth == 1, distribution of system score?\n",
    "#### TODO: Given ground truth == 0, distribution of system score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1179d8-ef8c-46dd-878b-55e24f848764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histogram_plot(df_repo_run_long, df_ground_truth=None, repo=None, id=None):\n",
    "    \"\"\"\n",
    "    Generate histogram across all repo and all checklist item\n",
    "    Optional to incorporate ground truth and select specific repo/checklist item\n",
    "    \"\"\"\n",
    "    # data\n",
    "    repo_data = df_repo_run_long.copy()\n",
    "    if repo:\n",
    "        repo_data = repo_data.query(f'repo == \"{repo}\"')\n",
    "    if id:\n",
    "        repo_data = repo_data.query(f'id == \"{id}\"')\n",
    "\n",
    "    # base histogram chart\n",
    "    base = alt.Chart().mark_bar().encode(\n",
    "                x=alt.X('eval_score:Q', title='Score'), \n",
    "                y=alt.Y('count()'), \n",
    "                color=alt.value('grey'),\n",
    "                size=alt.value(20),\n",
    "            )\n",
    "    \n",
    "    if df_ground_truth is not None:\n",
    "        # data\n",
    "        gt_data = df_ground_truth.copy()\n",
    "        if repo:\n",
    "            gt_data = gt_data.query(f'repo == \"{repo}\"')\n",
    "        if id:\n",
    "            gt_data = gt_data.query(f'id == \"{id}\"')\n",
    "        \n",
    "        repo_data = pd.merge(repo_data, gt_data, how='left', on=['repo', 'id'])\n",
    "        repo_data['is_equal_to_gt'] = repo_data['eval_score'] == repo_data['score']\n",
    "        \n",
    "        # base histogram chart\n",
    "        base = base.encode(\n",
    "                    color=alt.Color('is_equal_to_gt', scale=alt.Scale(range=['grey', 'green']), legend=None)\n",
    "                )\n",
    "        base += base.mark_text().encode(\n",
    "            text=alt.value('Ground Truth'),\n",
    "            x='score',\n",
    "            size=alt.value(10),\n",
    "            color=alt.value('green'),\n",
    "        )\n",
    "\n",
    "    plot = alt.layer(\n",
    "                base,\n",
    "                data=repo_data\n",
    "            ).properties(\n",
    "                width=200,\n",
    "                height=200,\n",
    "            ).facet(\n",
    "                row='repo',\n",
    "                column='id'\n",
    "            )        \n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f6f5e-d04a-4a23-a797-b7b4a221526f",
   "metadata": {},
   "source": [
    "#### Contingency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e22d2-f800-4821-bafe-493fb225c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_run_p = pd.melt(df_repo_run, id_vars=['repo', 'run'], var_name='id', value_name='eval_score')\n",
    "df_repo_run_p = pd.merge(df_repo_run_p, ground_truth, how='inner', on=['repo', 'id'])\n",
    "df_repo_run_p = df_repo_run_p.rename(columns={'score': 'ground_truth'})\n",
    "pd.pivot_table(df_repo_run_p, values='run', index=['repo', 'ground_truth'], columns=['eval_score'], aggfunc='count', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fa41b-eed6-47ea-bdb2-6d0a424f91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_histogram_plot(df_repo_run_p, df_ground_truth=ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7f99d-9e65-4eaf-911e-44ca1c0eb77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation2]",
   "language": "python",
   "name": "conda-env-test-creation2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
