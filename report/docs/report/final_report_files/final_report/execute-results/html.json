{
  "hash": "60dde13b85a694da0c103f9a9b3d6e46",
  "result": {
    "markdown": "---\nformat:\n  html:\n    code-fold: true\nbibliography: references.bib\ntitle: Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis\n---\n\n\n\n\n\nby John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin\n\n## Executive Summary\n\n#FIXME\n\n## Introduction\n\n### Problem Statement\n\nThe global artificial intelligence (AI) market is growing exponentially ([@grand2021artificial]), driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.\n\nHowever, ensuring the software quality of these systems remains a significant challenge ([@openja2023studying]). Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as misinformation ([@Ashley2024]), social bias ([@Alice2023]), substantial financial losses ([@Asheeta2019]) and safety hazards ([@David2023])\n\nTherefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?\n\n### Our Objectives\n\nWe propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate checklists based on ML research papers and best practices to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software's trustworthiness, quality, and reproducibility across both the industry and academia [@kapoor2022leakage].\n\n## Data Science Methods\n\n### Current Approaches\n\nTo ensure the reproducibility, trustworthiness, and lack of bias in ML systems, comprehensive testing is essential. We outlined some traditional approaches for assessing the completeness of ML system tests with their advantages and drawbacks as follows.\n\n1. **Code Coverage**\n\nCode coverage measures the proportion of source code of a program executed when a particular test suite is run. Widely used in software development, it quantifies test quality and is scalable due to its short processing time. However, it cannot indicate the reasons or specific ML areas where the test suites fall short under the context of ML system development.\n\n2. **Manual Evaluation**\n\nManual evaluation involves human experts reviewing the source code, whom can take the business logic into considerations and identify vulnerabilites. It often provides context-specific improvement suggestions and remains one of the most reliable practices ([@openja2023studying], [@alexander2023evaluating]). However, it is time-consuming and not scalable due to the scarcity of human experts. Moreover, different experts might put emphasis on different ML test areas and lack a comprehensive and holistic review of the ML system test suites.\n\n### Our Approach\n\nOur approach is to deliver an automated code review tool with the best practices of ML test suites embedded. This tool aims to educate ML users on best practices while providing comprehensive evaluations of their ML system codes.\n\nTo establish these best practices, we utilized data from ML research papers and recognized online resources. In collaboration with our partner, we researched industrial best practices ([@msise2023], [@jordan2020]) and academic literature ([@openja2023studying]), and consolidated testing strategies into a human-readable and machine-friendly checklist that can be embedded into the automated tool.\n\nFor development, we collected 11 GitHub repositories of ML projects as studied in [@openja2023studying]. These Python-based projects include comprehensive test suites. Our tool should be able to analyze these test suites, compare them with embedded best practices, and deliver evaluations.\n\nWe expect that our approach will provide scalable and reliable test suite evaluations for multiple ML projects. However, we recognize that our current best practices only focus on a few high-priority test areas due to time constraints. We plan to expand this scope in the future. While our tool's evaluations are not yet as reliable as human evaluations, we will quantify its performance.\n\n### Success Metrics\n\nTo properly assess the performance of our tool which leverages LLMs capability, we have taken reference of the methods in [@alexander2023evaluating] and defined two success metrics: accuracy and consistency. These metrics will help users (researchers, ML engineers, etc.) gauge the trustworthiness of our tool's evaluation results.\n\n1.  **Accuracy vs Human Expert Judgement**\n\nWe run our tool on ML projects from [@openja2023studying] to obtain evaluation results for each ML checklist item. These results are then compared with our manually assessed ground truth data based on the same criteria. Accuracy is calculated as the proportion of matching results to the total number of results.\n\n2.  **Consistency**\n\nWe perform multiple runs on each ML project to obtain evaluation results for each checklist item. Consistency is measured by calculating the standard deviation of these results across multiple runs for each project.\n\n## Data Product & Results\n\n### Data Products\n\nOur solution includes a curated checklist for robust ML testing and a Python package for checklist-based evaluation of ML project testing robustness using LLMs. The package is publicly available on the Python Packaging Index (PyPI).\n\nJustifications for these products are:\n\n- Checklists have been shown to reduce errors in software systems and promote code submissions ([@Atul2010], [@pineau2021improving]).\n- Python is widely used in ML, compatible with various OSes, and integrates well with LLMs. These ensure the ease of use and development.\n\n#### How to use the product\n\nThere are two ways to make use of this package:\n\n1.  **As a CLI tool.** A runnable command `fixml` is provided by the package. Once installed, users can perform codebase evaluations, generate test function specifications, and more by running subcommands under `fixml` in the terminal.\n\n2.  **As a high-level API.** Users can import necessary components from the package into their own systems. Documentation is available through docstrings.\n\nBy offering it as both CLI tool and API, our product is user-friendly to interact with, and versatile to support various use cases such as web application development and scientific research.\n\n#### System Design\n\n(FIXME To be revised) ![image](img/proposed_system_overview.png)\n\nThe design of our package follows object-oriented and SOLID principles, which is fully modularity. Users can easily switch between different prompts, models, and checklists, which facilitates code reusability and collaboration to extend its functionality.\n\nThere are five components in the system of our package:\n\n1.  **Code Analyzer** \n\nIt extracts test suites from the input codebase, to ensure only the most relevants details are provided to LLMs given token limits.\n\n2.  **Prompt Templates** \n\nIt stores prompt templates for instructing LLMs to generate responses in the expected format.\n\n3.  **Checklist** \n\nIt reads the curated checklist from a CSV file into a dictionary with a fixed schema for LLM injection. The package includes a default checklist for distribution.\n\n4.  **Runners** \n\nIt includes the Evaluator module, which assesses each test suite file using LLMs and outputs evaluation results, and the Generator module, which creates test specifications. Both modules feature validation, retry logic, and record response and relevant information.\n\n5.  **Parsers** \n\nIt converts Evaluator responses into evaluation reports in various formats (HTML, PDF) using the Jinja template engine, which enables customizable report structures.\n\n#### Checklist Design\n\nThe embedded checklist contains best practices for testing ML pipelines, and is curated from ML research and recognized online resources. Prompt engineering further improves performance. THis helps mitigate LLM hallucinations ([@zhang2023sirens]) by ensuring strict adherence to the checklist.\n\nExample checklist structure:\n\n|                  Column | Description                                                                                          |\n|------------------:|:----------------------------------------------------|\n|                      ID | Unique Identifier of the checklist item                                                          |\n|                   Topic | Test Area of the checklist item                                                                  |\n|                   Title | Title of the checklist item                                                                      |\n|             Requirement | Prompt for the checklist item to be injected into LLMs for evaluation                             |\n|            Explanations | Detailed explanations for human understanding                                  |\n|               Reference | References for the checklist item, e.g., academic papers                                                |\n| Is Evaluator Applicable | Indicates if the checklist item is used during evaluation (0 = No, 1 = Yes) |\n\n(FIXME To be revised) <img src=\"img/checklist_sample.png\" width=\"600\" />\n\n#### Artifacts\n\nUsing our package results in three artifacts:\n\n1.  **Evaluation Responses** \n\nThese responses include both LLM evaluation results and process metadata stored in JSON format.This supports downsteam tasks like report rendering and scientific research, etc.\n\n(FIXME To be revised) schema of the JSON saved & what kind of information is stored\n\n2.  **Evaluation Report** \n\nThis report presents structured evaluation results of ML projects, which includes a detailed breakdown of completeness scores and reasons for each score.\n\n(FIXME To be revised) <img src=\"img/test_evaluation_report_sample.png\" width=\"600\" />\n\n3.  **Test Specification Script** \n\nGenerated test specifications are stored as Python scripts.\n\n(FIXME To be revised) <img src=\"img/test_spec_sample.png\" width=\"600\" />\n\n### Evaluation Results\n\nAs described in `Success Metrics`, we conducted 30 iterations on each repository from [@openja2023studying] and examined the breakdown of the completeness score to assess our tool's evaluation quality. \n\n(FIXME: would it be better to show a table of the repos? like how the Openja does?) \n\n1. **Accuracy**\n\nWe targeted 3 of the repositories ([`lightfm`](https://github.com/lyst/lightfm), [`qlib`](https://github.com/microsoft/qlib), [`DeepSpeech`](https://github.com/mozilla/DeepSpeech)) for human evaluation compared our tool's outputs with the ground truth.\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ngt = pd.read_csv('ground_truth.csv')\ngt\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>DeepSpeech</th>\n      <th>lightfm</th>\n      <th>qlib</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.1</td>\n      <td>Ensure Data File Loads as Expected</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.2</td>\n      <td>Data in the Expected Format</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.5</td>\n      <td>Check for Duplicate Records in Data</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.2</td>\n      <td>Verify Data Split Proportion</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.3</td>\n      <td>Ensure Model Output Shape Aligns with Expectation</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6.1</td>\n      <td>Verify Evaluation Metrics Implementation</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.2</td>\n      <td>Evaluate Model's Performance Against Thresholds</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> Ground truth data for the 3 repositories. (1 = fully satisfied, 0.5 = partially satisfied, 0 = not satisfied)\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo\nimport altair as alt\nimport pandas as pd\n\ndf_repo__stat = pd.read_csv('score_stat_by_repo_3.5-turbo.csv')\ngt = pd.read_csv('ground_truth.csv')\ngt = gt.melt(id_vars=['id', 'title'], var_name='repo', value_name='ground_truth')\n\ndf_repo__stat_with_gt = df_repo__stat.merge(gt, on=['id', 'title', 'repo'])\n\nbase = alt.Chart(\n    df_repo__stat_with_gt.query('repo in [\"lightfm\", \"qlib\", \"DeepSpeech\"]')\n).transform_calculate(\n    min=\"max(0, datum.mean-datum.std)\",\n    max=\"min(1, datum.mean+datum.std)\"\n)\n    \n# generate the points\npoints = base.mark_point(\n    filled=True,\n    size=50,\n    color='black'\n).encode(\n    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title(\"Score\").axis(\n        labelExpr=\"datum.value % 0.5 ? null : datum.label\"\n    ),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))#.scale(domainMin=0, domainMax=1).title('Score'),\n)\n\n# generate the points for ground truth\ngt_points = base.mark_point(\n    filled=True,\n    size=200,\n    color='green',\n    shape=\"diamond\"\n).encode(\n    x=alt.X('ground_truth:Q'),\n    y=alt.Y('id_title:N')\n)\n\n# generate the error bars\nerrorbars = base.mark_errorbar().encode(\n    x=alt.X(\"min:Q\").title('1 SD'), #\"id:N\",\n    x2=\"max:Q\",\n    y=\"id_title:N\"\n)\n\n(gt_points + points + errorbars).facet(\n    column=alt.Column('repo:N').title(None)\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n\n<style>\n  #altair-viz-317946e8a4db434693266e8ed15e214a.vega-embed {\n    width: 100%;\n    display: flex;\n  }\n\n  #altair-viz-317946e8a4db434693266e8ed15e214a.vega-embed details,\n  #altair-viz-317946e8a4db434693266e8ed15e214a.vega-embed details summary {\n    position: relative;\n  }\n</style>\n<div id=\"altair-viz-317946e8a4db434693266e8ed15e214a\"></div>\n<script type=\"text/javascript\">\n  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-317946e8a4db434693266e8ed15e214a\") {\n      outputDiv = document.getElementById(\"altair-viz-317946e8a4db434693266e8ed15e214a\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n    };\n\n    function maybeLoadScript(lib, version) {\n      var key = `${lib.replace(\"-\", \"\")}_version`;\n      return (VEGA_DEBUG[key] == version) ?\n        Promise.resolve(paths[lib]) :\n        new Promise(function(resolve, reject) {\n          var s = document.createElement('script');\n          document.getElementsByTagName(\"head\")[0].appendChild(s);\n          s.async = true;\n          s.onload = () => {\n            VEGA_DEBUG[key] = version;\n            return resolve(paths[lib]);\n          };\n          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n          s.src = paths[lib];\n        });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else {\n      maybeLoadScript(\"vega\", \"5\")\n        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 12}}, \"data\": {\"name\": \"data-0cd69fd1c95de279cdd7d1f0310bd508\"}, \"facet\": {\"column\": {\"field\": \"repo\", \"title\": null, \"type\": \"nominal\"}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"point\", \"color\": \"green\", \"filled\": true, \"shape\": \"diamond\", \"size\": 200}, \"encoding\": {\"x\": {\"field\": \"ground_truth\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"id_title\", \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true, \"size\": 50}, \"encoding\": {\"x\": {\"axis\": {\"labelExpr\": \"datum.value % 0.5 ? null : datum.label\"}, \"field\": \"mean\", \"scale\": {\"domainMin\": 0, \"domainMax\": 1}, \"title\": \"Score\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": false, \"labelLimit\": 1000, \"labelPadding\": 10}, \"field\": \"id_title\", \"title\": null, \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}, {\"mark\": {\"type\": \"errorbar\"}, \"encoding\": {\"x\": {\"field\": \"min\", \"title\": \"1 SD\", \"type\": \"quantitative\"}, \"x2\": {\"field\": \"max\"}, \"y\": {\"field\": \"id_title\", \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}]}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-0cd69fd1c95de279cdd7d1f0310bd508\": [{\"repo\": \"lightfm\", \"id\": 2.1, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Ensure Data File Loads as Expected\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"ground_truth\": 1.0}, {\"repo\": \"lightfm\", \"id\": 3.2, \"count\": 30.0, \"mean\": 0.5, \"std\": 0.0, \"title\": \"Data in the Expected Format\", \"id_title\": \"3.2. Data in the Expected Format\", \"ground_truth\": 1.0}, {\"repo\": \"lightfm\", \"id\": 3.5, \"count\": 30.0, \"mean\": 0.0, \"std\": 0.0, \"title\": \"Check for Duplicate Records in Data\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"ground_truth\": 0.0}, {\"repo\": \"lightfm\", \"id\": 4.2, \"count\": 30.0, \"mean\": 0.8166666666666667, \"std\": 0.2450662589267805, \"title\": \"Verify Data Split Proportion\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"ground_truth\": 1.0}, {\"repo\": \"lightfm\", \"id\": 5.3, \"count\": 30.0, \"mean\": 0.4833333333333333, \"std\": 0.0912870929175276, \"title\": \"Ensure Model Output Shape Aligns with Expectation\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"ground_truth\": 0.5}, {\"repo\": \"lightfm\", \"id\": 6.1, \"count\": 30.0, \"mean\": 0.9166666666666666, \"std\": 0.1895245108947258, \"title\": \"Verify Evaluation Metrics Implementation\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"ground_truth\": 1.0}, {\"repo\": \"lightfm\", \"id\": 6.2, \"count\": 30.0, \"mean\": 0.9833333333333332, \"std\": 0.0912870929175276, \"title\": \"Evaluate Model's Performance Against Thresholds\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"ground_truth\": 1.0}, {\"repo\": \"qlib\", \"id\": 2.1, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Ensure Data File Loads as Expected\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"ground_truth\": 0.5}, {\"repo\": \"qlib\", \"id\": 3.2, \"count\": 30.0, \"mean\": 0.7666666666666667, \"std\": 0.2537081317024624, \"title\": \"Data in the Expected Format\", \"id_title\": \"3.2. Data in the Expected Format\", \"ground_truth\": 1.0}, {\"repo\": \"qlib\", \"id\": 3.5, \"count\": 30.0, \"mean\": 0.1166666666666666, \"std\": 0.2150915335760381, \"title\": \"Check for Duplicate Records in Data\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"ground_truth\": 0.0}, {\"repo\": \"qlib\", \"id\": 4.2, \"count\": 30.0, \"mean\": 0.4833333333333333, \"std\": 0.2069204966986668, \"title\": \"Verify Data Split Proportion\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"ground_truth\": 0.5}, {\"repo\": \"qlib\", \"id\": 5.3, \"count\": 30.0, \"mean\": 0.55, \"std\": 0.2012889499682243, \"title\": \"Ensure Model Output Shape Aligns with Expectation\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"ground_truth\": 1.0}, {\"repo\": \"qlib\", \"id\": 6.1, \"count\": 30.0, \"mean\": 0.6333333333333333, \"std\": 0.2916461404928373, \"title\": \"Verify Evaluation Metrics Implementation\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"ground_truth\": 1.0}, {\"repo\": \"qlib\", \"id\": 6.2, \"count\": 30.0, \"mean\": 0.6, \"std\": 0.203419051086243, \"title\": \"Evaluate Model's Performance Against Thresholds\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"ground_truth\": 1.0}]}}, {\"mode\": \"vega-lite\"});\n</script>\n```\n:::\n:::\n\n\n> Comparison of our system's satisfaction determination versus the ground truth for each checklist item and repository\n\nOur tool tends to underrate satisfying cases, which often classifies fully satisfied items as partially satisfied and partially satisfied items as not satisfied.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf_repo_run = pd.read_csv('score_by_repo_run_3.5-turbo.csv')\n\ndf_repo_run = df_repo_run.merge(gt, on=['id', 'title', 'repo'])\n\ncontingency_table = pd.pivot_table(\n    df_repo_run,\n    values='run', \n    index=['repo', 'id_title', 'ground_truth'], \n    columns=['score'],\n    aggfunc='count', \n    fill_value=0\n)\ncontingency_table.index.names = ['Repository', 'Checklist Item', 'Ground Truth']\ncontingency_table.sort_index(level=[0, 2])\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>score</th>\n      <th>0.0</th>\n      <th>0.5</th>\n      <th>1.0</th>\n    </tr>\n    <tr>\n      <th>Repository</th>\n      <th>Checklist Item</th>\n      <th>Ground Truth</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">lightfm</th>\n      <th>3.5. Check for Duplicate Records in Data</th>\n      <th>0.0</th>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5.3. Ensure Model Output Shape Aligns with Expectation</th>\n      <th>0.5</th>\n      <td>1</td>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2.1. Ensure Data File Loads as Expected</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3.2. Data in the Expected Format</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4.2. Verify Data Split Proportion</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>11</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>6.1. Verify Evaluation Metrics Implementation</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>5</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6.2. Evaluate Model's Performance Against Thresholds</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">qlib</th>\n      <th>3.5. Check for Duplicate Records in Data</th>\n      <th>0.0</th>\n      <td>23</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2.1. Ensure Data File Loads as Expected</th>\n      <th>0.5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4.2. Verify Data Split Proportion</th>\n      <th>0.5</th>\n      <td>3</td>\n      <td>25</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3.2. Data in the Expected Format</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>14</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>5.3. Ensure Model Output Shape Aligns with Expectation</th>\n      <th>1.0</th>\n      <td>1</td>\n      <td>25</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6.1. Verify Evaluation Metrics Implementation</th>\n      <th>1.0</th>\n      <td>2</td>\n      <td>18</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6.2. Evaluate Model's Performance Against Thresholds</th>\n      <th>1.0</th>\n      <td>0</td>\n      <td>24</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> Contingency table of our system's satisfaction determination versus the ground truth\n\nThe accuracy issue may be attributed to a need to improve our checklist prompts.\n\n2. **Consistency**\n\nAs the completeness scores from LLMs contain randomness, we examined the consistency of completeness scores across checklist items and repositories.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nstds = df_repo__stat[['repo', 'std', 'id_title']].pivot(index='repo', columns='id_title').copy()\nstds.columns = [col[1] for col in stds.columns]\nstds = stds.reset_index()\nstds = stds.melt(id_vars='repo', var_name='id_title')\n\nbase = alt.Chart(stds)\n\nbox = base.mark_boxplot(\n    color='grey',\n    opacity=0.5,\n    size=20,\n).encode(\n    x=alt.X('value:Q').title('Standard Deviation of Scores'),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))\n)\n\nstripplot = base.mark_circle(size=100).encode(\n    y=alt.Y( \n        'id_title:N',\n        axis=alt.Axis(ticks=False, grid=True, labels=True), \n        scale=alt.Scale(), \n    ), \n    x='value:Q',\n    yOffset=\"jitter:Q\",\n    color=alt.Color('id_title:N', legend=None),\n    tooltip='repo'\n).transform_calculate(\n    # Generate Gaussian jitter with a Box-Muller transform\n    jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\"\n)\n\n(\n    box + stripplot\n).configure_view( \n    stroke=None\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n).properties(\n    height=300, \n    width=600,\n    title=\"30 Runs on Openja's Repositories for each Checklist Item\"\n) \n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n\n<style>\n  #altair-viz-c81889d3af6f4deea4e2294c572c7ad6.vega-embed {\n    width: 100%;\n    display: flex;\n  }\n\n  #altair-viz-c81889d3af6f4deea4e2294c572c7ad6.vega-embed details,\n  #altair-viz-c81889d3af6f4deea4e2294c572c7ad6.vega-embed details summary {\n    position: relative;\n  }\n</style>\n<div id=\"altair-viz-c81889d3af6f4deea4e2294c572c7ad6\"></div>\n<script type=\"text/javascript\">\n  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-c81889d3af6f4deea4e2294c572c7ad6\") {\n      outputDiv = document.getElementById(\"altair-viz-c81889d3af6f4deea4e2294c572c7ad6\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n    };\n\n    function maybeLoadScript(lib, version) {\n      var key = `${lib.replace(\"-\", \"\")}_version`;\n      return (VEGA_DEBUG[key] == version) ?\n        Promise.resolve(paths[lib]) :\n        new Promise(function(resolve, reject) {\n          var s = document.createElement('script');\n          document.getElementsByTagName(\"head\")[0].appendChild(s);\n          s.async = true;\n          s.onload = () => {\n            VEGA_DEBUG[key] = version;\n            return resolve(paths[lib]);\n          };\n          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n          s.src = paths[lib];\n        });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else {\n      maybeLoadScript(\"vega\", \"5\")\n        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"stroke\": null}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 12}}, \"layer\": [{\"mark\": {\"type\": \"boxplot\", \"color\": \"grey\", \"opacity\": 0.5, \"size\": 20}, \"encoding\": {\"x\": {\"field\": \"value\", \"title\": \"Standard Deviation of Scores\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": false, \"labelLimit\": 1000, \"labelPadding\": 10}, \"field\": \"id_title\", \"title\": null, \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"id_title\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": {\"field\": \"repo\", \"type\": \"nominal\"}, \"x\": {\"field\": \"value\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true, \"labels\": true, \"ticks\": false}, \"field\": \"id_title\", \"scale\": {}, \"type\": \"nominal\"}, \"yOffset\": {\"field\": \"jitter\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"sqrt(-2*log(random()))*cos(2*PI*random())\", \"as\": \"jitter\"}]}], \"data\": {\"name\": \"data-6781bc08998d89e40d13eed2c6299b07\"}, \"height\": 300, \"title\": \"30 Runs on Openja's Repositories for each Checklist Item\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-6781bc08998d89e40d13eed2c6299b07\": [{\"repo\": \"lightfm\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"value\": 0.0}, {\"repo\": \"magenta\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"value\": 0.0912870929175276}, {\"repo\": \"mmf\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"value\": 0.0}, {\"repo\": \"nanodet\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"value\": 0.0}, {\"repo\": \"qlib\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"value\": 0.0}, {\"repo\": \"lightfm\", \"id_title\": \"3.2. Data in the Expected Format\", \"value\": 0.0}, {\"repo\": \"magenta\", \"id_title\": \"3.2. Data in the Expected Format\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"3.2. Data in the Expected Format\", \"value\": 0.2330457998496995}, {\"repo\": \"nanodet\", \"id_title\": \"3.2. Data in the Expected Format\", \"value\": 0.2542738138578039}, {\"repo\": \"qlib\", \"id_title\": \"3.2. Data in the Expected Format\", \"value\": 0.2537081317024624}, {\"repo\": \"lightfm\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"value\": 0.0}, {\"repo\": \"magenta\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"value\": 0.2537081317024624}, {\"repo\": \"nanodet\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"value\": 0.0}, {\"repo\": \"qlib\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"value\": 0.2150915335760381}, {\"repo\": \"lightfm\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"value\": 0.2450662589267805}, {\"repo\": \"magenta\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"value\": 0.1728729518208802}, {\"repo\": \"nanodet\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"value\": 0.0}, {\"repo\": \"qlib\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"value\": 0.2069204966986668}, {\"repo\": \"lightfm\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"value\": 0.0912870929175276}, {\"repo\": \"magenta\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"value\": 0.2491364395612199}, {\"repo\": \"nanodet\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"value\": 0.0}, {\"repo\": \"qlib\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"value\": 0.2012889499682243}, {\"repo\": \"lightfm\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"value\": 0.1895245108947258}, {\"repo\": \"magenta\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"value\": 0.1525642883146823}, {\"repo\": \"nanodet\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"value\": 0.2491364395612199}, {\"repo\": \"qlib\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"value\": 0.2916461404928373}, {\"repo\": \"lightfm\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"value\": 0.0912870929175276}, {\"repo\": \"magenta\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"value\": 0.0}, {\"repo\": \"mmf\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"value\": 0.2248882225544018}, {\"repo\": \"nanodet\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"value\": 0.1268540658512312}, {\"repo\": \"qlib\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"value\": 0.203419051086243}]}}, {\"mode\": \"vega-lite\"});\n</script>\n```\n:::\n:::\n\n\n> Standard deviations of the score for each checklist item. Each dot represents the standard deviation of scores from 30 runs of a single repository.\n\nWe identified two diverging cases:\n\ni. **High Standard Deviations** \n\nItems like `3.2 Data in the Expected Format` showed high standard deviations across repositories. This might indicate potential poor prompt quality for the LLM to produce consistent results. Improved prompt engineering could address this issue.\n\nii. **Outliers with High Standard Deviations**\n\nItems like `5.3 Ensure Model Output Shape Aligns with Expectation` had outliers with exceptionally high standard deviations, which is possibly due to unorthodox repositories. A careful manual examination is required for a more definitive conclusion.\n\n#### Comparison of `gpt-3.5-turbo` and `gpt-4o`\n\nTo evaluate if newer LLMs improve performance, we preliminarily compared outputs from `gpt-4o` and `gpt-3.5-turbo` on the `lightfm` repository. We observed that `gpt-4o` consistently returned \"Satisfied,\" which deviated from the ground truth.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo\ndf_repo_4o__stat = pd.read_csv('score_stat_by_repo_4o.csv')\ndf_repo_4o__stat_with_gt = df_repo_4o__stat.merge(gt, on=['id', 'title', 'repo'])\ndf_repo_4o__stat_with_gt['model'] = 'gpt-4o'\n\ndf_repo_35turbo__stat_with_gt = df_repo__stat_with_gt.query(\"repo == 'lightfm'\").copy()\ndf_repo_35turbo__stat_with_gt['model'] = 'gpt-3.5-turbo'\n\ndf_model_comp = pd.concat(\n    (df_repo_35turbo__stat_with_gt, df_repo_4o__stat_with_gt), \n    axis=0\n)\n\nbase = alt.Chart(\n    df_model_comp\n).transform_calculate(\n    min=\"max(0, datum.mean-datum.std)\",\n    max=\"min(1, datum.mean+datum.std)\"\n)\n    \n# generate the points\npoints = base.mark_point(\n    filled=True,\n    size=50,\n    color='black'\n).encode(\n    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title(\"Score\").axis(\n        labelExpr=\"datum.value % 0.5 ? null : datum.label\"\n    ),\n    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))#.scale(domainMin=0, domainMax=1).title('Score'),\n)\n\n# generate the points for ground truth\ngt_points = base.mark_point(\n    filled=True,\n    size=200,\n    color='green',\n    shape=\"diamond\"\n).encode(\n    x=alt.X('ground_truth:Q'),\n    y=alt.Y('id_title:N')\n)\n\n# generate the error bars\nerrorbars = base.mark_errorbar().encode(\n    x=alt.X(\"min:Q\").title('1 SD'), #\"id:N\",\n    x2=\"max:Q\",\n    y=\"id_title:N\"\n)\n\n(gt_points + points + errorbars).facet(\n    column=alt.Column('model:N').title(None)\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n\n<style>\n  #altair-viz-618f5c8c76b7419c866b1a2a93c03f41.vega-embed {\n    width: 100%;\n    display: flex;\n  }\n\n  #altair-viz-618f5c8c76b7419c866b1a2a93c03f41.vega-embed details,\n  #altair-viz-618f5c8c76b7419c866b1a2a93c03f41.vega-embed details summary {\n    position: relative;\n  }\n</style>\n<div id=\"altair-viz-618f5c8c76b7419c866b1a2a93c03f41\"></div>\n<script type=\"text/javascript\">\n  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-618f5c8c76b7419c866b1a2a93c03f41\") {\n      outputDiv = document.getElementById(\"altair-viz-618f5c8c76b7419c866b1a2a93c03f41\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n    };\n\n    function maybeLoadScript(lib, version) {\n      var key = `${lib.replace(\"-\", \"\")}_version`;\n      return (VEGA_DEBUG[key] == version) ?\n        Promise.resolve(paths[lib]) :\n        new Promise(function(resolve, reject) {\n          var s = document.createElement('script');\n          document.getElementsByTagName(\"head\")[0].appendChild(s);\n          s.async = true;\n          s.onload = () => {\n            VEGA_DEBUG[key] = version;\n            return resolve(paths[lib]);\n          };\n          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n          s.src = paths[lib];\n        });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else {\n      maybeLoadScript(\"vega\", \"5\")\n        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 12}}, \"data\": {\"name\": \"data-fa92dc5f0050e8b2ad7776e083e19b07\"}, \"facet\": {\"column\": {\"field\": \"model\", \"title\": null, \"type\": \"nominal\"}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"point\", \"color\": \"green\", \"filled\": true, \"shape\": \"diamond\", \"size\": 200}, \"encoding\": {\"x\": {\"field\": \"ground_truth\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"id_title\", \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true, \"size\": 50}, \"encoding\": {\"x\": {\"axis\": {\"labelExpr\": \"datum.value % 0.5 ? null : datum.label\"}, \"field\": \"mean\", \"scale\": {\"domainMin\": 0, \"domainMax\": 1}, \"title\": \"Score\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": false, \"labelLimit\": 1000, \"labelPadding\": 10}, \"field\": \"id_title\", \"title\": null, \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}, {\"mark\": {\"type\": \"errorbar\"}, \"encoding\": {\"x\": {\"field\": \"min\", \"title\": \"1 SD\", \"type\": \"quantitative\"}, \"x2\": {\"field\": \"max\"}, \"y\": {\"field\": \"id_title\", \"type\": \"nominal\"}}, \"transform\": [{\"calculate\": \"max(0, datum.mean-datum.std)\", \"as\": \"min\"}, {\"calculate\": \"min(1, datum.mean+datum.std)\", \"as\": \"max\"}]}]}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-fa92dc5f0050e8b2ad7776e083e19b07\": [{\"repo\": \"lightfm\", \"id\": 2.1, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Ensure Data File Loads as Expected\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"ground_truth\": 1.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 3.2, \"count\": 30.0, \"mean\": 0.5, \"std\": 0.0, \"title\": \"Data in the Expected Format\", \"id_title\": \"3.2. Data in the Expected Format\", \"ground_truth\": 1.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 3.5, \"count\": 30.0, \"mean\": 0.0, \"std\": 0.0, \"title\": \"Check for Duplicate Records in Data\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"ground_truth\": 0.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 4.2, \"count\": 30.0, \"mean\": 0.8166666666666667, \"std\": 0.2450662589267805, \"title\": \"Verify Data Split Proportion\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"ground_truth\": 1.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 5.3, \"count\": 30.0, \"mean\": 0.4833333333333333, \"std\": 0.0912870929175276, \"title\": \"Ensure Model Output Shape Aligns with Expectation\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"ground_truth\": 0.5, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 6.1, \"count\": 30.0, \"mean\": 0.9166666666666666, \"std\": 0.1895245108947258, \"title\": \"Verify Evaluation Metrics Implementation\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"ground_truth\": 1.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 6.2, \"count\": 30.0, \"mean\": 0.9833333333333332, \"std\": 0.0912870929175276, \"title\": \"Evaluate Model's Performance Against Thresholds\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"ground_truth\": 1.0, \"model\": \"gpt-3.5-turbo\"}, {\"repo\": \"lightfm\", \"id\": 2.1, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Ensure Data File Loads as Expected\", \"id_title\": \"2.1. Ensure Data File Loads as Expected\", \"ground_truth\": 1.0, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 3.2, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Data in the Expected Format\", \"id_title\": \"3.2. Data in the Expected Format\", \"ground_truth\": 1.0, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 3.5, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Check for Duplicate Records in Data\", \"id_title\": \"3.5. Check for Duplicate Records in Data\", \"ground_truth\": 0.0, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 4.2, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Verify Data Split Proportion\", \"id_title\": \"4.2. Verify Data Split Proportion\", \"ground_truth\": 1.0, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 5.3, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Ensure Model Output Shape Aligns with Expectation\", \"id_title\": \"5.3. Ensure Model Output Shape Aligns with Expectation\", \"ground_truth\": 0.5, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 6.1, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Verify Evaluation Metrics Implementation\", \"id_title\": \"6.1. Verify Evaluation Metrics Implementation\", \"ground_truth\": 1.0, \"model\": \"gpt-4o\"}, {\"repo\": \"lightfm\", \"id\": 6.2, \"count\": 30.0, \"mean\": 1.0, \"std\": 0.0, \"title\": \"Evaluate Model's Performance Against Thresholds\", \"id_title\": \"6.2. Evaluate Model's Performance Against Thresholds\", \"ground_truth\": 1.0, \"model\": \"gpt-4o\"}]}}, {\"mode\": \"vega-lite\"});\n</script>\n```\n:::\n:::\n\n\n> Comparison of satisfaction using `gpt-4o` versus `gpt-3.5-turbo` for each checklist item on lightfm\n\nFurther investigation into `gpt-4o` is required to determine its effectiveness in system performance.\n\n## Conclusion\n\n### Wrap Up\n\nThe development of FixML has been driven by the need of better quality assurance in ML systems and the current limitations of traditional testing methods on ML projects. FixML provides curated checklists and automated tools that enhance the evaluation and creation of test suites for ML projects. This in return, significantly reduces the time and effort required to assess the completeness of ML test suites, and thus promotes thorough and efficient assessment on ML projects.\n\n### Limitation & Future Improvement\n\nWhile FixML provides substantial benefits, there are limitations and areas to be addressed in future development:\n\n1.  **Specialized Checklist**\n\nThe default checklist is general and may not cover all requirements for different ML projects. Future development will focus on creating specialized checklists for tailored evaluations across various domains and project types. Collaboration with ML researchers is welcomed for creating specialized checklists based on specific use cases.\n\n2.  **Enhanced Test Evaluator**\n\nOur study reveals the accuracy and consistency issues on the evaluation results using OpenAI GPT-3.5-turbo model. Future improvements involves better prompt engineering techniques and support for multiple LLMs for enhanced performance and flexibility. User guidelines in prompt creation will be provided to facilitate collaboration with ML developers.\n\n3.  **Customized Test Specification**\n\nFuture developments will integrate project-specific information to produce customized test function skeletons. This may further encourage users to create comprehensive tests.\n\n4.  Workflow Optimization #FIXME: have to review whether to include as it seems lower priority.\n\nThe test evaluator and test specification generator are currently separate. Future improvements could embed a workflow engine that automatically takes actions based on LLM responses. This creates a more cohesive and efficient workflow, recues manual intervention, and improves overall system performance.\n\n5.  Performance Optimization #FIXME: have to review whether to include as it seems lower priority.\n\nAs FixML handles large codebases and complex evaluations, performance optimization is essential. Future developments will focus on improving the speed and accuracy of LLM responses, reducing analysis and report generation times, and ensuring scalability for handling larger and more complex projects.\n\nBy addressing these limitations and implementing future improvements, we aim for FixML to achieve better performance and contribute to the development of better ML systems, and ultimately enhance human life.\n\n## References\n\n",
    "supporting": [
      "final_report_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}