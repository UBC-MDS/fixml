{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b9be5c-c6c2-4fce-9bca-815f8772443a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78741377-167e-41d6-9542-c3593c0079ff",
   "metadata": {},
   "source": [
    "This Jupyter notebook is a tool to evaluate the consistency of ML Test Evaluation performed by ChatGPT based on the research paper (Alexander, R., Katz, L., Moore, C., & Schwartz, Z. (2023)). \\\n",
    "It serves the purpose of evaluating the application performance before and after checklist modification, and evaluating the application performance upon model setting changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427397a-321d-4ba8-ba63-512e18eea528",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb67c7b-0b04-4f5f-a42b-7b31dcd963bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../test_creation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12b6521a-4c59-4c34-ae5f-720706d2f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import TestEvaluator\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4a70a-f520-4a9e-83b0-92df5a10e82a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c1246-cb79-443c-a833-1784f4e25da3",
   "metadata": {},
   "source": [
    "Please specify the `test_functions_directory` below to load the ML test code base for the evaluation.\\\n",
    "The loaded test functions will be further split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74036e8d-1dee-459b-bdf7-fa797b262e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_functions_directory = '../../../lightfm/tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828448c6-8bd4-448d-8964-cda9e3481f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d2139ca-98d3-4253-a2e1-3ecdce1a1018",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03e63c-fb18-4361-b117-aa2355b7f5bb",
   "metadata": {},
   "source": [
    "Please specify the parameters, e.g. checklist, and the corresponding models to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4408d48-9590-444c-8725-52b06363fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210373f1-9354-434c-9103-5c4b767b14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperatures = [0.1]\n",
    "# models = ['gpt-3.5-turbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "457946ba-0723-45a1-9491-c40ede92992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_directory = '../../checklist/checklist_demo.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc01e5b0-f3d7-4230-b413-e94372d88634",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'checklist_demo_1'\n",
    "evaluator = TestEvaluator(test_functions_directory)\n",
    "evaluator.load_checklist(checklist_directory)\n",
    "models.append({'name': name, 'model': evaluator})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46898c0d-dd18-4812-a6ba-fced01aabcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'checklist_demo_2'\n",
    "evaluator = TestEvaluator(test_functions_directory)\n",
    "evaluator.load_checklist(checklist_directory)\n",
    "models.append({'name': name, 'model': evaluator})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f1a285c-f50e-423d-9d54-be0e15190244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'checklist_demo_1', 'model': <analyze.TestEvaluator at 0x15a9f2c90>},\n",
       " {'name': 'checklist_demo_2', 'model': <analyze.TestEvaluator at 0x15a9f2c60>}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fccbbab1-65de-495e-8071-38021e67cb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checklist_demo_1</td>\n",
       "      <td>&lt;analyze.TestEvaluator object at 0x15a9f2c90&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checklist_demo_2</td>\n",
       "      <td>&lt;analyze.TestEvaluator object at 0x15a9f2c60&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                          model\n",
       "0  checklist_demo_1  <analyze.TestEvaluator object at 0x15a9f2c90>\n",
       "1  checklist_demo_2  <analyze.TestEvaluator object at 0x15a9f2c60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cdd4a-6afe-4b39-8a65-46261ebaab16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9dee5f-fe3a-42ec-9daf-28ca7830d388",
   "metadata": {},
   "source": [
    "## API Running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ba74b-6ac9-4d6b-a11a-71f17a21614f",
   "metadata": {},
   "source": [
    "Incorporate the data, prompts and parameters, feed into OpenAI API for test runs and fetch responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffbc27df-db63-4e57-ab9d-3d6121327d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_runs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf4e29-eb96-4fac-b340-1c9dbe328477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_json(response, start='{', end='}'):\n",
    "#     start_idx = response.index(start)\n",
    "#     end_idx = response[::-1].index(end)\n",
    "#     if end_idx == 0:\n",
    "#         string = response[start_idx:]\n",
    "#     else:\n",
    "#         string = response[start_idx:-end_idx]\n",
    "#     return json.loads(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1414799-02dc-4ebf-9f18-45d7ae83c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:50<00:00,  8.46s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:43<00:00,  7.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:47<00:00,  7.92s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:49<00:00,  8.26s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for item in models:\n",
    "    for i in range(num_test_runs):\n",
    "        result = dict()\n",
    "        model = item['model']\n",
    "        model.evaluate()\n",
    "\n",
    "        result['score'] = model.get_completeness_score(score_format='number')\n",
    "        result['report'] = model.evaluation_report\n",
    "        result['model_name'] = item['name']\n",
    "        result['test_no'] = i+1\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e944a-802b-4ee1-beae-438d8178f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d66d396d-8665-48a1-bd57-299850165e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>report</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>checklist_demo_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>checklist_demo_1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>checklist_demo_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>checklist_demo_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                             report        model_name  \\\n",
       "0    1.0                                                ...  checklist_demo_1   \n",
       "1    1.0                                                ...  checklist_demo_1   \n",
       "2    1.0                                                ...  checklist_demo_2   \n",
       "3    1.0                                                ...  checklist_demo_2   \n",
       "\n",
       "   test_no  \n",
       "0        1  \n",
       "1        2  \n",
       "2        1  \n",
       "3        2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f020f-74e9-4650-a3cb-ad86abce1109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24ce51c6-0dbf-4fa7-8722-5bef38e0224a",
   "metadata": {},
   "source": [
    "## Result & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc750f68-4fa7-4264-a270-2f3cc0ea667c",
   "metadata": {},
   "source": [
    "The evaluation will be based on 2 metrics calculated from the response:\n",
    "- Completeness Score distribution: The distribution of the `num_test_runs` completeness scores per each set of parameters\n",
    "- Consistency Score: Out of all `checklist` items, the proportion of results remain consistent among `num_test_runs` runs per each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22ea8a4d-1f08-4f4d-9c80-4dc56ac16b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_score_df = results_df.drop(columns='report')\n",
    "completeness_score_df = completeness_score_df.pivot(index='model_name', columns='test_no', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69617811-0bde-48d2-8604-5ba692a65df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>test_no</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>checklist_demo_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checklist_demo_2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_no             1    2\n",
       "model_name                \n",
       "checklist_demo_1  1.0  1.0\n",
       "checklist_demo_2  1.0  1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd191f9e-6cb6-4964-b060-7976f1529edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# completeness_score_df.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "391b097d-df72-4490-a91d-7d0858852ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_df = pd.DataFrame()\n",
    "for i in results_df.index:\n",
    "    result = results_df.iloc[i]['report'].reset_index()\n",
    "    result['test_no'] = results_df.iloc[i]['test_no']\n",
    "    result['model_name'] = results_df.iloc[i]['model_name']\n",
    "    consistency_df = pd.concat([consistency_df, result], axis = 0, ignore_index=True)\n",
    "consistency_df = consistency_df.pivot(index=['model_name', 'ID'], columns=['test_no'], values=['is_Satisfied'])\n",
    "consistency_df.columns = consistency_df.columns.droplevel(level=0)\n",
    "consistency_df.columns.name = None\n",
    "consistency_df['consistency'] = consistency_df.eq(consistency_df.iloc[:, 0], axis=0).all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28c90383-e3c8-44a9-83a2-344495be26a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>consistency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">checklist_demo_1</th>\n",
       "      <th>1.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">checklist_demo_2</th>\n",
       "      <th>1.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        1    2  consistency\n",
       "model_name       ID                        \n",
       "checklist_demo_1 1.1  1.0  1.0         True\n",
       "                 1.2  1.0  1.0         True\n",
       "                 2.1  1.0  1.0         True\n",
       "                 5.1  1.0  1.0         True\n",
       "checklist_demo_2 1.1  1.0  1.0         True\n",
       "                 1.2  1.0  1.0         True\n",
       "                 2.1  1.0  1.0         True\n",
       "                 5.1  1.0  1.0         True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca45a592-7490-4f86-8357-0707ef81e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency_df.groupby(['model_name']).agg({'consistency': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c0b4b-ca68-4d44-9cb7-977e08abadb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
