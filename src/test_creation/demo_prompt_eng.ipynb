{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Prepare .env and API Key before running the script below\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb7fff1-47b5-4e3d-b204-07eb523b32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "312da605-da7d-4493-8acc-11301e5ef567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2079.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# load doc\n",
    "# loader = DirectoryLoader(\n",
    "#     # '../../../lightfm/tests', \n",
    "#     '../../../SVD_Compression/llm_test/test', \n",
    "#     glob=\"**/*.py\", \n",
    "#     show_progress=True, \n",
    "#     #use_multithreading=True,\n",
    "#     loader_cls=PythonLoader\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "# # retriever = vectorstore.as_retriever()\n",
    "# # context = retriever.invoke(\"How many test functions are there?\")\n",
    "\n",
    "# # define prompt and chat\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"messages\")\n",
    "# ])\n",
    "\n",
    "# # chat = ChatOpenAI(model='gpt-4-turbo')\n",
    "# chat = ChatOpenAI(model='gpt-4o', temperature=0.1)\n",
    "\n",
    "# # combine prompt, chat and doc\n",
    "# docs_chain = create_stuff_documents_chain(chat, prompt) \n",
    "\n",
    "# for chunk in docs_chain.stream({\n",
    "#     \"context\": all_splits,\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"How many test functions are there? Can you list them all?\")\n",
    "#     ],\n",
    "# }):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad8f2fc-5d20-48bb-a989-0c6d028196e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_sample = '''\n",
    "1. Does it contain test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization?\n",
    "2. Does it contain test case to verify that the output dimensions and values from model predictions match expected outcomes?\n",
    "3. Does it contain test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly?\n",
    "4. Does it contain test case to evaluate the model’s performance over training to identify potential overfitting? This could involve comparing training and validation loss.\n",
    "5. Does it contain test case to define and enforce performance thresholds for crucial metrics to guarantee model performance?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d6d3a4-8dab-4431-88a7-97ab9b150f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 1604.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# load doc\n",
    "loader = DirectoryLoader(\n",
    "    '../../../lightfm/tests', \n",
    "    # '../../../SVD_Compression/llm_test/test', \n",
    "    glob=\"**/*.py\", \n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# define prompt and chat\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior machine learning engineer who specializes in performing Machine Learning system testing. Extract and analyze the test functions from the codes:\\n\\n{context}\"),\n",
    "    (\"system\", f\"Here is the Machine Learning system testing checklist delimited by triple quotes '''{checklist_sample}'''\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "# chat = ChatOpenAI(model='gpt-4-turbo')\n",
    "chat = ChatOpenAI(model='gpt-4o', \n",
    "                  # temperature=0.1,\n",
    "                 )\n",
    "\n",
    "# combine prompt, chat and doc\n",
    "docs_chain = create_stuff_documents_chain(chat, prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe6c7d8-d86c-4bfd-a29a-2cb3ae6b7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist Evaluation:\n",
      "\n",
      "1. Requirement Title: Model Loading and Initialization\n",
      "   Requirement: Ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\n",
      "   Observation: The code includes tests for initializing the LightFM model and fitting it with various datasets.\n",
      "   Evaluation: Fulfilled\n",
      "\n",
      "2. Requirement Title: Output Dimensions and Values\n",
      "   Requirement: Verify that the output dimensions and values from model predictions match expected outcomes.\n",
      "   Observation: The code includes tests that validate predictions' values and shapes, ensuring they meet expected outcomes.\n",
      "   Evaluation: Fulfilled\n",
      "\n",
      "3. Requirement Title: Evaluation Metrics Accuracy\n",
      "   Requirement: Confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\n",
      "   Observation: The code includes tests for precision_at_k, recall_at_k, and auc_score to validate that these metrics are calculated correctly.\n",
      "   Evaluation: Fulfilled\n",
      "\n",
      "4. Requirement Title: Model Performance Over Training\n",
      "   Requirement: Evaluate the model’s performance over training to identify potential overfitting. This could involve comparing training and validation loss.\n",
      "   Observation: The code includes tests to compare performance metrics such as precision and AUC on both training and test datasets.\n",
      "   Evaluation: Fulfilled\n",
      "\n",
      "5. Requirement Title: Performance Thresholds\n",
      "   Requirement: Define and enforce performance thresholds for crucial metrics to guarantee model performance.\n",
      "   Observation: The code includes assertions to check that certain metrics exceed predefined thresholds, such as AUC over 0.84 and precision over 0.3.\n",
      "   Evaluation: Fulfilled\n",
      "\n",
      "Completion Score: 5/5"
     ]
    }
   ],
   "source": [
    "for chunk in docs_chain.stream({\n",
    "    \"context\": docs,\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"\"\"\n",
    "        Evaluate whether the codes has fulfilled the requirements and deliver a completion score. Do not include a summary evaluation.\n",
    "        Desired format:\n",
    "        Checklist Evaluation:\n",
    "            Requirement Title:\n",
    "            Requirement:\n",
    "            Observation:\n",
    "            Evaluation:\n",
    "        Completion Score:\n",
    "        \"\"\")\n",
    "    ],\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a9f2a-bb75-4e34-8d70-da09d793f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1551c-a568-49b7-a79e-be38935b1dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b155a3b-f109-4181-b5fe-5bc04714051c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0e418-229b-4018-974b-e1dbe4615d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
