{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca06b23-a3fb-466f-baf4-dc34a2720e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from modules.code_analyzer.repo import Repository\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.checklist.checklist import Checklist, ChecklistFormat\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589b42d3-4e71-4676-8892-4bf1346118ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEvaluator:\n",
    "    def __init__(self, repo_path=None):\n",
    "        self.repo = None\n",
    "        self.test_fps = [] # test file paths\n",
    "        self.test_dir_path = '' # test dir path # FIXME: required by `load_test_dir`\n",
    "        self.py_splits = []\n",
    "\n",
    "        # FIXME: Tony's \"Checklist - After Engineering\" version\n",
    "        self.checklist = \"\"\"\n",
    "            Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\n",
    "            Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\n",
    "            Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\n",
    "            Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\n",
    "            Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
    "            Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\n",
    "            Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\n",
    "            Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\n",
    "            Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\n",
    "            Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\n",
    "            Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\n",
    "            For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\n",
    "            Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\n",
    "            Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\n",
    "            If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\n",
    "            Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\n",
    "        \"\"\"\n",
    "        self.system_message = []\n",
    "        self.model = 'gpt-3.5-turbo'\n",
    "        self.temperature = 0\n",
    "        self.chain = None\n",
    "\n",
    "        # self.evaluation_message = \"\"\"\n",
    "        #     Your task is to answer each question in the checklist using only the provided test functions.\n",
    "        #     If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\n",
    "        #     Then, decide the completion score in a fraction format based on your answers. The denominator should be the number of checklist items.\n",
    "        #     Desired format:\n",
    "        #         Checklist Evaluation:\n",
    "        #             ID: \n",
    "        #             Title:\n",
    "        #             Requirement:\n",
    "        #             Observation:\n",
    "        #             Evaluation: Satisfied/Partially Satisfied/Not Satisfied\n",
    "        #             Score: (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\n",
    "        #         Completion Score: Number of satisfied requirements/Number of requirements\n",
    "        #             Number of satisfied requirements:\n",
    "        #             Number of partially satisfied requirements:\n",
    "        #             Number of not satisfied requirements:\n",
    "        # \"\"\"\n",
    "        self.evaluation_message = \"\"\"\n",
    "            Your task is to answer each question in the checklist using only the provided test functions.\n",
    "            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\n",
    "            Output a JSON format:\n",
    "                {\n",
    "                    \"ID\": \n",
    "                    \"Title\":\n",
    "                    \"Requirement\":\n",
    "                    \"Observation\":\n",
    "                    \"Functions\": [ ... ]\n",
    "                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\n",
    "                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "        self.evaluation_result = None\n",
    "\n",
    "        if repo_path is not None:\n",
    "            self.load_repo(repo_path)\n",
    "        \n",
    "    def load_repo(self, repo_path):\n",
    "        self.repo = Repository(repo_path)\n",
    "        self.test_fps = self.repo.list_test_files()['Python']\n",
    "\n",
    "    def load_test_file(self, file_path, overwrite=True):\n",
    "        loader = PythonLoader(file_path)\n",
    "        py = loader.load()\n",
    "        py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "        \n",
    "        if overwrite:\n",
    "            self.py_splits = py_splits\n",
    "        \n",
    "        return py_splits\n",
    "\n",
    "    # def load_all_test_files(self):\n",
    "    #     self.py_splits = []\n",
    "    #     for fp in self.test_fps:\n",
    "    #         self.py_splits += self.load_test_file(fp, overwrite=False)\n",
    "\n",
    "    def load_test_dir(self, dir_path):\n",
    "        self.test_dir_path = dir_path\n",
    "        \n",
    "        loader = DirectoryLoader(\n",
    "            dir_path,\n",
    "            glob=\"**/*.py\", \n",
    "            show_progress=True, \n",
    "            loader_cls=PythonLoader\n",
    "        )\n",
    "        docs = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        self.py_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    def load_checklist(self, checklist_path):\n",
    "        raw_checklist = Checklist(checklist_path, checklist_format=ChecklistFormat.YAML)\n",
    "        \n",
    "        checklist = []\n",
    "        for item in raw_checklist.get_all_tests():\n",
    "            checklist.append({\n",
    "                'ID': item['ID'],\n",
    "                'Title': item['Title'],\n",
    "                'Requirement': item['Requirement']\n",
    "            })\n",
    "            \n",
    "        self.checklist = json.dumps(checklist).replace('{', '[').replace('}', ']')\n",
    "\n",
    "    def init_system_message(self):\n",
    "        if len(self.checklist) == 0:\n",
    "            self.load_checklist()\n",
    "            \n",
    "        self.system_message = [\n",
    "            (\"system\", \"You are a senior machine learning engineer who specializes in performing Machine Learning system testing. Extract and analyze the test functions from the codes:\\n\\n{context}\"),\n",
    "            (\"system\", f\"Here is the Machine Learning system testing checklist delimited by triple quotes '''{self.checklist}'''\")\n",
    "        ]\n",
    "\n",
    "    def init_chain(self, system_message=None, model=None):\n",
    "        if system_message is None:\n",
    "            if len(self.system_message) == 0:\n",
    "                self.init_system_message()\n",
    "            system_message = self.system_message\n",
    "        else:\n",
    "            self.system_message = system_message\n",
    "\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            system_message + [\n",
    "                MessagesPlaceholder(variable_name=\"messages\")\n",
    "            ]\n",
    "        )\n",
    "        chat = ChatOpenAI(model=model, temperature=self.temperature)\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        self.chain = chain\n",
    "        return chain\n",
    "\n",
    "    def get_ai_response(self, message, context, history=None):\n",
    "        if self.chain is None:\n",
    "            self.init_chain()\n",
    "\n",
    "        if history is None:\n",
    "            history = ChatMessageHistory()\n",
    "\n",
    "        history.add_user_message(message)\n",
    "        \n",
    "        response = self.chain.invoke({\n",
    "            \"context\": context, \n",
    "            \"messages\": history.messages\n",
    "        })\n",
    "        history.add_ai_message(response)\n",
    "\n",
    "        return response, history\n",
    "\n",
    "    def get_evaluation_response(self, py_splits=None):\n",
    "        if py_splits is None:\n",
    "            py_splits = self.py_splits\n",
    "            \n",
    "        return self.get_ai_response(\n",
    "            message=self.evaluation_message, \n",
    "            context=py_splits\n",
    "        )\n",
    "\n",
    "    # FIXME: combine evaluation\n",
    "    # to be tested\n",
    "    def extract_json(self, response, start='[', end=']'):\n",
    "        start_idx = response.index(start)\n",
    "        end_idx = response[::-1].index(end)\n",
    "        if end_idx == 0:\n",
    "            string = response[start_idx:]\n",
    "        else:\n",
    "            string = response[start_idx:-end_idx]\n",
    "        return json.loads(string)\n",
    "\n",
    "    def evaluate(self, on_file=True):\n",
    "        result = []\n",
    "        if on_file:\n",
    "            for fp in tqdm(self.test_fps):\n",
    "                print(fp)\n",
    "                self.load_test_file(fp)\n",
    "                print(f\"# splits: {len(self.test_fps)}\")\n",
    "                response, history = self.get_evaluation_response() # FIXME: it sometimes tests only part of the checklist items\n",
    "                # print(response)\n",
    "                report = self.extract_json(response)\n",
    "                # print(report)\n",
    "                for item in report:\n",
    "                    item['file'] = fp\n",
    "                result += [{\n",
    "                    'file': fp,\n",
    "                    'report': report,\n",
    "                    'history': history\n",
    "                }]\n",
    "        else:\n",
    "            load_test_dir(self.test_dir_path)\n",
    "            response, history = self.get_evaluation_response()\n",
    "            report = self.extract_json(response)\n",
    "            for item in report:\n",
    "                item['file'] = self.test_dir_path\n",
    "            result += [{\n",
    "                'file': self.test_dir_path,\n",
    "                'report': report,\n",
    "                'history': history\n",
    "            }]\n",
    "\n",
    "        self.evaluation_result = result\n",
    "        return\n",
    "\n",
    "    def get_completeness_score(self):\n",
    "        report_df = pd.DataFrame(self.evaluation_result)['report'].explode('report').apply(pd.Series)\n",
    "        report_df = report_df[report_df['Title'] != 'Dummy Title']\n",
    "        report_df = report_df.groupby(['ID']).max('Score')\n",
    "        score = f'{report_df['Score'].sum()}/{report_df['Score'].count()}'\n",
    "        print(f'Score: {score}')\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c25bf0-91ac-4e08-853f-9cde9467e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"../../../lightfm/\"\n",
    "checklist_path = '../../checklist/checklist.yaml'\n",
    "test = TestEvaluator(repo_path)\n",
    "test.load_checklist(checklist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cd5180-aad2-4a3c-80f6-619e57772de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.load_test_dir('../../data/raw/openja/lightfm/tests/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7cd50c-16e0-4746-afa9-6f23da8346c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                  | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_fast_functions.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████                                                                           | 1/6 [00:08<00:43,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_movielens.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████                                                            | 2/6 [00:48<01:47, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_datasets.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████                                             | 3/6 [01:17<01:23, 27.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_cross_validation.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████                              | 4/6 [01:50<00:59, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_evaluation.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████████████████████████               | 5/6 [02:23<00:30, 30.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../lightfm/tests/test_data.py\n",
      "# splits: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:52<00:00, 28.81s/it]\n"
     ]
    }
   ],
   "source": [
    "test.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724b4e1f-70e7-4bfc-a6a1-a211ea5dd2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5.5/16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5.5/16'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_completeness_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaac96-b0fd-47f8-bb93-ffb49389e608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65e916d0-1cd2-47f9-a6f5-71e1e707f4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Score</th>\n",
       "      <th>Functions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <th>Write Descriptive Test Names</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[test_in_positives, test_movielens_accuracy, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <th>Keep Tests Focused</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[test_in_positives, test_movielens_accuracy, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <th>Prefer Narrow Assertions in Unit Tests</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[test_in_positives, test_movielens_accuracy, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <th>Keep Cause and Effect Clear</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[test_in_positives, test_movielens_accuracy, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <th>Ensure Data File Loads as Expected</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2</th>\n",
       "      <th>Ensure Saving Data/Figures Function Works as Expected</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.1</th>\n",
       "      <th>Files Contain Data</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.2</th>\n",
       "      <th>Data in the Expected Format</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[test_basic_fetching_movielens, test_basic_fet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.3</th>\n",
       "      <th>Data Does Not Contain Null Values or Outliers</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.1</th>\n",
       "      <th>Cleaning and Transformation Functions Work as Expected</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <th>Validate Model Input and Output Compatibility</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.2</th>\n",
       "      <th>Check Model is Learning During Fit</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.3</th>\n",
       "      <th>Ensure Model Output Shape Aligns with Expectation</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.4</th>\n",
       "      <th>Ensure Model Output Aligns with Task Trained</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.5</th>\n",
       "      <th>Validate Loss Reduction on Gradient Update</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.6</th>\n",
       "      <th>Check for Data Leakage</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.1</th>\n",
       "      <th>Dummy Title</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.1</th>\n",
       "      <th>Dummy Title</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Score        \\\n",
       "                                                         max count   \n",
       "ID  Title                                                            \n",
       "1.1 Write Descriptive Test Names                           1     6   \n",
       "1.2 Keep Tests Focused                                     1     6   \n",
       "1.3 Prefer Narrow Assertions in Unit Tests                 1     6   \n",
       "1.4 Keep Cause and Effect Clear                            1     6   \n",
       "2.1 Ensure Data File Loads as Expected                     0     5   \n",
       "2.2 Ensure Saving Data/Figures Function Works as Ex...     0     5   \n",
       "3.1 Files Contain Data                                     0     5   \n",
       "3.2 Data in the Expected Format                            1     5   \n",
       "3.3 Data Does Not Contain Null Values or Outliers          0     5   \n",
       "4.1 Cleaning and Transformation Functions Work as E...     0     5   \n",
       "5.1 Validate Model Input and Output Compatibility          0     5   \n",
       "5.2 Check Model is Learning During Fit                     0     5   \n",
       "5.3 Ensure Model Output Shape Aligns with Expectation      0     5   \n",
       "5.4 Ensure Model Output Aligns with Task Trained           0     5   \n",
       "5.5 Validate Loss Reduction on Gradient Update             0     5   \n",
       "5.6 Check for Data Leakage                                 0     5   \n",
       "6.1 Dummy Title                                            1     4   \n",
       "7.1 Dummy Title                                            1     4   \n",
       "\n",
       "                                                                                                Functions  \n",
       "                                                                                                      sum  \n",
       "ID  Title                                                                                                  \n",
       "1.1 Write Descriptive Test Names                        [test_in_positives, test_movielens_accuracy, t...  \n",
       "1.2 Keep Tests Focused                                  [test_in_positives, test_movielens_accuracy, t...  \n",
       "1.3 Prefer Narrow Assertions in Unit Tests              [test_in_positives, test_movielens_accuracy, t...  \n",
       "1.4 Keep Cause and Effect Clear                         [test_in_positives, test_movielens_accuracy, t...  \n",
       "2.1 Ensure Data File Loads as Expected                                                                 []  \n",
       "2.2 Ensure Saving Data/Figures Function Works as Ex...                                                 []  \n",
       "3.1 Files Contain Data                                                                                 []  \n",
       "3.2 Data in the Expected Format                         [test_basic_fetching_movielens, test_basic_fet...  \n",
       "3.3 Data Does Not Contain Null Values or Outliers                                                      []  \n",
       "4.1 Cleaning and Transformation Functions Work as E...                                                 []  \n",
       "5.1 Validate Model Input and Output Compatibility                                                      []  \n",
       "5.2 Check Model is Learning During Fit                                                                 []  \n",
       "5.3 Ensure Model Output Shape Aligns with Expectation                                                  []  \n",
       "5.4 Ensure Model Output Aligns with Task Trained                                                       []  \n",
       "5.5 Validate Loss Reduction on Gradient Update                                                         []  \n",
       "5.6 Check for Data Leakage                                                                             []  \n",
       "6.1 Dummy Title                                                                                        []  \n",
       "7.1 Dummy Title                                                                                        []  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports = []\n",
    "for res in test.evaluation_result:\n",
    "    reports += res['report']\n",
    "\n",
    "evaluationdf = pd.DataFrame(reports)\n",
    "report = df.groupby(['ID', 'Title']).agg({\n",
    "    'Score': ['max', 'count'],\n",
    "    'Functions': ['sum'],\n",
    "})\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd9934bb-89ab-4fc4-8331-528435893653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     m/tests/test_fast_functions.py\n",
       "4     ightfm/tests/test_movielens.py\n",
       "22    lightfm/tests/test_datasets.py\n",
       "40    tests/test_cross_validation.py\n",
       "56    ghtfm/tests/test_evaluation.py\n",
       "74    nja/lightfm/tests/test_data.py\n",
       "92    enja/lightfm/tests/test_api.py\n",
       "Name: file, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('ID == \"1.1\"')['file'].str[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95353a1a-58a0-4be9-8d4f-1d56b79dbb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     ightfm/tests/test_movielens.py\n",
       "36     lightfm/tests/test_datasets.py\n",
       "54     tests/test_cross_validation.py\n",
       "70     ghtfm/tests/test_evaluation.py\n",
       "88     nja/lightfm/tests/test_data.py\n",
       "106    enja/lightfm/tests/test_api.py\n",
       "Name: file, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('ID == \"5.5\"')['file'].str[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8663cc3-ddda-4347-a733-5645601e04e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '1.1',\n",
       "  'Title': 'Write Descriptive Test Names',\n",
       "  'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "  'Observation': \"The test function 'test_in_positives' has a descriptive name that indicates it is testing for positive cases related to the input data.\",\n",
       "  'Functions': ['test_in_positives'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1,\n",
       "  'file': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'},\n",
       " {'ID': '1.2',\n",
       "  'Title': 'Keep Tests Focused',\n",
       "  'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "  'Observation': \"The test function 'test_in_positives' focuses on testing the behavior of the '__test_in_positives' function with a specific matrix setup for positive cases.\",\n",
       "  'Functions': ['test_in_positives'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1,\n",
       "  'file': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'},\n",
       " {'ID': '1.3',\n",
       "  'Title': 'Prefer Narrow Assertions in Unit Tests',\n",
       "  'Requirement': 'Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.',\n",
       "  'Observation': \"The assertions in the 'test_in_positives' function are focused on specific behaviors related to the '__test_in_positives' function.\",\n",
       "  'Functions': ['test_in_positives'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1,\n",
       "  'file': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'},\n",
       " {'ID': '1.4',\n",
       "  'Title': 'Keep Cause and Effect Clear',\n",
       "  'Requirement': 'Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.',\n",
       "  'Observation': \"The modifications to the matrix object and the corresponding assertions are in close proximity within the 'test_in_positives' function.\",\n",
       "  'Functions': ['test_in_positives'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1,\n",
       "  'file': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.evaluation_result[0]['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f3c0a8b-493c-42d9-83f6-733f82266a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report[('Score', 'max')].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943f8aa-f8e6-4b80-ade0-f65fd2c0e6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507ce5ea-bf1c-4860-9ccb-32410e73ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load_test_file(test.test_fps[2])\n",
    "#test.load_all_test_files()\n",
    "len(test.py_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22918ee-b441-4157-a545-f53f6e357b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report, history = test.get_evaluation_response(test.py_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5af190-f383-4669-9311-fcf13b0f3fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.index>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb7da40-60b2-4ee1-ba43-846b716cadcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '1.1',\n",
       "  'Title': 'Write Descriptive Test Names',\n",
       "  'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "  'Observation': \"The test functions have descriptive names such as 'test_basic_fetching_movielens' and 'test_basic_fetching_stackexchange'.\",\n",
       "  'Functions': ['test_basic_fetching_movielens',\n",
       "   'test_basic_fetching_stackexchange'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1},\n",
       " {'ID': '1.2',\n",
       "  'Title': 'Keep Tests Focused',\n",
       "  'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "  'Observation': 'The test functions focus on specific scenarios related to fetching data from movielens and stackexchange datasets.',\n",
       "  'Functions': ['test_basic_fetching_movielens',\n",
       "   'test_basic_fetching_stackexchange'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1},\n",
       " {'ID': '1.3',\n",
       "  'Title': 'Prefer Narrow Assertions in Unit Tests',\n",
       "  'Requirement': 'Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.',\n",
       "  'Observation': 'The test functions contain narrow assertions focusing on specific properties of the fetched data.',\n",
       "  'Functions': ['test_basic_fetching_movielens',\n",
       "   'test_basic_fetching_stackexchange'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1},\n",
       " {'ID': '1.4',\n",
       "  'Title': 'Keep Cause and Effect Clear',\n",
       "  'Requirement': 'Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.',\n",
       "  'Observation': 'The modifications to objects and corresponding assertions are logically grouped together within the test functions.',\n",
       "  'Functions': ['test_basic_fetching_movielens',\n",
       "   'test_basic_fetching_stackexchange'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1},\n",
       " {'ID': '2.1',\n",
       "  'Title': 'Ensure Data File Loads as Expected',\n",
       "  'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "  'Observation': 'The test functions do not directly involve loading data files but focus on fetching data from specific datasets.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '2.2',\n",
       "  'Title': 'Ensure Saving Data/Figures Function Works as Expected',\n",
       "  'Requirement': 'Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.',\n",
       "  'Observation': 'The test functions do not involve saving data or figures.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '3.1',\n",
       "  'Title': 'Files Contain Data',\n",
       "  'Requirement': 'Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.',\n",
       "  'Observation': 'The test functions do not directly involve checking data files.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '3.2',\n",
       "  'Title': 'Data in the Expected Format',\n",
       "  'Requirement': 'Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.',\n",
       "  'Observation': 'The test functions ensure that the fetched data is in the expected format for further processing.',\n",
       "  'Functions': ['test_basic_fetching_movielens',\n",
       "   'test_basic_fetching_stackexchange'],\n",
       "  'Evaluation': 'Satisfied',\n",
       "  'Score': 1},\n",
       " {'ID': '3.3',\n",
       "  'Title': 'Data Does Not Contain Null Values or Outliers',\n",
       "  'Requirement': 'Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.',\n",
       "  'Observation': 'The test functions do not directly check for null values or outliers in the fetched data.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '4.1',\n",
       "  'Title': 'Cleaning and Transformation Functions Work as Expected',\n",
       "  'Requirement': 'Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.',\n",
       "  'Observation': 'The test functions focus on fetching data rather than cleaning and transformation functions.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.1',\n",
       "  'Title': 'Validate Model Input and Output Compatibility',\n",
       "  'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "  'Observation': 'The test functions do not involve testing model input and output compatibility.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.2',\n",
       "  'Title': 'Check Model is Learning During Fit',\n",
       "  'Requirement': \"For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\",\n",
       "  'Observation': 'The test functions do not involve training or fitting models.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.3',\n",
       "  'Title': 'Ensure Model Output Shape Aligns with Expectation',\n",
       "  'Requirement': \"Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\",\n",
       "  'Observation': 'The test functions do not involve testing model outputs.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.4',\n",
       "  'Title': 'Ensure Model Output Aligns with Task Trained',\n",
       "  'Requirement': \"Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\",\n",
       "  'Observation': 'The test functions do not involve verifying model output values.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.5',\n",
       "  'Title': 'Validate Loss Reduction on Gradient Update',\n",
       "  'Requirement': \"If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\",\n",
       "  'Observation': 'The test functions do not involve training models with gradient descent.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0},\n",
       " {'ID': '5.6',\n",
       "  'Title': 'Check for Data Leakage',\n",
       "  'Requirement': 'Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.',\n",
       "  'Observation': 'The test functions do not involve checking for data leakage.',\n",
       "  'Functions': [],\n",
       "  'Evaluation': 'Not Satisfied',\n",
       "  'Score': 0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.extract_json(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a582e2aa-6dac-4d59-894e-2fa8f926b7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist Evaluation:\n",
      "\n",
      "1. Requirement Title: Write Descriptive Test Names\n",
      "   Requirement: Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\n",
      "   Observation: The test functions are named `test_basic_fetching_movielens` and `test_basic_fetching_stackexchange`, which are reasonably descriptive.\n",
      "   Evaluation: Satisfied\n",
      "\n",
      "2. Requirement Title: Keep Tests Focused\n",
      "   Requirement: Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\n",
      "   Observation: The tests cover multiple scenarios within a single function, such as different configurations for fetching data and different datasets.\n",
      "   Evaluation: Not Satisfied\n",
      "\n",
      "3. Requirement Title: Prefer Narrow Assertions in Unit Tests\n",
      "   Requirement: Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\n",
      "   Observation: The tests include multiple assertions that cover various aspects of the data fetching process.\n",
      "   Evaluation: Partially Satisfied\n",
      "\n",
      "4. Requirement Title: Keep Cause and Effect Clear\n",
      "   Requirement: Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\n",
      "   Observation: The modifications and assertions are generally close together, but the tests are somewhat lengthy and cover multiple scenarios.\n",
      "   Evaluation: Partially Satisfied\n",
      "\n",
      "5. Requirement Title: Ensure Data File Loads as Expected\n",
      "   Requirement: Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
      "   Observation: The tests verify that data-loading functions return the expected results and handle different configurations.\n",
      "   Evaluation: Satisfied\n",
      "\n",
      "6. Requirement Title: Ensure Saving Data/Figures Function Works as Expected\n",
      "   Requirement: Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\n",
      "   Observation: There are no tests related to saving data or figures.\n",
      "   Evaluation: Not Satisfied\n",
      "\n",
      "7. Requirement Title: Files Contain Data\n",
      "   Requirement: Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\n",
      "   Observation: The tests check that the fetched data contains the expected number of non-zero entries.\n",
      "   Evaluation: Satisfied\n",
      "\n",
      "8. Requirement Title: Data in the Expected Format\n",
      "   Requirement: Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\n",
      "   Observation: The tests verify that the fetched data is in the expected sparse matrix format.\n",
      "   Evaluation: Satisfied\n",
      "\n",
      "9. Requirement Title: Data Does Not Contain Null Values or Outliers\n",
      "   Requirement: Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\n",
      "   Observation: There are no explicit checks for null values or outliers in the tests.\n",
      "   Evaluation: Not Satisfied\n",
      "\n",
      "10. Requirement Title: Cleaning and Transformation Functions Work as Expected\n",
      "    Requirement: Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\n",
      "    Observation: The tests do not cover cleaning or transformation functions.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "11. Requirement Title: Validate Model Input and Output Compatibility\n",
      "    Requirement: Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\n",
      "    Observation: The tests verify that the fetched data has the expected shapes and types.\n",
      "    Evaluation: Satisfied\n",
      "\n",
      "12. Requirement Title: Check Model is Learning During Fit\n",
      "    Requirement: For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\n",
      "    Observation: The tests do not cover model training or fitting.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "13. Requirement Title: Ensure Model Output Shape Aligns with Expectation\n",
      "    Requirement: Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\n",
      "    Observation: The tests do not cover model output shapes.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "14. Requirement Title: Ensure Model Output Aligns with Task Trained\n",
      "    Requirement: Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\n",
      "    Observation: The tests do not cover model output values.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "15. Requirement Title: Validate Loss Reduction on Gradient Update\n",
      "    Requirement: If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\n",
      "    Observation: The tests do not cover gradient updates or loss reduction.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "16. Requirement Title: Check for Data Leakage\n",
      "    Requirement: Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\n",
      "    Observation: The tests do not explicitly check for data leakage.\n",
      "    Evaluation: Not Satisfied\n",
      "\n",
      "Completion Score: 5/15\n",
      "    Number of satisfied requirements: 5\n",
      "    Number of partially satisfied requirements: 2\n",
      "    Number of not satisfied requirements: 8\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce089084-6e0f-4cff-956c-3177f7fa80c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[[id: 0, Title: Write Descriptive Test Names, Requirement: Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\\\\n], [id: 1, Title: Keep Tests Focused, Requirement: Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\\\\n], [id: 2, Title: Prefer Narrow Assertions in Unit Tests, Requirement: Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\\\\n], [id: 3, Title: Keep Cause and Effect Clear, Requirement: Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\\\\n], [id: 4, Title: Ensure Data File Loads as Expected, Requirement: Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\\\\n], [id: 5, Title: Ensure Saving Data/Figures Function Works as Expected, Requirement: Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\\\\n], [id: 6, Title: Files Contain Data, Requirement: Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\\\\n], [id: 7, Title: Data in the Expected Format, Requirement: Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\\\\n], [id: 8, Title: Data Does Not Contain Null Values or Outliers, Requirement: Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\\\\n], [id: 9, Title: Cleaning and Transformation Functions Work as Expected, Requirement: Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\\\\n], [id: 10, Title: Validate Model Input and Output Compatibility, Requirement: Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\\\\n], [id: 11, Title: Check Model is Learning During Fit, Requirement: For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\\\\n], [id: 12, Title: Ensure Model Output Shape Aligns with Expectation, Requirement: Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\\\\n], [id: 13, Title: Ensure Model Output Aligns with Task Trained, Requirement: Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\\\\n], [id: 14, Title: Validate Loss Reduction on Gradient Update, Requirement: If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\\\\n], [id: 15, Title: Check for Data Leakage, Requirement: Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\\\\n], [id: 16, Title: Dummy Title, Requirement: This is a dummy item and there is nothing needed to act on.\\\\n], [id: 17, Title: Dummy Title, Requirement: This is a dummy item and there is nothing needed to act on.\\\\n]]\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d073aef5-efca-4b26-9ee5-ece5af8093e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[\\\\{id: 0, Title: Write Descriptive Test Names, Requirement: Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\\\\n\\\\}, \\\\{id: 1, Title: Keep Tests Focused, Requirement: Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\\\\n\\\\}, \\\\{id: 2, Title: Prefer Narrow Assertions in Unit Tests, Requirement: Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\\\\n\\\\}, \\\\{id: 3, Title: Keep Cause and Effect Clear, Requirement: Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\\\\n\\\\}, \\\\{id: 4, Title: Ensure Data File Loads as Expected, Requirement: Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\\\\n\\\\}, \\\\{id: 5, Title: Ensure Saving Data/Figures Function Works as Expected, Requirement: Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\\\\n\\\\}, \\\\{id: 6, Title: Files Contain Data, Requirement: Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\\\\n\\\\}, \\\\{id: 7, Title: Data in the Expected Format, Requirement: Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\\\\n\\\\}, \\\\{id: 8, Title: Data Does Not Contain Null Values or Outliers, Requirement: Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\\\\n\\\\}, \\\\{id: 9, Title: Cleaning and Transformation Functions Work as Expected, Requirement: Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\\\\n\\\\}, \\\\{id: 10, Title: Validate Model Input and Output Compatibility, Requirement: Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\\\\n\\\\}, \\\\{id: 11, Title: Check Model is Learning During Fit, Requirement: For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\\\\n\\\\}, \\\\{id: 12, Title: Ensure Model Output Shape Aligns with Expectation, Requirement: Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\\\\n\\\\}, \\\\{id: 13, Title: Ensure Model Output Aligns with Task Trained, Requirement: Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\\\\n\\\\}, \\\\{id: 14, Title: Validate Loss Reduction on Gradient Update, Requirement: If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\\\\n\\\\}, \\\\{id: 15, Title: Check for Data Leakage, Requirement: Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\\\\n\\\\}, \\\\{id: 16, Title: Dummy Title, Requirement: This is a dummy item and there is nothing needed to act on.\\\\n\\\\}, \\\\{id: 17, Title: Dummy Title, Requirement: This is a dummy item and there is nothing needed to act on.\\\\n\\\\}]\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eba191-b968-4319-a518-68bb838cf59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
