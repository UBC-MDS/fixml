{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7542dd-42ec-4836-a907-6a1fd80044ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 5318.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three test functions in the provided code. They are:\n",
      "\n",
      "1. test_basic_fetching_stackexchange\n",
      "2. test_bpr_precision\n",
      "3. test_bpr_precision_multithreaded"
     ]
    }
   ],
   "source": [
    "# load doc\n",
    "loader = DirectoryLoader(\n",
    "    '../../data/raw/openja/lightfm/tests', \n",
    "    glob=\"**/*.py\", \n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "docs = retriever.invoke(\"How many test functions are there?\")\n",
    "\n",
    "# define prompt and chat\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "# combine prompt, chat and doc\n",
    "docs_chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "for chunk in docs_chain.stream({\n",
    "    \"context\": docs,\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"How many test functions are there? Can you list them all?\")\n",
    "    ],\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a9f2a-bb75-4e34-8d70-da09d793f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8545fe-8640-4f94-8a07-7ee17ebe45b2",
   "metadata": {},
   "source": [
    "### wrapped into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326dfa99-6c3d-4cb7-bead-d194384572ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b155a3b-f109-4181-b5fe-5bc04714051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(path):\n",
    "    loader = PythonLoader(path)\n",
    "    py = loader.load()\n",
    "    py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "    return py_splits\n",
    "\n",
    "def get_ai_response(message, py_splits, history=None, chain=None):\n",
    "    if chain is None:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "        chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        \n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    history.add_user_message(message)\n",
    "    resp = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(resp)\n",
    "\n",
    "    return resp, history, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d0e418-229b-4018-974b-e1dbe4615d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 functions defined in the code. They are as follows:\n",
      "\n",
      "1. _generate_data\n",
      "2. _precision_at_k\n",
      "3. _recall_at_k\n",
      "4. _auc\n",
      "5. test_precision_at_k\n",
      "6. test_precision_at_k_with_ties\n",
      "7. test_recall_at_k\n",
      "8. test_auc_score\n",
      "9. test_intersections_check\n",
      "10. model.predict\n",
      "11. evaluation.precision_at_k\n",
      "12. evaluation.recall_at_k\n"
     ]
    }
   ],
   "source": [
    "py_splits = load_test_file('../../data/raw/openja/lightfm/tests/test_evaluation.py')\n",
    "\n",
    "resp, history, chain = get_ai_response(\n",
    "    message=\"How many functions are defined in the code? list them all\",\n",
    "    py_splits=py_splits\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae6364d-c0cc-4859-9ff0-47201e507c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. `_generate_data`: This function generates a dataset where every user has interactions in both the training and the test set. It takes number of users, number of items, density, and test fraction as the parameters.\n",
      "\n",
      "2. `_precision_at_k`: This function calculates precision at 'k' using a given model, ground_truth, 'k' value, and optionally train data, user features and item features. Precision at 'k' is the proportion of recommended items in the top-k set that are relevant.\n",
      "\n",
      "3. `_recall_at_k`: This function calculates recall at 'k' using a given model, ground_truth, 'k' value, and optionally train data, user features and item features. Recall at 'k' is the proportion of relevant items found in the top-k recommendations.\n",
      "\n",
      "4. `_auc`: This function calculates Area Under the ROC Curve (AUC) using a given model, ground_truth, and optionally train data, user features and item features.\n",
      "\n",
      "5. `test_precision_at_k`: This function tests the 'precision_at_k' function by generating data, fitting a model, and comparing the calculated precision to the expected precision.\n",
      "\n",
      "6. `test_precision_at_k_with_ties`: This function tests a special case where all predictions are zero, resulting in a precision of zero.\n",
      "\n",
      "7. `test_recall_at_k`: This function tests the 'recall_at_k' function by generating data, fitting a model, and comparing the calculated recall to the expected recall.\n",
      "\n",
      "8. `test_auc_score`: This function tests the 'auc_score' function by generating data, fitting a model, and comparing the calculated AUC score to the expected AUC score.\n",
      "\n",
      "9. `test_intersections_check`: This function checks if an error is raised when the training and test sets have interactions in common. It also checks if no error is raised when there are no interactions in common or if the 'check_intersections' flag is set to False.\n",
      "\n",
      "10. `model.predict`: This is a method of the 'model' object. It predicts the scores for user-item pairs.\n",
      "\n",
      "11. `evaluation.precision_at_k`: This is a method from the 'evaluation' module. It measures the precision at 'k' for a model.\n",
      "\n",
      "12. `evaluation.recall_at_k`: This is a method from the 'evaluation' module. It measures the recall at 'k' for a model.\n"
     ]
    }
   ],
   "source": [
    "resp, history, _ = get_ai_response(\n",
    "    message=\"What is each of the functions doing?\",\n",
    "    py_splits=py_splits,\n",
    "    history=history,\n",
    "    chain=chain\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db4b636-82c9-4ea5-863e-29f81d7026c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following functions are related to Machine Learning (ML) pipeline test cases:\n",
      "\n",
      "1. `test_precision_at_k`: This function tests the precision at 'k' function, which is a common evaluation metric in recommendation systems.\n",
      "\n",
      "2. `test_precision_at_k_with_ties`: This function tests the precision at 'k' function in a special scenario where all predictions are zero, ensuring the metric handles this edge case correctly.\n",
      "\n",
      "3. `test_recall_at_k`: This function tests the recall at 'k' function, another common evaluation metric in recommendation systems.\n",
      "\n",
      "4. `test_auc_score`: This function tests the Area Under the ROC Curve (AUC) function, a common evaluation metric for binary classification problems.\n",
      "\n",
      "5. `test_intersections_check`: This function tests if the evaluation functions correctly handle situations where the training and test sets have common interactions. This is important for ensuring the evaluation metrics are being calculated correctly and the model isn't simply memorizing the training data.\n"
     ]
    }
   ],
   "source": [
    "resp, history, _ = get_ai_response(\n",
    "    message=\"Which of them are related to ML pipeline test cases?\",\n",
    "    py_splits=py_splits,\n",
    "    history=history,\n",
    "    chain=chain\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273db18c-13c4-4c86-a4c8-f42e0b0e37c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
