{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0a59a9-185c-4f17-a0dd-fa2534958ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "checklist = Checklist(\"../../checklist/checklist.csv\", checklist_format=ChecklistFormat.CSV)\n",
    "repo = Repository(\"../../data/raw/openja/lightfm_demo/\")\n",
    "prompt_format = EvaluationPromptFormat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff5602f-260f-48f3-9b38-a5109556ac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.60s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator = PerFileTestEvaluator(llm, prompt_format=prompt_format, repository=repo, checklist=checklist)\n",
    "response = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218a50de-7496-40e9-b7c1-fc40410b9408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "                                                                                             Requirement  \\\n",
      "ID  Title                                                                                                  \n",
      "2.1 Ensure Data File Loads as Expected                 Ensure that data-loading functions correctly l...   \n",
      "3.2 Data in the Expected Format                        Verify that the data to be ingested matches th...   \n",
      "3.5 Check for Duplicate Records in Data                Check for duplicate records in the dataset and...   \n",
      "4.2 Verify Data Split Proportion                       Check that the data is split into training and...   \n",
      "5.3 Ensure Model Output Shape Aligns with Expectation  Ensure the shape of the model's output aligns ...   \n",
      "6.1 Verify Evaluation Metrics Implementation           Verify that the evaluation metrics are correct...   \n",
      "6.2 Evaluate Model's Performance Against Thresholds    Compute evaluation metrics for both the traini...   \n",
      "\n",
      "                                                       is_Satisfied  \\\n",
      "ID  Title                                                             \n",
      "2.1 Ensure Data File Loads as Expected                          0.0   \n",
      "3.2 Data in the Expected Format                                 0.0   \n",
      "3.5 Check for Duplicate Records in Data                         0.0   \n",
      "4.2 Verify Data Split Proportion                                0.5   \n",
      "5.3 Ensure Model Output Shape Aligns with Expectation           0.0   \n",
      "6.1 Verify Evaluation Metrics Implementation                    0.5   \n",
      "6.2 Evaluate Model's Performance Against Thresholds             0.5   \n",
      "\n",
      "                                                       n_files_tested  \\\n",
      "ID  Title                                                               \n",
      "2.1 Ensure Data File Loads as Expected                              3   \n",
      "3.2 Data in the Expected Format                                     3   \n",
      "3.5 Check for Duplicate Records in Data                             3   \n",
      "4.2 Verify Data Split Proportion                                    3   \n",
      "5.3 Ensure Model Output Shape Aligns with Expectation               3   \n",
      "6.1 Verify Evaluation Metrics Implementation                        3   \n",
      "6.2 Evaluate Model's Performance Against Thresholds                 3   \n",
      "\n",
      "                                                                                            Observations  \\\n",
      "ID  Title                                                                                                  \n",
      "2.1 Ensure Data File Loads as Expected                 [(test_cross_validation.py) The code does not ...   \n",
      "3.2 Data in the Expected Format                        [(test_cross_validation.py) The code does not ...   \n",
      "3.5 Check for Duplicate Records in Data                [(test_cross_validation.py) The code does not ...   \n",
      "4.2 Verify Data Split Proportion                       [(test_cross_validation.py) The code tests the...   \n",
      "5.3 Ensure Model Output Shape Aligns with Expectation  [(test_cross_validation.py) The code does not ...   \n",
      "6.1 Verify Evaluation Metrics Implementation           [(test_cross_validation.py) The code does not ...   \n",
      "6.2 Evaluate Model's Performance Against Thresholds    [(test_cross_validation.py) The code does not ...   \n",
      "\n",
      "                                                                                     Function References  \n",
      "ID  Title                                                                                                 \n",
      "2.1 Ensure Data File Loads as Expected                 [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "3.2 Data in the Expected Format                        [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "3.5 Check for Duplicate Records in Data                [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "4.2 Verify Data Split Proportion                       [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "5.3 Ensure Model Output Shape Aligns with Expectation  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "6.1 Verify Evaluation Metrics Implementation           [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "6.2 Evaluate Model's Performance Against Thresholds    [{'File Path': '../../data/raw/openja/lightfm_...  \n",
      "\n",
      "Score: 1.5/7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5/7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ResponseParser(response)\n",
    "parser.get_completeness_score(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d717ba5d-dc9d-477d-a9db-ccb993f48f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnshiu/Desktop/ubc_mds/capstone/test-creation/src/test_creation/report.html /Users/johnshiu/Desktop/ubc_mds/capstone/test-creation/src/test_creation\n"
     ]
    }
   ],
   "source": [
    "parser.export_evaluation_report(\"report.html\", \"html\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273d124-cf0b-43db-891b-8f7cb0470a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbdae9cf-3520-4fc0-85ff-853ceb16b6d3",
   "metadata": {},
   "source": [
    "### Consistency evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273db18c-13c4-4c86-a4c8-f42e0b0e37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from consistency_check import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a682a42-8807-48c6-9de4-0558838e3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = Checklist(\"../../checklist/checklist.csv\", checklist_format=ChecklistFormat.CSV) # FIXME: use newer checklist\n",
    "repo = Repository(\"../../data/raw/openja/lightfm_demo/\")\n",
    "prompt_format = EvaluationPromptFormat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889fd144-c4c1-4365-81f5-317f3cf6c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-3.5-turbo\n",
      "Test Run No.: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:40<00:00, 13.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Run No.: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:34<00:00, 11.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o\n",
      "Test Run No.: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:53<00:00, 17.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Run No.: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:51<00:00, 17.17s/it]\n"
     ]
    }
   ],
   "source": [
    "gpt35 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "gpt4o = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "evaluator35 = PerFileTestEvaluator(gpt35, prompt_format=prompt_format, repository=repo, checklist=checklist)\n",
    "evaluator4o = PerFileTestEvaluator(gpt4o, prompt_format=prompt_format, repository=repo, checklist=checklist)\n",
    "\n",
    "consist_eval = ConsistencyEvaluator()\n",
    "consist_eval.evaluate(\n",
    "    models=[\n",
    "        {'name': 'gpt-3.5-turbo', 'model': evaluator35},\n",
    "        {'name': 'gpt-4o', 'model': evaluator4o}\n",
    "    ],\n",
    "    num_test_runs=2,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07875448-9c58-4ec0-94b8-de9be8870011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>report</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>ID                                        ...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>ID                                        ...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>ID                                        ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>ID                                        ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                             report     model_name  \\\n",
       "0  0.285714      ID                                        ...  gpt-3.5-turbo   \n",
       "1  0.142857      ID                                        ...  gpt-3.5-turbo   \n",
       "2  0.714286      ID                                        ...         gpt-4o   \n",
       "3  0.714286      ID                                        ...         gpt-4o   \n",
       "\n",
       "   test_no  \n",
       "0        1  \n",
       "1        2  \n",
       "2        1  \n",
       "3        2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consist_eval.evaluation_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2724c3-1dd9-4ccb-be9f-c2d87715cecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>is_Satisfied</th>\n",
       "      <th>n_files_tested</th>\n",
       "      <th>Observations</th>\n",
       "      <th>Function References</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>Ensure that data-loading functions correctly l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2</td>\n",
       "      <td>Data in the Expected Format</td>\n",
       "      <td>Verify that the data to be ingested matches th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>Check for Duplicate Records in Data</td>\n",
       "      <td>Check for duplicate records in the dataset and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>Verify Data Split Proportion</td>\n",
       "      <td>Check that the data is split into training and...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code includes ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>Ensure Model Output Shape Aligns with Expectation</td>\n",
       "      <td>Ensure the shape of the model's output aligns ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.1</td>\n",
       "      <td>Verify Evaluation Metrics Implementation</td>\n",
       "      <td>Verify that the evaluation metrics are correct...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>Evaluate Model's Performance Against Thresholds</td>\n",
       "      <td>Compute evaluation metrics for both the traini...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>[(test_cross_validation.py) The code does not ...</td>\n",
       "      <td>[{'File Path': '../../data/raw/openja/lightfm_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              Title  \\\n",
       "0  2.1                 Ensure Data File Loads as Expected   \n",
       "1  3.2                        Data in the Expected Format   \n",
       "2  3.5                Check for Duplicate Records in Data   \n",
       "3  4.2                       Verify Data Split Proportion   \n",
       "4  5.3  Ensure Model Output Shape Aligns with Expectation   \n",
       "5  6.1           Verify Evaluation Metrics Implementation   \n",
       "6  6.2    Evaluate Model's Performance Against Thresholds   \n",
       "\n",
       "                                         Requirement  is_Satisfied  \\\n",
       "0  Ensure that data-loading functions correctly l...           0.0   \n",
       "1  Verify that the data to be ingested matches th...           0.0   \n",
       "2  Check for duplicate records in the dataset and...           0.0   \n",
       "3  Check that the data is split into training and...           0.5   \n",
       "4  Ensure the shape of the model's output aligns ...           0.0   \n",
       "5  Verify that the evaluation metrics are correct...           1.0   \n",
       "6  Compute evaluation metrics for both the traini...           0.5   \n",
       "\n",
       "   n_files_tested                                       Observations  \\\n",
       "0               3  [(test_cross_validation.py) The code does not ...   \n",
       "1               3  [(test_cross_validation.py) The code does not ...   \n",
       "2               3  [(test_cross_validation.py) The code does not ...   \n",
       "3               3  [(test_cross_validation.py) The code includes ...   \n",
       "4               3  [(test_cross_validation.py) The code does not ...   \n",
       "5               3  [(test_cross_validation.py) The code does not ...   \n",
       "6               3  [(test_cross_validation.py) The code does not ...   \n",
       "\n",
       "                                 Function References  \n",
       "0  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "1  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "2  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "3  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "4  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "5  [{'File Path': '../../data/raw/openja/lightfm_...  \n",
       "6  [{'File Path': '../../data/raw/openja/lightfm_...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consist_eval.evaluation_reports['report'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa8141a-666a-45d2-9b64-afa9e0d14d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>0.010204</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score      \n",
       "                    var count\n",
       "model_name                   \n",
       "gpt-3.5-turbo  0.010204     2\n",
       "gpt-4o         0.000000     2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_var = consist_eval.evaluation_reports.groupby('model_name').agg({\n",
    "    'score': ['var', 'count']\n",
    "})\n",
    "score_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b1f94c8-1883-4435-84c7-b0687a6e6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/r3dvzdx10pxf47gvdqf81r9h0000gn/T/ipykernel_24102/1426530661.py:5: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  f_score = score_var[('score', 'var')]['gpt-3.5-turbo'] / score_var[('score', 'var')]['gpt-4o'] # var(prev) / var(curr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "\n",
      "2-tail test:\n",
      "  Successfully reject the null hypothesis: Var(Completeness_Score(Current Version)) == Var(Completeness_Score(Last Week Version))\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "TAIL = 2\n",
    "ALPHA = 0.05 #Or whatever you want your alpha to be.\n",
    "\n",
    "f_score = score_var[('score', 'var')]['gpt-3.5-turbo'] / score_var[('score', 'var')]['gpt-4o'] # var(prev) / var(curr)\n",
    "p_value = 1 - scipy.stats.f.cdf(\n",
    "    f_score, \n",
    "    score_var[('score', 'count')]['gpt-3.5-turbo'] - 1, # degree of freedom (prev)\n",
    "    score_var[('score', 'count')]['gpt-4o'] - 1         # degree of freedom (curr)\n",
    ") \n",
    "\n",
    "print(f\"p-value: {p_value}\")\n",
    "print()\n",
    "\n",
    "print(f\"{TAIL}-tail test:\")\n",
    "if p_value < ALPHA / 2 or p_value > 1 - ALPHA/2:\n",
    "    print(\"  Successfully reject the null hypothesis: Var(Completeness_Score(Current Version)) == Var(Completeness_Score(Last Week Version))\")\n",
    "else:\n",
    "    print(\"  Failed to reject the null hypothesis: Var(Completeness_Score(Current Version)) == Var(Completeness_Score(Last Week Version))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ba66d-7837-4b4d-9e50-f9ac7f17b023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
