{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7542dd-42ec-4836-a907-6a1fd80044ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 5318.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three test functions in the provided code. They are:\n",
      "\n",
      "1. test_basic_fetching_stackexchange\n",
      "2. test_bpr_precision\n",
      "3. test_bpr_precision_multithreaded"
     ]
    }
   ],
   "source": [
    "# load doc\n",
    "loader = DirectoryLoader(\n",
    "    '../../data/raw/openja/lightfm/tests', \n",
    "    glob=\"**/*.py\", \n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "docs = retriever.invoke(\"How many test functions are there?\")\n",
    "\n",
    "# define prompt and chat\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "# combine prompt, chat and doc\n",
    "docs_chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "for chunk in docs_chain.stream({\n",
    "    \"context\": docs,\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"How many test functions are there? Can you list them all?\")\n",
    "    ],\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a9f2a-bb75-4e34-8d70-da09d793f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8545fe-8640-4f94-8a07-7ee17ebe45b2",
   "metadata": {},
   "source": [
    "### wrapped into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326dfa99-6c3d-4cb7-bead-d194384572ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b155a3b-f109-4181-b5fe-5bc04714051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKLIST_SAMPLE = '''\n",
    "1. Does it contain test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization?\n",
    "2. Does it contain test case to verify that the output dimensions and values from model predictions match expected outcomes?\n",
    "3. Does it contain test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly?\n",
    "4. Does it contain test case to evaluate the model’s performance over training to identify potential overfitting? This could involve comparing training and validation loss.\n",
    "5. Does it contain test case to define and enforce performance thresholds for crucial metrics to guarantee model performance?\n",
    "'''\n",
    "\n",
    "def load_test_file(path):\n",
    "    loader = PythonLoader(path)\n",
    "    py = loader.load()\n",
    "    py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "    return py_splits\n",
    "\n",
    "def get_ai_response(message, py_splits, history=None, chain=None):\n",
    "    if chain is None:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "        chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        \n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    history.add_user_message(message)\n",
    "    resp = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(resp)\n",
    "\n",
    "    return resp, history, chain\n",
    "\n",
    "def get_ai_responses(messages, py_splits, verbose=True):\n",
    "    for i, msg in enumerate(messages):\n",
    "        if verbose:\n",
    "            print(f\"Q: {msg}\")\n",
    "        \n",
    "        if i == 0:\n",
    "            resp, history, chain = get_ai_response(\n",
    "                message=messages[i],\n",
    "                py_splits=py_splits\n",
    "            )\n",
    "        else:\n",
    "            resp, history, _ = get_ai_response(\n",
    "                message=messages[i],\n",
    "                py_splits=py_splits,\n",
    "                history=history,\n",
    "                chain=chain\n",
    "            )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Response: {resp}\")\n",
    "            print()\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_ml_tests(py_splits, history, checklist):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a senior machine learning engineer who specializes in performing Machine Learning system testing. Extract and analyze the test functions from the codes:\\n\\n{context}\"),\n",
    "        (\"system\", f\"Here is the Machine Learning system testing checklist delimited by triple quotes '''{checklist}'''\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    chat = ChatOpenAI(model='gpt-4')\n",
    "    chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "    history.add_user_message(\"\"\"\n",
    "        Evaluate whether the codes has fulfilled the requirements and deliver a completion score. Do not include a summary evaluation.\n",
    "        Desired JSON format:\n",
    "            {\n",
    "                \"Requirement Title\":\n",
    "                \"Requirement\":\n",
    "                \"Observation\":\n",
    "                \"Related Functions\": [ ... ]\n",
    "                \"Evaluation\": (1 for Fulfilled / 0.5 for Partially fulfilled / 0 for Not fulfilled)\n",
    "            }\n",
    "    \"\"\")\n",
    "    \n",
    "    # for chunk in chain.stream({\n",
    "    #     \"context\": py_splits,\n",
    "    #     \"messages\": history.messages\n",
    "    # }):\n",
    "    #     print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    report = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(report)\n",
    "\n",
    "    return report, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d0e418-229b-4018-974b-e1dbe4615d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_splits = load_test_file('../../data/raw/openja/lightfm/tests/test_evaluation.py')\n",
    "\n",
    "history = get_ai_responses(\n",
    "    py_splits=py_splits,\n",
    "    messages=[\n",
    "        \"How many functions are defined in the code? list them all\",\n",
    "        \"What is each of the functions doing?\",\n",
    "        \"Which of them are related to ML pipeline test cases?\"\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "report, history = evaluate_ml_tests(py_splits, history, CHECKLIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6364d-c0cc-4859-9ff0-47201e507c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Loading and Initialization\",\n",
      "        \"Requirement\": \"Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The code doesn't seem to have specific test cases for model loading and initialization.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Output Validation\",\n",
      "        \"Requirement\": \"Test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The code includes tests checking the outputs of the model predictions and comparing them with expected values.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Evaluation Metrics Correctness\",\n",
      "        \"Requirement\": \"Test case to confirm the accuracy and correctness of evaluation metrics used within the system.\",\n",
      "        \"Observation\": \"The code features tests for the correctness of evaluation metrics such as precision, recall, and AUC.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Overfitting Check\",\n",
      "        \"Requirement\": \"Test case to evaluate the model’s performance over training to identify potential overfitting.\",\n",
      "        \"Observation\": \"The code does not seem to include specific tests for checking overfitting during the model's training process.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds\",\n",
      "        \"Requirement\": \"Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The code tests the model's performance by comparing the output metrics with expected values but does not appear to set specific performance thresholds.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 0.5\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74d2bc-7f11-4c7e-a039-404ae5c6609f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57aa2ee8-f61f-4af2-a46d-e2f35ceab1c6",
   "metadata": {},
   "source": [
    "### run through test files in a repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db4b636-82c9-4ea5-863e-29f81d7026c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ../../data/raw/openja/lightfm/tests/test_datasets.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████████████████▌                                                                                                                                                                  | 1/5 [00:59<03:58, 59.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Loading and Initialization\",\n",
      "        \"Requirement\": \"The test case should ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The provided code does not include any explicit model loading or initialization; it is primarily focused on fetching and structuring datasets.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Prediction Outcomes\",\n",
      "        \"Requirement\": \"The test case should verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The provided code does not include any model prediction functionality, hence there is no test for verifying output dimensions and values from model predictions.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Evaluation Metrics\",\n",
      "        \"Requirement\": \"The test case should confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\",\n",
      "        \"Observation\": \"The provided code does not include any computation of evaluation metrics, hence there is no test for confirming the accuracy and correctness of such metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Evaluation\",\n",
      "        \"Requirement\": \"The test case should evaluate the model’s performance over training to identify potential overfitting. This could involve comparing training and validation loss.\",\n",
      "        \"Observation\": \"The provided code does not include any model training or performance evaluation, hence there is no test for identifying potential overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Threshold\",\n",
      "        \"Requirement\": \"The test case should define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The provided code does not define or enforce any performance thresholds for crucial metrics. It is mainly focused on testing the data fetching and structuring process.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Data Fetching and Structuring\",\n",
      "        \"Requirement\": \"The test case should ensure the datasets are fetched and structured correctly without errors, checking for issues in data setup or initialization.\",\n",
      "        \"Observation\": \"The provided code includes tests for fetching and structuring two datasets: MovieLens and StackExchange. It checks if the returned data is of the correct type, format, and shape.\",\n",
      "        \"Related Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\n",
      "        \"Evaluation\": 1\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_cross_validation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                                                         | 2/5 [01:56<02:53, 57.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    \n",
      "    {\n",
      "        \"Requirement Title\": \"Model Loading Test\",\n",
      "        \"Requirement\": \"Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The provided code does not perform any tests related to model loading or initialization.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Output Verification Test\",\n",
      "        \"Requirement\": \"Test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The provided code does not perform any tests related to verifying the output dimensions and values from model predictions.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Evaluation Metrics Accuracy Test\",\n",
      "        \"Requirement\": \"Test case to confirm the accuracy and correctness of evaluation metrics used within the system.\",\n",
      "        \"Observation\": \"The provided code does not perform any tests related to ensuring the accuracy and correctness of evaluation metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Overfitting Test\",\n",
      "        \"Requirement\": \"Test case to evaluate the model’s performance over training to identify potential overfitting.\",\n",
      "        \"Observation\": \"The provided code does not perform any tests related to identifying potential overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds Test\",\n",
      "        \"Requirement\": \"Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The provided code does not perform any tests related to defining and enforcing performance thresholds.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Data Splitting Test\",\n",
      "        \"Requirement\": \"Test case to ensure the data is split properly into training and testing sets.\",\n",
      "        \"Observation\": \"The provided code performs a test to ensure the data is split properly into training and testing sets with the correct percentage of data allocated to the test set and that the training and testing sets are disjoint.\",\n",
      "        \"Related Functions\": [\"test_random_train_test_split\"],\n",
      "        \"Evaluation\": 1\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_evaluation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 3/5 [03:11<02:11, 65.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Loading and Initializing Models\",\n",
      "        \"Requirement\": \"The system should contain a test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The system initializes the LightFM model in every testing function and there are no explicit tests for model loading or initialization.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_precision_at_k_with_ties\", \"test_recall_at_k\", \"test_auc_score\", \"test_intersections_check\"],\n",
      "        \"Evaluation\": 0.5\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Output Dimensions and Values\",\n",
      "        \"Requirement\": \"The system should contain a test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The system performs checks on the output of evaluation metrics in every testing function.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_precision_at_k_with_ties\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Evaluation Metrics Accuracy\",\n",
      "        \"Requirement\": \"The system should contain a test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\",\n",
      "        \"Observation\": \"The system checks the accuracy of precision, recall, and AUC in the respective testing functions.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Performance Over Training\",\n",
      "        \"Requirement\": \"The system should contain a test case to evaluate the model’s performance over training to identify potential overfitting. This could involve comparing training and validation loss.\",\n",
      "        \"Observation\": \"The system does not contain explicit tests for evaluating the model's performance over training time or for identifying overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds for Crucial Metrics\",\n",
      "        \"Requirement\": \"The system should contain a test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The system does not appear to contain tests that define or enforce performance thresholds for crucial metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_data.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 4/5 [04:02<00:59, 59.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"ML Pipeline Test Case - Dataset Fitting\",\n",
      "        \"Requirement\": \"A test case should be in place to ensure the dataset fitting process works as expected.\",\n",
      "        \"Observation\": \"The functions 'test_fitting' and 'test_fitting_no_identity' tests the dataset fitting process for different scenarios. They check whether the interactions and features shapes are as expected.\",\n",
      "        \"Related Functions\": [\"test_fitting\", \"test_fitting_no_identity\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"ML Pipeline Test Case - Exception Handling\",\n",
      "        \"Requirement\": \"A test case should be in place to ensure the ML pipeline can handle unexpected inputs or situations.\",\n",
      "        \"Observation\": \"The function 'test_exceptions' checks if ValueError is raised when trying to build interactions with values outside the specified range. The system recovers without crashing, which is a good practice in ML pipelines.\",\n",
      "        \"Related Functions\": [\"test_exceptions\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"ML Pipeline Test Case - Feature Building\",\n",
      "        \"Requirement\": \"A test case should be in place to ensure the feature building process works as expected, including normalization.\",\n",
      "        \"Observation\": \"The function 'test_build_features' tests the feature building process of the dataset. It checks if the built user and item features are correct and if the normalization of features is correct.\",\n",
      "        \"Related Functions\": [\"test_build_features\"],\n",
      "        \"Evaluation\": 1\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_api.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [05:31<00:00, 66.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation of the code in the desired JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Loading and Initialization\",\n",
      "        \"Requirement\": \"The model should load and initialize without errors\",\n",
      "        \"Observation\": \"The code contains tests for model loading and initialization. It checks for cases where input data may be empty or have different formats and datatypes.\",\n",
      "        \"Related Functions\": [\"test_empty_matrix\", \"test_matrix_types\", \"test_input_dtypes\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Predictions\",\n",
      "        \"Requirement\": \"The output dimensions and values from model predictions match expected outcomes\",\n",
      "        \"Observation\": \"The code contains tests to check the model's predict function under different conditions including not being fitted, overflow, and standard conditions.\",\n",
      "        \"Related Functions\": [\"test_predict\", \"test_overflow_predict\", \"test_predict_not_fitted\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Accuracy of Evaluation Metrics\",\n",
      "        \"Requirement\": \"The accuracy and correctness of evaluation metrics used within the system are computed correctly\",\n",
      "        \"Observation\": \"The code does not directly test evaluation metrics like precision, recall, etc. The tests are more focused on checking the functionality of the model.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Over Training\",\n",
      "        \"Requirement\": \"The model’s performance over training should be evaluated to identify potential overfitting\",\n",
      "        \"Observation\": \"The code does not contain tests that evaluate the model's performance over time to identify overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds\",\n",
      "        \"Requirement\": \"Performance thresholds for crucial metrics should be defined and enforced to guarantee model performance\",\n",
      "        \"Observation\": \"The code does not include tests that define and enforce performance thresholds for any metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.repo import Repository # under ~/src/test_creation/: ln -s ../code_analyzer/modules .\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "repo = Repository(\"../../data/raw/openja/lightfm/\")\n",
    "test_files = repo.list_test_files()\n",
    "test_files = test_files['Python']\n",
    "\n",
    "reports = defaultdict()\n",
    "for test_file in tqdm(test_files):\n",
    "    print(f\"Evaluating {test_file}\")\n",
    "    py_splits = load_test_file(test_file)\n",
    "\n",
    "    history = get_ai_responses(\n",
    "        py_splits=py_splits,\n",
    "        messages=[\n",
    "            \"How many functions are defined in the code? list them all\",\n",
    "            \"What is each of the functions doing?\",\n",
    "            \"Which of them are related to ML pipeline test cases?\"\n",
    "        ],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    report, history = evaluate_ml_tests(py_splits, history, CHECKLIST_SAMPLE)\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "    reports[test_file] = {\n",
    "        'report': report,\n",
    "        'history': history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273db18c-13c4-4c86-a4c8-f42e0b0e37c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
