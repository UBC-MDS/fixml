{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54ef8670-4c0a-45d6-8954-8b8aab11f994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "#from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "42069395-2ad1-46da-9d30-e601a2ba7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(path):\n",
    "    loader = PythonLoader(path)\n",
    "    py = loader.load()\n",
    "    py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "    return py_splits\n",
    "\n",
    "def get_ai_response(message, py_splits, history=None, chain=None):\n",
    "    if chain is None:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "        chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        \n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    history.add_user_message(message)\n",
    "    resp = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(resp)\n",
    "\n",
    "    return resp, history, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "311eb94f-7800-4221-b67f-2b3c0f387d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the provided code, there are 11 functions defined. They are:\n",
      "\n",
      "1. _generate_data\n",
      "2. _precision_at_k\n",
      "3. _recall_at_k\n",
      "4. _auc\n",
      "5. test_precision_at_k\n",
      "6. test_precision_at_k_with_ties\n",
      "7. test_recall_at_k\n",
      "8. test_auc_score\n",
      "9. test_intersections_check\n"
     ]
    }
   ],
   "source": [
    "py_splits = load_test_file('../../data/raw/openja/lightfm/tests/test_evaluation.py')\n",
    "\n",
    "resp, history, chain = get_ai_response(\n",
    "    message=\"How many functions are defined in the code? list them all\",\n",
    "    py_splits=py_splits\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3d21d37-b037-4ad7-8776-036a8fd6bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. `_generate_data`: This function generates a dataset where every user has interactions in both the training and the testing set.\n",
      "\n",
      "2. `_precision_at_k`: This function computes the precision at k for a model's predictions. Precision at k is a measure of how many of the top k recommendations are relevant.\n",
      "\n",
      "3. `_recall_at_k`: This function computes the recall at k for a model's predictions. Recall at k is a measure of how many of the relevant items are included in the top k recommendations.\n",
      "\n",
      "4. `_auc`: This function computes the Area Under the ROC Curve (AUC) for a model's predictions. The ROC curve is a plot of the true positive rate against the false positive rate, and the AUC measures the entire two-dimensional area underneath this curve.\n",
      "\n",
      "5. `test_precision_at_k`: This function tests the `precision_at_k` function to ensure it is working correctly.\n",
      "\n",
      "6. `test_precision_at_k_with_ties`: This function tests the `precision_at_k` function when there are ties in the model's predictions.\n",
      "\n",
      "7. `test_recall_at_k`: This function tests the `recall_at_k` function to ensure it is working correctly.\n",
      "\n",
      "8. `test_auc_score`: This function tests the `_auc` function to ensure it is working correctly.\n",
      "\n",
      "9. `test_intersections_check`: This function tests the intersection check in the evaluation functions. The intersection check ensures that the training and testing sets do not have any interactions in common.\n"
     ]
    }
   ],
   "source": [
    "resp, history, _ = get_ai_response(\n",
    "    message=\"What is each of the functions doing?\",\n",
    "    py_splits=py_splits,\n",
    "    history=history,\n",
    "    chain=chain\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62a1d0ae-a1b4-4482-a0f8-e2ad91f772b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following functions are related to Machine Learning (ML) pipeline test cases:\n",
      "\n",
      "1. `test_precision_at_k`: This function tests the precision at k for a model's predictions.\n",
      "\n",
      "2. `test_precision_at_k_with_ties`: This function tests the precision at k when there are ties in the model's predictions.\n",
      "\n",
      "3. `test_recall_at_k`: This function tests the recall at k for a model's predictions.\n",
      "\n",
      "4. `test_auc_score`: This function tests the Area Under the ROC Curve (AUC) for a model's predictions.\n",
      "\n",
      "5. `test_intersections_check`: This function tests the intersection check in the evaluation functions, which ensures that the training and testing sets do not have any interactions in common.\n"
     ]
    }
   ],
   "source": [
    "resp, history, _ = get_ai_response(\n",
    "    message=\"Which of them are related to ML pipeline test cases?\",\n",
    "    py_splits=py_splits,\n",
    "    history=history,\n",
    "    chain=chain\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907758d1-fb25-4024-b4f4-268e7773e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c98d19-50cf-41f9-bd38-6f4ef825fb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c848fc10-328d-43ce-8a3b-9caccb31c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 2016.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# load doc\n",
    "loader = DirectoryLoader(\n",
    "    '../../data/raw/openja/lightfm/tests', \n",
    "    #glob=\"**/*.py\", \n",
    "    glob=\"**/*.py\",\n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "# # retriever = vectorstore.as_retriever(k=4)\n",
    "# # docs = retriever.invoke(\"How many functions are there?\")\n",
    "\n",
    "# # define prompt and chat\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"messages\")\n",
    "# ])\n",
    "# chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "# # combine prompt, chat and doc\n",
    "# docs_chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "# for chunk in docs_chain.stream({\n",
    "#     \"context\": all_splits,\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"How many test functions are explicitly defined in this code? List them all and explain what each of them are doing.\")\n",
    "#     ],\n",
    "# }):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6ccf044-e9ab-457d-a70b-2fee41a06bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PythonLoader(\"../../data/raw/openja/lightfm/tests/test_evaluation.py\")\n",
    "py = loader.load()\n",
    "py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "len(py_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8a3b784-c516-433f-a1c5-c8d8906df524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_movielens.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_datasets.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_cross_validation.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/__init__.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_evaluation_2.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_data.py'},\n",
       " {'source': '../../data/raw/openja/lightfm/tests/test_api.py'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a79d3d1-e1e6-4849-972b-d890b46b6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code defines the following functions:\n",
      "\n",
      "1. `_generate_data(num_users, num_items, density=0.1, test_fraction=0.2)`\n",
      "2. `_precision_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`\n",
      "3. `_recall_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`\n",
      "4. `_auc(model, ground_truth, train=None, user_features=None, item_features=None)`\n",
      "5. `test_precision_at_k()`\n",
      "6. `test_precision_at_k_with_ties()`\n",
      "7. `test_recall_at_k()`\n",
      "8. `test_auc_score()`\n",
      "9. `test_intersections_check()`"
     ]
    }
   ],
   "source": [
    "# define prompt and chat\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "chat = ChatOpenAI(model='gpt-4-turbo')\n",
    "\n",
    "# combine prompt, chat and doc\n",
    "docs_chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "for chunk in docs_chain.stream({\n",
    "    \"context\": sub_splits, #all_splits,\n",
    "    \"messages\": [\n",
    "        #HumanMessage(content=\"How many test functions are explicitly defined in this code? List them all and explain what each of them are doing.\")\n",
    "        HumanMessage(content=\"Which many functions are defined in the code? list them all\")\n",
    "    ],\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "#[split.metadata for split in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3d0dcd9-afe0-49de-84b7-f9fda8008451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code defines the following functions:\n",
      "\n",
      "1. `_generate_data(num_users, num_items, density=0.1, test_fraction=0.2)`\n",
      "2. `_precision_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`\n",
      "3. `_recall_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`\n",
      "4. `_auc(model, ground_truth, train=None, user_features=None, item_features=None)`\n",
      "5. `test_precision_at_k()`\n",
      "6. `test_precision_at_k_with_ties()`\n",
      "7. `test_recall_at_k()`\n",
      "8. `test_auc_score()`\n",
      "9. `test_intersections_check()`\n"
     ]
    }
   ],
   "source": [
    "tmp = docs_chain.invoke({\n",
    "    \"context\": sub_splits, #all_splits,\n",
    "    \"messages\": [\n",
    "        #HumanMessage(content=\"How many test functions are explicitly defined in this code? List them all and explain what each of them are doing.\")\n",
    "        HumanMessage(content=\"Which many functions are defined in the code? list them all\")\n",
    "    ],\n",
    "})\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c43d0be7-a006-467e-a9ad-c698a78b04b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of what each function in the code does:\n",
      "\n",
      "1. **`_generate_data(num_users, num_items, density=0.1, test_fraction=0.2)`**\n",
      "   - This function generates synthetic data for testing in the form of user-item interaction matrices. It creates two matrices: `train` and `test`. Each user is assigned a set of items they interact with, split between the training and testing sets based on the specified `density` of interactions and the `test_fraction`.\n",
      "\n",
      "2. **`_precision_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`**\n",
      "   - Computes the precision at k for a given model and ground truth data. It optionally takes into account training interactions (to exclude them from the precision calculation), and user and item features. This function manually computes the precision metric, which is used to validate the built-in precision computation of the LightFM library.\n",
      "\n",
      "3. **`_recall_at_k(model, ground_truth, k, train=None, user_features=None, item_features=None)`**\n",
      "   - Similar to `_precision_at_k`, this function computes the recall at k metric for the model predictions against the ground truth data. It also considers optional training interactions to exclude, and user and item features. This helps in assessing how well the model can retrieve relevant items.\n",
      "\n",
      "4. **`_auc(model, ground_truth, train=None, user_features=None, item_features=None)`**\n",
      "   - Computes the Area Under the ROC Curve (AUC) for each user in the dataset given the model predictions and the ground truth interactions. It considers optional training data to exclude, as well as user and item features. The function calculates the AUC to measure the ability of the model to rank positive items higher than negative ones.\n",
      "\n",
      "5. **`test_precision_at_k()`**\n",
      "   - A test function that validates the precision at k metric's implementation by comparing results from the custom `_precision_at_k` implementation against the built-in LightFM function `evaluation.precision_at_k`. It also checks the function behavior with different values of k and examines the handling of training interactions.\n",
      "\n",
      "6. **`test_precision_at_k_with_ties()`**\n",
      "   - Tests the behavior of the precision at k calculation when all model predictions are the same (i.e., ties). It sets all model parameters related to predictions to zero, thereby creating a scenario where all predicted scores are tied, and ensures that the precision is handled correctly in such cases.\n",
      "\n",
      "7. **`test_recall_at_k()`**\n",
      "   - Similar to `test_precision_at_k`, this function tests the recall at k metric by comparing the custom implementation (`_recall_at_k`) with the built-in LightFM function under various conditions, including consideration of training interactions.\n",
      "\n",
      "8. **`test_auc_score()`**\n",
      "   - Tests the AUC scoring function. It compares the AUC scores obtained from the custom `_auc` implementation to those from the built-in LightFM function, ensuring they are within an acceptable margin of error. This test also handles scenarios with and without training interaction exclusion.\n",
      "\n",
      "9. **`test_intersections_check()`**\n",
      "   - This function tests the robustness of evaluation metrics when train and test datasets intersect. It ensures that proper errors are raised when intersections are checked and not supposed to exist, and verifies that no errors are raised when intersections are allowed or when train and test sets truly have no intersections.\n"
     ]
    }
   ],
   "source": [
    "tmp2 = docs_chain.invoke({\n",
    "    \"context\": sub_splits, \n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Which many functions are defined in the code? list them all\"),\n",
    "        AIMessage(content=tmp),\n",
    "        HumanMessage(content=\"What is each of the functions doing?\")\n",
    "    ],\n",
    "})\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8ff92bc-1446-4023-907a-d3bf91dddb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions related to ML (Machine Learning) pipeline test cases in the provided code are specifically designed to test the performance and behavior of the LightFM model under various conditions. These test case functions validate different aspects of the model's predictions, ensuring that the implementation of the evaluation metrics works as expected. Here are those functions:\n",
      "\n",
      "1. **`test_precision_at_k()`**\n",
      "   - Tests the precision at k metric implementation by comparing it against a custom calculation. It checks the metric under different configurations of k and examines how it handles training interactions.\n",
      "\n",
      "2. **`test_precision_at_k_with_ties()`**\n",
      "   - Focuses on testing the model's behavior in scenarios where there are ties in the predictions (i.e., all predicted scores are the same). This test is crucial for understanding how the model handles edge cases in ranking predictions.\n",
      "\n",
      "3. **`test_recall_at_k()`**\n",
      "   - Similar to the precision test, this function tests the recall at k metric. It compares the results from a custom implementation to the built-in LightFM function, ensuring the model accurately retrieves relevant items.\n",
      "\n",
      "4. **`test_auc_score()`**\n",
      "   - Tests the Area Under the ROC Curve (AUC) metric, comparing the custom implementation to the built-in function. This test is important for evaluating the model's ranking performance, particularly its ability to distinguish between positive and negative interactions.\n",
      "\n",
      "5. **`test_intersections_check()`**\n",
      "   - Tests how the model and its evaluation functions handle scenarios where the training and testing data might overlap. This function ensures that the model's evaluation metrics correctly identify and handle intersections, which is crucial for the integrity of training and testing phases in an ML pipeline.\n",
      "\n",
      "These test functions help ensure the reliability and accuracy of the model's evaluation metrics, which are fundamental components of the ML pipeline, particularly in the context of recommendation systems like those typically implemented using LightFM.\n"
     ]
    }
   ],
   "source": [
    "tmp3 = docs_chain.invoke({\n",
    "    \"context\": sub_splits, \n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Which many functions are defined in the code? list them all\"),\n",
    "        AIMessage(content=tmp),\n",
    "        HumanMessage(content=\"What is each of the functions doing?\"),\n",
    "        AIMessage(content=tmp2),\n",
    "        HumanMessage(content=\"Which of them are related to ML pipeline test cases?\"),\n",
    "    ],\n",
    "})\n",
    "print(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798f97a-27c8-425e-8a45-54e59b417e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bd4182a-6803-4996-a194-d1065fd49666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[split.metadata['source'] for split in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ebd3d3a-57c3-4d54-8eb1-6c9fb5770e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import numpy as np\\n\\nimport pytest\\n\\nimport scipy.sparse as sp\\n\\nfrom sklearn.metrics import roc_auc_score\\n\\nfrom lightfm.lightfm import LightFM\\nfrom lightfm import evaluation\\n\\n\\ndef _generate_data(num_users, num_items, density=0.1, test_fraction=0.2):\\n    # Generate a dataset where every user has interactions\\n    # in both the train and the test set.\\n\\n    train = sp.lil_matrix((num_users, num_items), dtype=np.float32)\\n    test = sp.lil_matrix((num_users, num_items), dtype=np.float32)\\n\\n    for user_id in range(num_users):\\n        positives = np.random.choice(\\n            num_items, size=int(density * num_items), replace=False\\n        )\\n\\n        for item_id in positives[: int(test_fraction * len(positives))]:\\n            test[user_id, item_id] = 1.0\\n\\n        for item_id in positives[int(test_fraction * len(positives)) :]:\\n            train[user_id, item_id] = 1.0\\n\\n    return train.tocoo(), test.tocoo()', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='def _precision_at_k(\\n    model, ground_truth, k, train=None, user_features=None, item_features=None\\n):\\n    # Alternative test implementation\\n\\n    ground_truth = ground_truth.tocsr()\\n\\n    no_users, no_items = ground_truth.shape\\n\\n    pid_array = np.arange(no_items, dtype=np.int32)\\n\\n    precisions = []\\n\\n    uid_array = np.empty(no_items, dtype=np.int32)\\n\\n    if train is not None:\\n        train = train.tocsr()\\n\\n    for user_id, row in enumerate(ground_truth):\\n        uid_array.fill(user_id)\\n\\n        predictions = model.predict(\\n            uid_array,\\n            pid_array,\\n            user_features=user_features,\\n            item_features=item_features,\\n            num_threads=4,\\n        )\\n        if train is not None:\\n            train_items = train[user_id].indices\\n            top_k = set(\\n                [x for x in np.argsort(-predictions) if x not in train_items][:k]\\n            )\\n        else:\\n            top_k = set(np.argsort(-predictions)[:k])', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='true_pids = set(row.indices[row.data == 1])\\n\\n        if true_pids:\\n            precisions.append(len(top_k & true_pids) / float(k))\\n\\n    return sum(precisions) / len(precisions)\\n\\n\\ndef _recall_at_k(\\n    model, ground_truth, k, train=None, user_features=None, item_features=None\\n):\\n    # Alternative test implementation\\n\\n    ground_truth = ground_truth.tocsr()\\n\\n    no_users, no_items = ground_truth.shape\\n\\n    pid_array = np.arange(no_items, dtype=np.int32)\\n\\n    recalls = []\\n\\n    uid_array = np.empty(no_items, dtype=np.int32)\\n\\n    if train is not None:\\n        train = train.tocsr()\\n\\n    for user_id, row in enumerate(ground_truth):\\n        uid_array.fill(user_id)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='predictions = model.predict(\\n            uid_array,\\n            pid_array,\\n            user_features=user_features,\\n            item_features=item_features,\\n            num_threads=4,\\n        )\\n        if train is not None:\\n            train_items = train[user_id].indices\\n            top_k = set(\\n                [x for x in np.argsort(-predictions) if x not in train_items][:k]\\n            )\\n        else:\\n            top_k = set(np.argsort(-predictions)[:k])\\n\\n        true_pids = set(row.indices[row.data == 1])\\n\\n        if true_pids:\\n            recalls.append(len(top_k & true_pids) / float(len(true_pids)))\\n\\n    return sum(recalls) / len(recalls)\\n\\n\\ndef _auc(model, ground_truth, train=None, user_features=None, item_features=None):\\n\\n    ground_truth = ground_truth.tocsr()\\n\\n    no_users, no_items = ground_truth.shape\\n\\n    pid_array = np.arange(no_items, dtype=np.int32)\\n\\n    scores = []\\n\\n    if train is not None:\\n        train = train.tocsr()', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='for user_id, row in enumerate(ground_truth):\\n        uid_array = np.empty(no_items, dtype=np.int32)\\n        uid_array.fill(user_id)\\n        predictions = model.predict(\\n            uid_array,\\n            pid_array,\\n            user_features=user_features,\\n            item_features=item_features,\\n            num_threads=4,\\n        )\\n\\n        true_pids = row.indices[row.data == 1]\\n\\n        grnd = np.zeros(no_items, dtype=np.int32)\\n        grnd[true_pids] = 1\\n\\n        if not len(true_pids):\\n            continue\\n\\n        if train is not None:\\n            train_indices = train[user_id].indices\\n            not_in_train = np.array([x not in train_indices for x in range(no_items)])\\n            scores.append(roc_auc_score(grnd[not_in_train], predictions[not_in_train]))\\n        else:\\n            scores.append(roc_auc_score(grnd, predictions))\\n\\n    return scores\\n\\n\\ndef test_precision_at_k():\\n\\n    no_users, no_items = (10, 100)\\n\\n    train, test = _generate_data(no_users, no_items)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='model = LightFM(loss=\"bpr\")\\n\\n    # We want a high precision to catch the k=1 case\\n    model.fit_partial(test)\\n\\n    for k in (10, 5, 1):\\n\\n        # Without omitting train interactions\\n        precision = evaluation.precision_at_k(model, test, k=k)\\n        expected_mean_precision = _precision_at_k(model, test, k)\\n\\n        assert np.allclose(precision.mean(), expected_mean_precision)\\n        assert len(precision) == (test.getnnz(axis=1) > 0).sum()\\n        assert (\\n            len(evaluation.precision_at_k(model, train, preserve_rows=True))\\n            == test.shape[0]\\n        )\\n\\n        # With omitting train interactions\\n        precision = evaluation.precision_at_k(\\n            model, test, k=k, train_interactions=train\\n        )\\n        expected_mean_precision = _precision_at_k(model, test, k, train=train)\\n\\n        assert np.allclose(precision.mean(), expected_mean_precision)\\n\\n\\ndef test_precision_at_k_with_ties():\\n\\n    no_users, no_items = (10, 100)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='train, test = _generate_data(no_users, no_items)\\n\\n    model = LightFM(loss=\"bpr\")\\n    model.fit_partial(train)\\n\\n    # Make all predictions zero\\n    model.user_embeddings = np.zeros_like(model.user_embeddings)\\n    model.item_embeddings = np.zeros_like(model.item_embeddings)\\n    model.user_biases = np.zeros_like(model.user_biases)\\n    model.item_biases = np.zeros_like(model.item_biases)\\n\\n    k = 10\\n\\n    precision = evaluation.precision_at_k(model, test, k=k)\\n\\n    # Pessimistic precision with all ties\\n    assert precision.mean() == 0.0\\n\\n\\ndef test_recall_at_k():\\n\\n    no_users, no_items = (10, 100)\\n\\n    train, test = _generate_data(no_users, no_items)\\n\\n    model = LightFM(loss=\"bpr\")\\n    model.fit_partial(test)\\n\\n    for k in (10, 5, 1):\\n\\n        # Without omitting train interactions\\n        recall = evaluation.recall_at_k(model, test, k=k)\\n        expected_mean_recall = _recall_at_k(model, test, k)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='assert np.allclose(recall.mean(), expected_mean_recall)\\n        assert len(recall) == (test.getnnz(axis=1) > 0).sum()\\n        assert (\\n            len(evaluation.recall_at_k(model, train, preserve_rows=True))\\n            == test.shape[0]\\n        )\\n\\n        # With omitting train interactions\\n        recall = evaluation.recall_at_k(model, test, k=k, train_interactions=train)\\n        expected_mean_recall = _recall_at_k(model, test, k, train=train)\\n\\n        assert np.allclose(recall.mean(), expected_mean_recall)\\n\\n\\ndef test_auc_score():\\n\\n    no_users, no_items = (10, 100)\\n\\n    train, test = _generate_data(no_users, no_items)\\n\\n    model = LightFM(loss=\"bpr\")\\n    model.fit_partial(train)\\n\\n    auc = evaluation.auc_score(model, test, num_threads=2)\\n    expected_auc = np.array(_auc(model, test))', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='assert auc.shape == expected_auc.shape\\n    assert np.abs(auc.mean() - expected_auc.mean()) < 0.01\\n    assert len(auc) == (test.getnnz(axis=1) > 0).sum()\\n    assert len(evaluation.auc_score(model, train, preserve_rows=True)) == test.shape[0]\\n\\n    # With omitting train interactions\\n    auc = evaluation.auc_score(model, test, train_interactions=train, num_threads=2)\\n    expected_auc = np.array(_auc(model, test, train))\\n    assert np.abs(auc.mean() - expected_auc.mean()) < 0.01\\n\\n\\ndef test_intersections_check():\\n\\n    no_users, no_items = (10, 100)\\n\\n    train, test = _generate_data(no_users, no_items)\\n\\n    model = LightFM(loss=\"bpr\")\\n    model.fit_partial(train)\\n\\n    # check error is raised when train and test have interactions in common\\n    with pytest.raises(ValueError):\\n        evaluation.auc_score(\\n            model, train, train_interactions=train, check_intersections=True\\n        )', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='with pytest.raises(ValueError):\\n        evaluation.recall_at_k(\\n            model, train, train_interactions=train, check_intersections=True\\n        )\\n\\n    with pytest.raises(ValueError):\\n        evaluation.precision_at_k(\\n            model, train, train_interactions=train, check_intersections=True\\n        )\\n\\n    with pytest.raises(ValueError):\\n        evaluation.reciprocal_rank(\\n            model, train, train_interactions=train, check_intersections=True\\n        )\\n\\n    # check no errors raised when train and test have no interactions in common\\n    evaluation.auc_score(\\n        model, test, train_interactions=train, check_intersections=True\\n    )\\n    evaluation.recall_at_k(\\n        model, test, train_interactions=train, check_intersections=True\\n    )\\n    evaluation.precision_at_k(\\n        model, test, train_interactions=train, check_intersections=True\\n    )\\n    evaluation.reciprocal_rank(\\n        model, test, train_interactions=train, check_intersections=True\\n    )', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'}),\n",
       " Document(page_content='# check no error is raised when there are intersections but flag is False\\n    evaluation.auc_score(\\n        model, train, train_interactions=train, check_intersections=False\\n    )\\n    evaluation.recall_at_k(\\n        model, train, train_interactions=train, check_intersections=False\\n    )\\n    evaluation.precision_at_k(\\n        model, train, train_interactions=train, check_intersections=False\\n    )\\n    evaluation.reciprocal_rank(\\n        model, train, train_interactions=train, check_intersections=False\\n    )', metadata={'source': '../../data/raw/openja/lightfm/tests/test_evaluation.py'})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[split.metadata['source'] for split in all_splits]\n",
    "\n",
    "sub_splits = [\n",
    "    split for split in all_splits\n",
    "    if split.metadata['source'] == '../../data/raw/openja/lightfm/tests/test_evaluation.py'\n",
    "]\n",
    "sub_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e128c-9865-48ca-b34e-d60fd2014ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
