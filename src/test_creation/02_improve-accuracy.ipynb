{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ef8670-4c0a-45d6-8954-8b8aab11f994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5879411-42b3-4768-a02a-09c818ef674e",
   "metadata": {},
   "source": [
    "### basic chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c848fc10-328d-43ce-8a3b-9caccb31c1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I said \"I love programming\" in French, which is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 61, 'total_tokens': 82}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-53b73ad2-9b05-46a4-b643-5f6da841a6d8-0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"Translate this sentence from English to French: I love programming.\"),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c06655f-c404-4392-831f-4fc13f827ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I just translated \"I love programming\" to French, which is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 75, 'total_tokens': 97}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e6cea2bd-cc9e-488d-80f9-b6d552472b8d-0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(\"hi!\")\n",
    "chat_history.add_ai_message(\"whats up?\")\n",
    "chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "chat_history.add_ai_message(\n",
    "    chain.invoke({\"messages\": chat_history.messages})\n",
    ")\n",
    "\n",
    "chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3471ec67-c036-4ba5-945a-901be50e8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I translated \"I love programming\" into French, which is \"J'adore la programmation.\""
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"messages\": chat_history.messages}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9938a5-7f40-43b8-a659-6a0fab71d5a0",
   "metadata": {},
   "source": [
    "### load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d221d147-2bb4-41e3-96a2-c5d53db59b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2237.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "loader = DirectoryLoader(\n",
    "    '../../data/raw/openja/lightfm/tests', \n",
    "    glob=\"**/*.py\", \n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5dadc1b-5f90-4fbb-a1b5-78740ca72e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py              test_data.py             test_fast_functions.py\n",
      "test_api.py              test_datasets.py         test_movielens.py\n",
      "test_cross_validation.py test_evaluation.py\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/raw/openja/lightfm/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65c5bcaf-40a1-4fd9-82a2-9db90f0d094d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "\n",
      "import scipy.sparse as sp\n",
      "\n",
      "\n",
      "from lightfm import _lightfm_fast\n",
      "\n",
      "\n",
      "def test_in_positives():\n",
      "\n",
      "    mat = sp.csr_matrix(np.array([[0, 1], [1, 0]])).astype(np.float32)\n",
      "\n",
      "    assert not _lightfm_fast.__test_in_positives(0, 0, _lightfm_fast.CSRMatrix(mat))\n",
      "    assert _lightfm_fast.__test_in_positives(0, 1, _lightfm_fast.CSRMatrix(mat))\n",
      "\n",
      "    assert _lightfm_fast.__test_in_positives(1, 0, _lightfm_fast.CSRMatrix(mat))\n",
      "    assert not _lightfm_fast.__test_in_positives(1, 1, _lightfm_fast.CSRMatrix(mat))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': '../../data/raw/openja/lightfm/tests/test_fast_functions.py'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a15019c4-b8fc-4462-87dc-6b028e35196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a783e5b-8dd3-4d0a-a1dd-195b6d7e9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b68c7e35-8e0f-4700-9551-02c3a426f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fd70b70-f9e6-480e-919c-26248e71e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='@pytest.mark.skip(reason=\"Runs out of memory in CI\")\\ndef test_basic_fetching_stackexchange():\\n\\n    test_fractions = (0.2, 0.5, 0.6)\\n\\n    for test_fraction in test_fractions:\\n        data = fetch_stackexchange(\\n            \"crossvalidated\",\\n            min_training_interactions=0,\\n            test_set_fraction=test_fraction,\\n        )\\n\\n        train = data[\"train\"]\\n        test = data[\"test\"]\\n\\n        assert isinstance(train, sp.coo_matrix)\\n        assert isinstance(test, sp.coo_matrix)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_datasets.py'}),\n",
       " Document(page_content='assert train.shape == test.shape\\n\\n        frac = float(test.getnnz()) / (train.getnnz() + test.getnnz())\\n        assert abs(frac - test_fraction) < 0.01\\n\\n    for dataset in (\"crossvalidated\", \"stackoverflow\"):', metadata={'source': '../../data/raw/openja/lightfm/tests/test_datasets.py'}),\n",
       " Document(page_content='assert test.nnz / float(data.nnz) == test_percentage\\n    _assert_disjoint(train, test)', metadata={'source': '../../data/raw/openja/lightfm/tests/test_cross_validation.py'}),\n",
       " Document(page_content='def test_bpr_precision():\\n\\n    model = LightFM(learning_rate=0.05, loss=\"bpr\", random_state=SEED)\\n\\n    model.fit_partial(train, epochs=10)\\n\\n    (train_precision, test_precision, full_train_auc, full_test_auc) = _get_metrics(\\n        model, train, test\\n    )\\n\\n    assert train_precision > 0.45\\n    assert test_precision > 0.07\\n\\n    assert full_train_auc > 0.91\\n    assert full_test_auc > 0.87\\n\\n\\ndef test_bpr_precision_multithreaded():', metadata={'source': '../../data/raw/openja/lightfm/tests/test_movielens.py'})]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k is the number of chunks to retrieve\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "\n",
    "docs = retriever.invoke(\"How many test functions are there?\")\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df4289-7a4b-41b8-b0f8-f571765dbc8a",
   "metadata": {},
   "source": [
    "### combine chatbot and document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb7542dd-42ec-4836-a907-6a1fd80044ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 1811.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three test functions in the given code snippets. However, there are duplicate appearances of each function, so if we consider unique function names, there are only three. The test functions are:\n",
      "\n",
      "1. test_bpr_precision()\n",
      "2. test_bpr_precision_multithreaded()\n",
      "3. test_warp_kos_precision()"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# load doc\n",
    "loader = DirectoryLoader(\n",
    "    '../../data/raw/openja/lightfm/tests', \n",
    "    glob=\"**/*.py\", \n",
    "    show_progress=True, \n",
    "    #use_multithreading=True,\n",
    "    loader_cls=PythonLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "docs = retriever.invoke(\"How many test functions are there?\")\n",
    "\n",
    "# define prompt and chat\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "# combine prompt, chat and doc\n",
    "docs_chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "for chunk in docs_chain.stream({\n",
    "    \"context\": docs,\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"How many test functions are there? Can you list them all?\")\n",
    "    ],\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb071849-2faa-451d-b59d-606b009a3bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a9f2a-bb75-4e34-8d70-da09d793f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1551c-a568-49b7-a79e-be38935b1dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b155a3b-f109-4181-b5fe-5bc04714051c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0e418-229b-4018-974b-e1dbe4615d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "960bc9e1-a2bc-437c-abc1-6f3df79cf731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4')\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a ChatGPT 4.0, a large language model trained by OpenAI. Answer as concisely as possible.\\nKnowledge cutoff: 2023-12-31\\nCurrent date: {CurrentDate}\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output_str = chain.invoke({\n",
    "    \"input\": \"can you write me a sample python code, please?\",\n",
    "    \"CurrentDate\": \"2024-05-01\"\n",
    "})\n",
    "\n",
    "type(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3717d367-a7c1-479c-b615-ec6fd8f8cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e1d64633-08b6-48d4-86aa-353603a55bcc-0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "llm.invoke([HumanMessage('Hi')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc133340-0eb7-475d-abe8-f9433368c87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e8064db8-a961-4bb8-b3db-f07b09dc7189-0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9d19dd-e3cc-4fea-a3de-9fc13d7c9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     llm = ChatOpenAI()\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\", \"You are a world class Machine Learning engineer.\"),\n",
    "#         (\"user\", \"{input}\")\n",
    "#     ])\n",
    "\n",
    "#     output_parser = StrOutputParser()\n",
    "\n",
    "#     chain = prompt | llm | output_parser\n",
    "\n",
    "#     output_str = chain.invoke({\"input\": \"Hello LLM World! I can't wait to use you to write a test suite for my ML pipeline!\"})\n",
    "\n",
    "#     print(output_str)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb1d48-0fbd-43f3-ab53-a5bd702933b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
