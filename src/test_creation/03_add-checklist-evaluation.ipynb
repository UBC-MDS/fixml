{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b86aff-df94-4fb5-a547-11d65e54896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcd7bf4e-a5d4-4a57-bb50-d26d487fd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKLIST_SAMPLE = '''\n",
    "1. Does it contain test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization?\n",
    "2. Does it contain test case to verify that the output dimensions and values from model predictions match expected outcomes?\n",
    "3. Does it contain test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly?\n",
    "4. Does it contain test case to evaluate the model’s performance over training to identify potential overfitting? This could involve comparing training and validation loss.\n",
    "5. Does it contain test case to define and enforce performance thresholds for crucial metrics to guarantee model performance?\n",
    "'''\n",
    "\n",
    "def load_test_file(path):\n",
    "    loader = PythonLoader(path)\n",
    "    py = loader.load()\n",
    "    py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "    return py_splits\n",
    "\n",
    "def get_ai_response(message, py_splits, history=None, chain=None):\n",
    "    if chain is None:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a coder analyzer. Please understand the code and answer the question as accurate as possible. Analyze the test functions from the codes below:\\n\\n{context}\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "        chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        \n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    history.add_user_message(message)\n",
    "    resp = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(resp)\n",
    "\n",
    "    return resp, history, chain\n",
    "\n",
    "def get_ai_responses(messages, py_splits, verbose=True):\n",
    "    for i, msg in enumerate(messages):\n",
    "        if verbose:\n",
    "            print(f\"Q: {msg}\")\n",
    "        \n",
    "        if i == 0:\n",
    "            resp, history, chain = get_ai_response(\n",
    "                message=messages[i],\n",
    "                py_splits=py_splits\n",
    "            )\n",
    "        else:\n",
    "            resp, history, _ = get_ai_response(\n",
    "                message=messages[i],\n",
    "                py_splits=py_splits,\n",
    "                history=history,\n",
    "                chain=chain\n",
    "            )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Response: {resp}\")\n",
    "            print()\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_ml_tests(py_splits, history, checklist):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a senior machine learning engineer who specializes in performing Machine Learning system testing. Extract and analyze the test functions from the codes:\\n\\n{context}\"),\n",
    "        (\"system\", f\"Here is the Machine Learning system testing checklist delimited by triple quotes '''{checklist}'''\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    chat = ChatOpenAI(model='gpt-4')\n",
    "    chain = create_stuff_documents_chain(chat, prompt)\n",
    "\n",
    "    history.add_user_message(\"\"\"\n",
    "        Evaluate whether the codes has fulfilled the requirements and deliver a completion score. Do not include a summary evaluation.\n",
    "        Desired JSON format:\n",
    "            {\n",
    "                \"Requirement Title\":\n",
    "                \"Requirement\":\n",
    "                \"Observation\":\n",
    "                \"Related Functions\": [ ... ]\n",
    "                \"Evaluation\": (1 for Fulfilled / 0.5 for Partially fulfilled / 0 for Not fulfilled)\n",
    "            }\n",
    "    \"\"\")\n",
    "    \n",
    "    # for chunk in chain.stream({\n",
    "    #     \"context\": py_splits,\n",
    "    #     \"messages\": history.messages\n",
    "    # }):\n",
    "    #     print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    report = chain.invoke({\n",
    "        \"context\": py_splits, \n",
    "        \"messages\": history.messages\n",
    "    })\n",
    "\n",
    "    history.add_ai_message(report)\n",
    "\n",
    "    return report, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "742b0391-ae53-45a3-a8b7-fb7816c0a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_splits = load_test_file('../../data/raw/openja/lightfm/tests/test_evaluation.py')\n",
    "\n",
    "history = get_ai_responses(\n",
    "    py_splits=py_splits,\n",
    "    messages=[\n",
    "        \"How many functions are defined in the code? list them all\",\n",
    "        \"What is each of the functions doing?\",\n",
    "        \"Which of them are related to ML pipeline test cases?\"\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "report, history = evaluate_ml_tests(py_splits, history, CHECKLIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82b23285-c6bc-49f2-a5fc-6d17a38a9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Loading and Initialization\",\n",
      "        \"Requirement\": \"Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization\",\n",
      "        \"Observation\": \"The code doesn't seem to have explicit test cases for model loading and initialization. The model is fit within each test case but there is no explicit verification of successful model setup or initialization.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Output Dimensions and Values from Model Predictions\",\n",
      "        \"Requirement\": \"Test case to verify that the output dimensions and values from model predictions match expected outcomes\",\n",
      "        \"Observation\": \"The code does seem to test the output of model predictions within the test cases for precision, recall, and AUC. However, it does not explicitly verify the dimensions of these outputs.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 0.5\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Accuracy and Correctness of Evaluation Metrics\",\n",
      "        \"Requirement\": \"Test case to confirm the accuracy and correctness of evaluation metrics used within the system\",\n",
      "        \"Observation\": \"The code includes test cases to verify the correctness of the precision, recall, and AUC functions. It compares the output of these functions against expected outputs.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Model’s Performance Over Training\",\n",
      "        \"Requirement\": \"Test case to evaluate the model’s performance over training to identify potential overfitting\",\n",
      "        \"Observation\": \"The code does not include any test cases that evaluate the model's performance over time during training. Therefore, it does not explicitly check for overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds for Crucial Metrics\",\n",
      "        \"Requirement\": \"Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance\",\n",
      "        \"Observation\": \"The code does not include test cases that enforce specific thresholds for the performance metrics. It checks for correctness of the metrics but does not enforce minimum acceptable values.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ad617a7-59c3-469c-b97f-4d66ab08a6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Requirement Title': 'Model Loading and Initialization',\n",
       "  'Requirement': 'Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization',\n",
       "  'Observation': \"The code doesn't seem to have explicit test cases for model loading and initialization. The model is fit within each test case but there is no explicit verification of successful model setup or initialization.\",\n",
       "  'Related Functions': [],\n",
       "  'Evaluation': 0},\n",
       " {'Requirement Title': 'Output Dimensions and Values from Model Predictions',\n",
       "  'Requirement': 'Test case to verify that the output dimensions and values from model predictions match expected outcomes',\n",
       "  'Observation': 'The code does seem to test the output of model predictions within the test cases for precision, recall, and AUC. However, it does not explicitly verify the dimensions of these outputs.',\n",
       "  'Related Functions': ['test_precision_at_k',\n",
       "   'test_recall_at_k',\n",
       "   'test_auc_score'],\n",
       "  'Evaluation': 0.5},\n",
       " {'Requirement Title': 'Accuracy and Correctness of Evaluation Metrics',\n",
       "  'Requirement': 'Test case to confirm the accuracy and correctness of evaluation metrics used within the system',\n",
       "  'Observation': 'The code includes test cases to verify the correctness of the precision, recall, and AUC functions. It compares the output of these functions against expected outputs.',\n",
       "  'Related Functions': ['test_precision_at_k',\n",
       "   'test_recall_at_k',\n",
       "   'test_auc_score'],\n",
       "  'Evaluation': 1},\n",
       " {'Requirement Title': 'Model’s Performance Over Training',\n",
       "  'Requirement': 'Test case to evaluate the model’s performance over training to identify potential overfitting',\n",
       "  'Observation': \"The code does not include any test cases that evaluate the model's performance over time during training. Therefore, it does not explicitly check for overfitting.\",\n",
       "  'Related Functions': [],\n",
       "  'Evaluation': 0},\n",
       " {'Requirement Title': 'Performance Thresholds for Crucial Metrics',\n",
       "  'Requirement': 'Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance',\n",
       "  'Observation': 'The code does not include test cases that enforce specific thresholds for the performance metrics. It checks for correctness of the metrics but does not enforce minimum acceptable values.',\n",
       "  'Related Functions': [],\n",
       "  'Evaluation': 0}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7589113e-c14a-4d5f-93e7-7387b4f316be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist Evaluation:\n",
      "\n",
      "Requirement Title: Model Loading and Initialization\n",
      "Requirement: Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\n",
      "Observation: The code includes functions to fit a model to the data, but there is no explicit test case to check if the model loads correctly without errors.\n",
      "Related Functions: N/A\n",
      "Evaluation: Not fulfilled\n",
      "\n",
      "Requirement Title: Output Verification\n",
      "Requirement: Test case to verify that the output dimensions and values from model predictions match expected outcomes.\n",
      "Observation: The code includes several tests to verify that the outputs of the evaluation metrics match the expected outcomes. However, there isn't a specific test for verifying the dimensions and values of model predictions.\n",
      "Related Functions: test_precision_at_k, test_precision_at_k_with_ties, test_recall_at_k, test_auc_score\n",
      "Evaluation: Partially fulfilled\n",
      "\n",
      "Requirement Title: Evaluation Metrics Accuracy\n",
      "Requirement: Test case to confirm the accuracy and correctness of evaluation metrics used within the system.\n",
      "Observation: The code includes several tests to verify that the outputs of the evaluation metrics match the expected outcomes.\n",
      "Related Functions: test_precision_at_k, test_precision_at_k_with_ties, test_recall_at_k, test_auc_score\n",
      "Evaluation: Fulfilled\n",
      "\n",
      "Requirement Title: Overfitting Identification\n",
      "Requirement: Test case to evaluate the model’s performance over training to identify potential overfitting.\n",
      "Observation: The code does not include a test case to identify potential overfitting.\n",
      "Related Functions: N/A\n",
      "Evaluation: Not fulfilled\n",
      "\n",
      "Requirement Title: Performance Thresholds\n",
      "Requirement: Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\n",
      "Observation: The code includes tests that compare the model's performance to expected outcomes, but does not include a test case to enforce performance thresholds.\n",
      "Related Functions: test_precision_at_k, test_precision_at_k_with_ties, test_recall_at_k, test_auc_score\n",
      "Evaluation: Partially fulfilled\n",
      "\n",
      "Completion Score: 50%"
     ]
    }
   ],
   "source": [
    "evaluate_ml_tests(py_splits, history, CHECKLIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ee6225d-3a7a-4bc3-a07f-b90d82151d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ../../data/raw/openja/lightfm/tests/test_datasets.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████████████████▌                                                                                                                                                                  | 1/5 [00:47<03:09, 47.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Model Data Loading\",\n",
      "        \"Requirement\": \"The code should contain test cases to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The code is testing data fetching methods, ensuring that the data loaded is as expected and correctly formatted.\",\n",
      "        \"Related Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Output Dimensions and Values Verification\",\n",
      "        \"Requirement\": \"The code should contain test cases to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The code tests the shape and type of the fetched data. However, it does not test any model predictions.\",\n",
      "        \"Related Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\n",
      "        \"Evaluation\": 0.5\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Evaluation Metrics Accuracy\",\n",
      "        \"Requirement\": \"The code should contain test cases to confirm the accuracy and correctness of evaluation metrics used within the system.\",\n",
      "        \"Observation\": \"The code does not test the accuracy or correctness of any evaluation metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Model Performance Over Training\",\n",
      "        \"Requirement\": \"The code should contain test cases to evaluate the model’s performance over training to identify potential overfitting.\",\n",
      "        \"Observation\": \"The code does not contain any tests related to model training or performance evaluation.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Performance Thresholds Enforcement\",\n",
      "        \"Requirement\": \"The code should contain test cases to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The code does not contain any tests related to performance thresholds or model performance enforcement.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_cross_validation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                                                         | 2/5 [01:44<02:38, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Data Splitting\",\n",
      "        \"Requirement\": \"The code should contain a test case to ensure that the data splitting function correctly splits the dataset into disjoint training and test sets.\",\n",
      "        \"Observation\": \"The function `test_random_train_test_split(test_percentage)` is a test case that checks the data splitting functionality. It asserts that the training and test sets are disjoint and that the size of the test set is as expected. Hence, the requirement is fulfilled.\",\n",
      "        \"Related Functions\": [\"test_random_train_test_split(test_percentage)\", \"_assert_disjoint(x, y)\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Model Loading and Initialization\",\n",
      "        \"Requirement\": \"The code should contain a test case to ensure that models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The provided code does not contain any test case related to model loading and initialization.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Model Prediction Output\",\n",
      "        \"Requirement\": \"The code should contain a test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The provided code does not contain any test case related to verifying the output dimensions and values from model predictions.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Evaluation Metrics\",\n",
      "        \"Requirement\": \"The code should contain a test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\",\n",
      "        \"Observation\": \"The provided code does not contain any test case related to confirming the accuracy and correctness of evaluation metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Model Performance Over Training\",\n",
      "        \"Requirement\": \"The code should contain a test case to evaluate the model's performance over training to identify potential overfitting.\",\n",
      "        \"Observation\": \"The provided code does not contain any test case related to evaluating the model's performance over training.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Performance Thresholds\",\n",
      "        \"Requirement\": \"The code should contain a test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The provided code does not contain any test case related to defining and enforcing performance thresholds.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_evaluation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 3/5 [02:43<01:51, 55.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation in the desired JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Model Loading and Initialization\",\n",
      "        \"Requirement\": \"Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The code does not have a specific test case to check the loading and initialization of the models.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Output Dimensions and Values\",\n",
      "        \"Requirement\": \"Test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The code has test cases to validate the output from the model match the expected outcomes in precision, recall, and AUC.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Accuracy of Evaluation Metrics\",\n",
      "        \"Requirement\": \"Test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\",\n",
      "        \"Observation\": \"The code has test cases to validate the accuracy and correctness of precision, recall, and AUC metrics.\",\n",
      "        \"Related Functions\": [\"test_precision_at_k\", \"test_recall_at_k\", \"test_auc_score\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Model Overfitting\",\n",
      "        \"Requirement\": \"Test case to evaluate the model’s performance over training to identify potential overfitting. This could involve comparing training and validation loss.\",\n",
      "        \"Observation\": \"The code does not have a specific test case to check for model overfitting.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Case for Performance Thresholds\",\n",
      "        \"Requirement\": \"Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\",\n",
      "        \"Observation\": \"The code does not have a specific test case to check for performance thresholds for crucial metrics.\",\n",
      "        \"Related Functions\": [],\n",
      "        \"Evaluation\": 0\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_data.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 4/5 [03:25<00:50, 50.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Fitting\",\n",
      "        \"Requirement\": \"A test case to ensure models are loaded correctly and the fitting method of the Dataset class works correctly.\",\n",
      "        \"Observation\": \"The function `test_fitting` successfully tests the fitting method of the Dataset class and the shapes of the interaction, user features, and item features are as expected.\",\n",
      "        \"Related Functions\": [\"test_fitting\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Fitting with No Identity\",\n",
      "        \"Requirement\": \"A test case to ensure models are loaded correctly and the fitting method of the Dataset class works correctly without identity features.\",\n",
      "        \"Observation\": \"The function `test_fitting_no_identity` successfully tests the fitting method of the Dataset class when identity features are turned off.\",\n",
      "        \"Related Functions\": [\"test_fitting_no_identity\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Exception Handling\",\n",
      "        \"Requirement\": \"A test case to ensure the model handles exceptions correctly during the fitting process.\",\n",
      "        \"Observation\": \"The function `test_exceptions` successfully tests the exception handling in the Dataset class during the fitting process.\",\n",
      "        \"Related Functions\": [\"test_exceptions\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Test Feature Building\",\n",
      "        \"Requirement\": \"A test case to ensure the feature building functionality works correctly.\",\n",
      "        \"Observation\": \"The function `test_build_features` successfully tests the feature building functionality, including building from lists and dictionaries and normalization of features.\",\n",
      "        \"Related Functions\": [\"test_build_features\"],\n",
      "        \"Evaluation\": 1\n",
      "    }\n",
      "]\n",
      "\n",
      "Evaluating ../../data/raw/openja/lightfm/tests/test_api.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [04:54<00:00, 58.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Model Initialization\",\n",
      "        \"Requirement\": \"The code should contain a test case to ensure that models are loaded correctly without errors, checking for issues in model setup or initialization.\",\n",
      "        \"Observation\": \"The code includes several tests such as test_return_self(), test_param_sanity(), test_sklearn_api(), test_predict_not_fitted() that verify the model initialization and setup.\",\n",
      "        \"Related Functions\": [\"test_return_self()\", \"test_param_sanity()\", \"test_sklearn_api()\", \"test_predict_not_fitted()\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Model Prediction\",\n",
      "        \"Requirement\": \"The code should contain a test case to verify that the output dimensions and values from model predictions match expected outcomes.\",\n",
      "        \"Observation\": \"The test_predict() function checks the model's ability to predict scores for users and items. The test_predict_ranks() function tests the model's ability to predict ranks of items.\",\n",
      "        \"Related Functions\": [\"test_predict()\", \"test_predict_ranks()\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Handling of Different Data Types and Formats\",\n",
      "        \"Requirement\": \"The code should contain a test case to confirm the model’s ability to handle different data types and formats.\",\n",
      "        \"Observation\": \"The test_input_dtypes(), test_matrix_types() functions verify the model's handling of different data types and sparse matrix formats.\",\n",
      "        \"Related Functions\": [\"test_input_dtypes()\", \"test_matrix_types()\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Error Handling\",\n",
      "        \"Requirement\": \"The code should contain a test case to verify the model's ability to raise and handle errors and exceptions effectively.\",\n",
      "        \"Observation\": \"Several tests such as test_param_sanity(), test_predict_not_fitted(), test_nan_features(), test_nan_interactions(), test_overflow_predict() check the model's error handling capabilities.\",\n",
      "        \"Related Functions\": [\"test_param_sanity()\", \"test_predict_not_fitted()\", \"test_nan_features()\", \"test_nan_interactions()\", \"test_overflow_predict()\"],\n",
      "        \"Evaluation\": 1\n",
      "    },\n",
      "    {\n",
      "        \"Requirement Title\": \"Testing Edge Cases\",\n",
      "        \"Requirement\": \"The code should contain a test case to verify the model’s performance in edge cases, such as an empty matrix or very few items.\",\n",
      "        \"Observation\": \"The test_empty_matrix(), test_warp_few_items() functions test the model's performance in edge cases.\",\n",
      "        \"Related Functions\": [\"test_empty_matrix()\", \"test_warp_few_items()\"],\n",
      "        \"Evaluation\": 1\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.repo import Repository\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "repo = Repository(\"../../data/raw/openja/lightfm/\")\n",
    "test_files = repo.list_test_files()\n",
    "test_files = test_files['Python']\n",
    "\n",
    "reports = defaultdict()\n",
    "for test_file in tqdm(test_files):\n",
    "    print(f\"Evaluating {test_file}\")\n",
    "    py_splits = load_test_file(test_file)\n",
    "\n",
    "    history = get_ai_responses(\n",
    "        py_splits=py_splits,\n",
    "        messages=[\n",
    "            \"How many functions are defined in the code? list them all\",\n",
    "            \"What is each of the functions doing?\",\n",
    "            \"Which of them are related to ML pipeline test cases?\"\n",
    "        ],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    report, history = evaluate_ml_tests(py_splits, history, CHECKLIST_SAMPLE)\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "    reports[test_file] = {\n",
    "        'report': report,\n",
    "        'history': history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccfe3f35-3164-4606-9933-b3048f4b0764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist Evaluation:\n",
      "\n",
      "1. Requirement Title: Model Loading and Initialization\n",
      "   Requirement: Test case to ensure models are loaded correctly without errors, checking for issues in model setup or initialization.\n",
      "   Observation: The code does not involve any model loading or initialization. It tests data fetching and data structure.\n",
      "   Related Functions: None\n",
      "   Evaluation: Not fulfilled\n",
      "\n",
      "2. Requirement Title: Output Dimensions and Values\n",
      "   Requirement: Test case to verify that the output dimensions and values from model predictions match expected outcomes.\n",
      "   Observation: The code verifies the dimensions of the fetched data. However, there are no model predictions involved in the test.\n",
      "   Related Functions: test_basic_fetching_movielens, test_basic_fetching_stackexchange\n",
      "   Evaluation: Partially fulfilled\n",
      "\n",
      "3. Requirement Title: Evaluation Metrics\n",
      "   Requirement: Test case to confirm the accuracy and correctness of evaluation metrics used within the system, ensuring that metrics such as precision, recall, AUC, etc., are computed correctly.\n",
      "   Observation: The code does not involve any computation or verification of evaluation metrics.\n",
      "   Related Functions: None\n",
      "   Evaluation: Not fulfilled\n",
      "\n",
      "4. Requirement Title: Overfitting Detection\n",
      "   Requirement: Test case to evaluate the model’s performance over training to identify potential overfitting. This could involve comparing training and validation loss.\n",
      "   Observation: The code does not evaluate model performance or overfitting. It tests data fetching and data structure.\n",
      "   Related Functions: None\n",
      "   Evaluation: Not fulfilled\n",
      "\n",
      "5. Requirement Title: Performance Thresholds\n",
      "   Requirement: Test case to define and enforce performance thresholds for crucial metrics to guarantee model performance.\n",
      "   Observation: The code does not define or enforce any performance thresholds. It tests data fetching and data structure.\n",
      "   Related Functions: None\n",
      "   Evaluation: Not fulfilled\n",
      "\n",
      "Completion Score: 10% (1 out of 5 requirements partially fulfilled)\n"
     ]
    }
   ],
   "source": [
    "print(reports['../../data/raw/openja/lightfm/tests/test_datasets.py']['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3f34d-66e3-4785-b05d-990b984c40b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
