{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4933df8d-6a7e-45c8-8b05-d33811ee09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb2cc86-28c7-4080-a288-4bbdbb675bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "checklist = Checklist('../../checklist/checklist.csv', checklist_format=ChecklistFormat.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ec0b9e-e7f9-4aa7-b28f-49054c6da0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestGenerator(llm, checklist=checklist)\n",
    "# generator.prompt = PromptTemplate(\n",
    "#     template=\"You are an expert Machine Learning Engineer.\\n\"\n",
    "#              \"Please generate empty test functions with numpy-format docstring based corresponding requirement of given checklist.\\n\"\n",
    "#              #\"{format_instructions}\\n\" # FIXME: define python function format\n",
    "#              \"Here is the checklist as a list of JSON objects:\\n```{checklist}```\\n\",\n",
    "#     description=\"Test Specification Generation for Machine Learning Project\",\n",
    "#     input_variables=[\"checklist\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9571619f-ffe2-45a4-a3fb-fc2d85e254f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_in_expected_format():\n",
      "    \"\"\"Verifies that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\"\"\"\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "result = generator.generate_spec()\n",
    "print(generator.spec[1]['Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea5db86-512e-4f33-a2e3-984cd91e3894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"ID\": \"2.1\", \"Title\": \"Ensure Data File Loads as Expected\", \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\"}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d2450cf-e7d0-4fff-9798-a99fa91ab43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"id\": {\"description\": \"The corresponding `ID` of the checklist item provided\", \"title\": \"Id\", \"type\": \"string\"}, \"title\": {\"description\": \"The corresponding `Title` of the checklist item provided\", \"title\": \"Title\", \"type\": \"string\"}, \"Function\": {\"description\": \"A test function with the docstring of numpy format\", \"title\": \"Function\", \"type\": \"string\"}}, \"required\": [\"id\", \"title\", \"Function\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "651019c1-2316-4795-8bd2-e150570393cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Any\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class TestSpec(BaseModel):\n",
    "    ID: str = Field(description=\"The corresponding `ID` of the checklist item provided\")\n",
    "    Title: str = Field(description=\"The corresponding `Title` of the checklist item provided\")\n",
    "    Function: str = Field(description=\"A Python test function with the docstring of numpy format\") # FIXME: define python function format\n",
    "\n",
    "class TestSpecs(BaseModel):\n",
    "    results: List[TestSpec]\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=TestSpecs)\n",
    "        \n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are an expert Machine Learning Engineer.\\n\"\n",
    "             \"Please generate Python test functions based on corresponding requirement of given checklist, with docstring of numpy format.\\n\"\n",
    "             \"{format_instructions}\\n\" \n",
    "             \"Here is the checklist as a list of JSON objects:\\n```{checklist}```\\n\",\n",
    "             #\"Here is the code to be analyzed:\\n{context}\",\n",
    "    description=\"Test Specification Generation for Machine Learning Project\",\n",
    "    #input_variables=[\"checklist\", \"context\"],\n",
    "    input_variables=[\"checklist\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"checklist\": generator.test_items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcada402-dae7-4bda-98d1-3faaf31d246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_file_load():\n",
      "    \"\"\"\n",
      "    Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
      "    \"\"\"\n",
      "    # Test implementation here\n"
     ]
    }
   ],
   "source": [
    "print(response['results'][0]['Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a799478-f796-4ada-9acc-66e48d098b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a7c7448-194f-477c-b3b5-5206f4b4148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"name of an actor\", \"title\": \"Name\", \"type\": \"string\"}, \"film_names\": {\"description\": \"list of names of films they starred in\", \"items\": {\"type\": \"string\"}, \"title\": \"Film Names\", \"type\": \"array\"}}, \"required\": [\"name\", \"film_names\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "#     input_variables=[\"query\"],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm | parser\n",
    "\n",
    "# chain.invoke({\"query\": actor_query})\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4c6e2-ded9-4e7b-b937-6ba4e73caf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa0ddb78-8796-47c6-b68d-422a40895d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'ID': '2.1',\n",
       "   'Title': 'Ensure Data File Loads as Expected',\n",
       "   'Function': 'def test_data_file_loads_as_expected():\\n    \"\"\"Tests the data-loading function to ensure files load correctly and return expected results\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '3.2',\n",
       "   'Title': 'Data in the Expected Format',\n",
       "   'Function': 'def test_data_in_expected_format():\\n    \"\"\"Verifies that the data matches the expected format for processing algorithms\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '3.5',\n",
       "   'Title': 'Check for Duplicate Records in Data',\n",
       "   'Function': 'def test_check_duplicate_records_in_data():\\n    \"\"\"Checks for duplicate records in the dataset and ensures there are none\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '4.2',\n",
       "   'Title': 'Verify Data Split Proportion',\n",
       "   'Function': 'def test_verify_data_split_proportion():\\n    \"\"\"Checks that data is split into training and testing sets in the expected proportion\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '5.3',\n",
       "   'Title': 'Ensure Model Output Shape Aligns with Expectation',\n",
       "   'Function': 'def test_model_output_shape_aligns_with_expectation():\\n    \"\"\"Ensures the shape of the model\\'s output aligns with the expected structure based on the task\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '6.1',\n",
       "   'Title': 'Verify Evaluation Metrics Implementation',\n",
       "   'Function': 'def test_verify_evaluation_metrics_implementation():\\n    \"\"\"Verifies that evaluation metrics are correctly implemented and appropriate for the model\\'s task\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'},\n",
       "  {'ID': '6.2',\n",
       "   'Title': \"Evaluate Model's Performance Against Thresholds\",\n",
       "   'Function': 'def test_evaluate_model_performance_against_thresholds():\\n    \"\"\"Computes evaluation metrics for training and testing datasets and ensures they exceed predefined thresholds\\n\\n    Parameters:\\n    ----------\\n    Returns:\\n    ----------\\n    \"\"\"\\n    pass'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"You are an expert Machine Learning Engineer.\\n\"\n",
    "             \"Please generate example Python test functions for machine learning projects.\\n\"\n",
    "             \"{format_instructions}\\n\" \n",
    "             \"Here is a list of JSON objects, in which the 'Function's are the functions to be filled:\\n```{functions}```\\n\",\n",
    "             #\"Here is the code to be analyzed:\\n{context}\",\n",
    "    description=\"Filling Test Functions for Machine Learning Project\",\n",
    "    #input_variables=[\"checklist\", \"context\"],\n",
    "    input_variables=[\"functions\"],\n",
    "    partial_variables={\"format_instructions\": generator.parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "class TestGeneration(BaseModel):\n",
    "    ID: str = Field(description=\"The corresponding `ID` of the checklist item provided\")\n",
    "    Title: str = Field(description=\"The corresponding `Title` of the checklist item provided\")\n",
    "    Function: str = Field(description=\"A Python test function\")\n",
    "\n",
    "class GenResult(BaseModel):\n",
    "    results: List[TestGeneration]\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=GenResult)\n",
    "\n",
    "chain = prompt | generator.llm | parser\n",
    "\n",
    "response = chain.invoke({\"functions\": json.dumps(generator.spec)})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6559ec86-2a1b-40f0-8f55-52b017013126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_file_loads_as_expected():\n",
      "    \"\"\"Tests the data-loading function to ensure files load correctly and return expected results\n",
      "\n",
      "    Parameters:\n",
      "    ----------\n",
      "    Returns:\n",
      "    ----------\n",
      "    \"\"\"\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "print(response['results'][0]['Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec8fd8e-dcde-48d0-be58-d7f64f29637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_file_loads_as_expected():\n",
      "    \"\"\"Tests the data-loading function to ensure files load correctly and return expected results\n",
      "\n",
      "    Parameters:\n",
      "    ----------\n",
      "    Returns:\n",
      "    ----------\n",
      "    \"\"\"\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "print(func['results'][0]['Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1101f18-bb84-4f22-abac-0cb2b507db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator.chain.invoke({\"checklist\": generator.test_items})\n",
    "\n",
    "result = response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e4cc20-428c-4643-ad51-63f5604c7488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_data_file_loads_as_expected():\n",
      "    \"\"\"\n",
      "    Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
      "    \"\"\"\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b523f4a-5187-45b1-96fe-b8bcc196e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"TestSpecGeneration\": {\"properties\": {\"ID\": {\"description\": \"The corresponding `ID` of the checklist item provided\", \"title\": \"Id\", \"type\": \"string\"}, \"Title\": {\"description\": \"The corresponding `Title` of the checklist item provided\", \"title\": \"Title\", \"type\": \"string\"}, \"Function\": {\"description\": \"A test function with the docstring of numpy format\", \"title\": \"Function\", \"type\": \"string\"}}, \"required\": [\"ID\", \"Title\", \"Function\"], \"title\": \"TestSpecGeneration\", \"type\": \"object\"}}, \"properties\": {\"results\": {\"items\": {\"$ref\": \"#/$defs/TestSpecGeneration\"}, \"title\": \"Results\", \"type\": \"array\"}}, \"required\": [\"results\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(generator.parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab2b917-4d3f-40a0-a40d-1d6a34cebd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21cf481-4456-4691-8c5d-e75d30277f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function callable(obj, /)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f42aa-3877-426c-a4b8-d47bef272eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
